{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make CNN training set\n",
    "\n",
    "The goal of this notebook is to provide a workflow for annotating cells in subvolumes using Neuroglancer. This notebook does a few things:\n",
    "1. Starts a Neuroglancer session and provides the link\n",
    "2. Loads in your cell channel data into Neuroglancer\n",
    "3. Allows you to define the bounds and size of subvolumes and then draw them on Neuroglancer \n",
    "4. Allows you to create an annotation layer which will contain the annotations that you will draw and then ultimately save out. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "In order to run the code in this notebook, you will need a conda environment with python3 and containing some additional libraries. This environment, which I call \"ng\" below but you could call whatever you want, can be set up in the following way:\n",
    "\n",
    "- First download and install anaconda3. You can see if you already have it by typing \"which conda\" into your linux terminal. If the result shows you a filepath then you already have it installed. \n",
    "\n",
    "Once you have anaconda, in a linux terminal run the following commands to set up your new conda environment:\n",
    "- conda create -n ng python=3.8 -y\n",
    "- conda activate ng \n",
    "- pip install cloud-volume\n",
    "- pip install neuroglancer <br>\n",
    "- pip install --user ipykernel\n",
    "- python -m ipykernel install --user --name=ng\n",
    "\n",
    "Once this is all installed, start the jupyter notebook server  make sure to select this conda environment as the kernel when running this notebook via Kernel -> Change Kernel (you may have to restart the jupyter notebook server if you just created the conda environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host your brain data via cloudvolume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quirk with Neuroglancer is that the data need to be hosted via a server in order for Neuroglancer to load them. You will use your own computer as a server for your data. Below is code that will host your cell channel 642 data for the brain zimmerman_02-f12 on port 1337 of your machine. Note that you must run the following code in a separate python session (make sure to activate the \"ng\" conda environment before starting the python session). If you just ran it in this jupyter notebook it would work but it would cause the notebook to hang and you wouldn't be able to run the rest of the code. Note that you will need to have the LightSheetData bucket mounted on whatever machine you're working on for this to work. \n",
    "```python\n",
    "import cloudvolume\n",
    "vol = cloudvolume.CloudVolume('file:///jukebox/LightSheetData/lightserv/cz15/zimmerman_02/zimmerman_02-f12/imaging_request_1/viz/rawdata/channel_642_corrected')\n",
    "vol.viewer(port=1337)  \n",
    "```\n",
    "Later when you want to host the rest of the brains, the only part of the filepath above that you need to change is the sample name, i.e. zimmerman_02-f12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import neuroglancer as ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the BRAIN CoGS client\n",
    "ng.set_static_content_source(url='https://neuroglancer-braincogs.appspot.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data you are hosting on port 1337 into Neuroglancer and generate the link\n",
    "First set the sample name. In this example we are using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = \"zimmerman_02-f11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:38145/v/e8c0d885551b7dfed36f1b63be72be3e6255be5a/\n"
     ]
    }
   ],
   "source": [
    "viewer = ng.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    s.layers[f'{sample_name} cell channel'] = ng.ImageLayer(source='precomputed://http://localhost:1337')\n",
    "print(viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the link above to open Neuroglancer in a new tab. I recommend using Google Chrome for your Neuroglancer sessions. If you don't open the link first none of the code below will work. If you re-run the cell above it will generate an entirely new Neuroglancer session and a new link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the background is quite dark. Use the \"f\" key to increase brightness and \"d\" key to decrease brightness. \"i\" inverts the colors. The \"h\" key brings up the help menu with the whole list of commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a subvolume box for annotations\n",
    "To define a subvolume, you need to supply the size and the location of the subvolume. \n",
    "\n",
    "First, we will define the size in pixels, keeping in mind that 200x200x50 is a minimum\n",
    "The pixel size is 1.8 um x 1.8 um x 2 um (x,y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "npix_subvol = [300,300,50] # x,y,z "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now decide where to put the subvolume. Navigate in Neuroglancer to the place where you want the subvolume to start, then right click that spot in any of the 2D viewer panels. The x,y,z coordinates of where you clicked (also the location of the red, green and blue axis lines) will be shown in white in the top left of the screen. Update the list below with those coordinates. Note that these coordinates will be the origin (not center) of the subvolume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_offset_origvol = [3200,3500,1800] # x,y,z of where the subvolume starts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the cell below without changing anything. Then look in Neuroglancer and your subvolume will be created as a red box. \n",
    "The screen will automatically pan to somewhere near the origin of the subvolume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subvolume(npix_subvol,voxel_offset_origvol):  \n",
    "    use_mm_offset=False\n",
    "    xzfill,yzfill,zzfill = [str(coord).zfill(4) for coord in voxel_offset_origvol]\n",
    "    layer_name = f'{sample_name}_subvol_x{xzfill}_y_{yzfill}_z{zzfill}_dim_cells'\n",
    "    with viewer.txn() as s:\n",
    "        dim_dict = s.dimensions\n",
    "        nm_scale_dict = {}\n",
    "        for coord in ['x','y','z']:\n",
    "            dim_obj = dim_dict[coord]\n",
    "            unit = dim_obj.unit\n",
    "            assert unit == 'm'\n",
    "            nm_scale_dict[coord] = dim_obj.scale*1e9\n",
    "    nm_scale_dict\n",
    "\n",
    "    if not use_mm_offset:\n",
    "        mm_offset_x = voxel_offset_origvol[0]*nm_scale_dict['x']*1e-6 \n",
    "        mm_offset_y = voxel_offset_origvol[1]*nm_scale_dict['y']*1e-6 \n",
    "        mm_offset_z = voxel_offset_origvol[2]*nm_scale_dict['z']*1e-6 \n",
    "        mm_offset = [mm_offset_x,mm_offset_y,mm_offset_z]\n",
    "\n",
    "    voxel_offset = [int(round(mm_offset[0]*1e6/nm_scale_dict['x'])),\n",
    "                    int(round(mm_offset[1]*1e6/nm_scale_dict['y'])),\n",
    "                    int(round(mm_offset[2]*1e6/nm_scale_dict['z']))]\n",
    "    subvol = np.zeros(npix_subvol, dtype=np.uint8)\n",
    "\n",
    "    with viewer.txn() as s:\n",
    "        s.layers[layer_name] = ng.ImageLayer(\n",
    "            source=ng.LocalVolume(\n",
    "                subvol,\n",
    "                dimensions=ng.CoordinateSpace(\n",
    "                    scales=[nm_scale_dict['x'], nm_scale_dict['y'], nm_scale_dict['z']], \n",
    "                    units=['nm', 'nm', 'nm'],\n",
    "                    names=['x', 'y', 'z']),\n",
    "                voxel_offset=voxel_offset), \n",
    "        )\n",
    "        # Then pan to the location of this new volume\n",
    "        # To do that, need dimensions of your original data layer\n",
    "\n",
    "        # find the position of the new subvolume in these units\n",
    "        x_subvol_orig = mm_offset[0]*1e6/nm_scale_dict['x']+10\n",
    "        y_subvol_orig = mm_offset[1]*1e6/nm_scale_dict['y']+10\n",
    "        z_subvol_orig = mm_offset[2]*1e6/nm_scale_dict['z']\n",
    "        layer = s.layers[layer_name]\n",
    "        layer.annotationColor = \"#ff0000\"\n",
    "        s.selected_layer.layer = layer_name \n",
    "        s.selected_layer.visible = True\n",
    "        # Pan/zoom to near the origin of the subvolume \n",
    "        s.voxel_coordinates = [x_subvol_orig,y_subvol_orig,z_subvol_orig]\n",
    "        s.cross_section_scale=0.5\n",
    "\n",
    "make_subvolume(npix_subvol,voxel_offset_origvol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a red square show up in at least one of the 2D panels and in the 3d viewer panel. \n",
    "\n",
    "It might be a little darker or lighter than the rest of the volume. Use the opacity slider in the \"Rendering\" tab on the right hand side panel which should also have popped up to change the contrast between the subvolume and the rest of the volume. ctrl+mousewheel zooms in and out (or two finger drag up/down on a laptop).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, make an annotation layer so you can start making annotations\n",
    "with viewer.txn() as s:\n",
    "    xzfill,yzfill,zzfill = [str(coord).zfill(4) for coord in voxel_offset_origvol]\n",
    "    layer_name = f'{sample_name}_annotation_x{xzfill}_y_{yzfill}_z{zzfill}_dim_cells'\n",
    "    s.layers[layer_name]=ng.AnnotationLayer()\n",
    "    annot_layer = s.layers[layer_name]\n",
    "    annot_layer.tool = \"annotatePoint\"\n",
    "    s.cross_section_depth=-40 # conrols the z-depth in microns over which the annotated points will persist. \n",
    "    # negative sign is necessary for some reason.\n",
    "    # Toggle this in Neuroglancer with alt+\"=\" or alt+\"-\" to decrease and increase the depth\n",
    "    s.selected_layer.layer = layer_name \n",
    "    s.selected_layer.visible = True\n",
    "with viewer.config_state.txn() as st:\n",
    "    st.status_messages['update'] = 'Annotate points by ctrl+left clicking'   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate points by ctrl+left clicking. You can control the size of the points via the size slider in the \"Rendering\" tab on the right side control panel. When you are done with your annotations, use the \"Export to CSV\" button on the right hand side panel under the \"Annotations\" tab. Just make sure that the correct annotation layer is selected (right click on the layer box at the top of the screen and it will be outlined in green) and save the file with the filename below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstart = str(voxel_offset_origvol[0]).zfill(4)\n",
    "ystart = str(voxel_offset_origvol[1]).zfill(4)\n",
    "zstart = str(voxel_offset_origvol[2]).zfill(4)\n",
    "annotation_savename = f\"{sample_name}_x{xstart}_y{ystart}_z{zstart}_annotations.csv\"\n",
    "print(annotation_savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ng",
   "language": "python",
   "name": "ng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
