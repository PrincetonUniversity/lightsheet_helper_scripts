{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuroglancer\n",
    "import cloudvolume\n",
    "import pandas as pd, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README\n",
    "This example assumes you have precomputed datasets already made and hosted with cloudvolume.\n",
    "To host the raw data in this example that I already converted to precomputed format, open an ipython terminal and do:\n",
    "```python\n",
    "import cloudvolume \n",
    "vol = cloudvolume.CloudVolume('file:///jukebox/scratch/zmd/contra_ipsi_projection_studies_20191125/20180322_jg_bl6f_prv_28/647') \n",
    "vol.viewer(port=1344) # You can choose the port here, but remember it for later\n",
    "```\n",
    "\n",
    "```python\n",
    "import cloudvolume \n",
    "vol = cloudvolume.CloudVolume('file:///jukebox/scratch/zmd/contra_ipsi_projection_studies_20191125/20180322_jg_bl6f_prv_28/atlas') \n",
    "vol.viewer(port=1345) # You can choose the port here, but remember it for later\n",
    "```\n",
    "\n",
    "The last command will cause the window to hang -- that is the expected behavior. Do not try to do this in this jupyter notebook as it will cause your session to hang and you won't be able to run any of the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the neuroglancer client you want to use. \n",
    "Default is Seung lab's which should work fine for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuroglancer.set_static_content_source(url='https://neuromancer-seung-import.appspot.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the raw data into neuroglancer and generate the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:34561/v/0b200c25b1af51d71bfae8b11e54f258649fdb15/\n"
     ]
    }
   ],
   "source": [
    "# This volume handle can be used to notify the viewer that the data has changed.\n",
    "viewer = neuroglancer.Viewer()\n",
    "\n",
    "# Load in a segmentation layer (e.g. the raw-space allen atlas) that is being hosted with cloudvolume\n",
    "with viewer.txn() as s:\n",
    "    s.layers['20180322_jg_bl6f_prv_28'] = neuroglancer.ImageLayer(source='precomputed://http://localhost:1344',\n",
    "    shader = '''\n",
    "    void main() {\n",
    "  float v = toNormalized(getDataValue(0)) * 30.0;\n",
    "  float w = 1.0 - v;\n",
    "  emitRGBA(vec4(w, w, w, v));\n",
    "}\n",
    "'''                                                 \n",
    "    )    \n",
    "    s.layers['rawatlas_20180322_jg_bl6f_prv_28'] = neuroglancer.SegmentationLayer(source='precomputed://http://localhost:1345'\n",
    "    )\n",
    "\n",
    "print(viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the look-up table to make a key binding to the Neuroglancer session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First you need to create a dictionary mapping the atlas id to the region name. \n",
    "# Here's my example\n",
    "\n",
    "csv_file = '/jukebox/LightSheetTransfer/atlas/ls_id_table_w_voxelcounts_16bit.xlsx'\n",
    "\n",
    "df = pd.read_excel(csv_file)\n",
    "\n",
    "atlas_df = df.copy()\n",
    "\n",
    "atlas_dict = {}\n",
    "for ind, row in atlas_df.iterrows():\n",
    "    atlas_dict[int(row['id'])] = row['name']\n",
    "    \n",
    "# Here is the actual code for making the key binding. Copy and paste this\n",
    "# into your jupyter notebook where you have already made the viewer() object\n",
    "\n",
    "num_actions = 0\n",
    "def my_action3(s):\n",
    "    global num_actions\n",
    "    num_actions += 1\n",
    "    with viewer.config_state.txn() as st:\n",
    "        region_id = s.selected_values['rawatlas_20180322_jg_bl6f_prv_28']\n",
    "        region_name = atlas_dict.get(region_id)\n",
    "        st.status_messages['hello'] = ('%i:%s' %\n",
    "                                    (region_id, region_name))\n",
    "\n",
    "    print('Got my-action %i' % num_actions)\n",
    "#     print('  Mouse position: %s' % (s.mouse_voxel_coordinates,))\n",
    "    print('  Layer selected values: %s' % (s.selected_values,))\n",
    "viewer.actions.add('my-action3', my_action3)\n",
    "\n",
    "with viewer.config_state.txn() as s:\n",
    "    s.input_event_bindings.viewer['keyp'] = 'my-action3'\n",
    "    s.status_messages['hello'] = 'Welcome to the segment labeling example. Press \"p\" to see region name under cursor.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take screenshots (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the viewer and take a screenshot. Good for videos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdst = '/jukebox/wang/zahra/neuroglancer/screenshots/20170115_tp_bl6_lob6b_ml_04/frontal_areas'\n",
    "#make sure these directories exist\n",
    "if not os.path.exists(os.path.dirname(svdst)): os.mkdir(os.path.dirname(svdst)) #brain directory\n",
    "if not os.path.exists(svdst): os.mkdir(svdst) #structure directory\n",
    "for i in range(180, 530):\n",
    "    with viewer.config_state.txn() as s:\n",
    "        s.show_ui_controls = False\n",
    "        s.show_panel_borders = False\n",
    "    with viewer.txn() as s:\n",
    "        s.voxel_coordinates = [3081, 1314,i]\n",
    "#    with viewer.config_state.txn() as s:\n",
    "#        s.viewer_size = [1000,1000]\n",
    "    ss = neuroglancer.ScreenshotSaver(viewer, svdst)\n",
    "    ss.capture(index=i)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, can also take a single view screenshot of the current view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '/jukebox/wang/zahra/neuroglancer/screenshots/20170115_tp_bl6_lob6b_ml_04/frontal_areas/0000000.png')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svdst = '/jukebox/wang/zahra/neuroglancer/screenshots/20170115_tp_bl6_lob6b_ml_04/frontal_areas'\n",
    "if not os.path.exists(svdst): os.mkdir(svdst)\n",
    "with viewer.config_state.txn() as s:\n",
    "    s.show_ui_controls = False\n",
    "    s.show_panel_borders = False\n",
    "    s.viewer_size = [1000, 1000]\n",
    "ss = neuroglancer.ScreenshotSaver(viewer, svdst)\n",
    "ss.capture(index=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return viewer state to normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.config_state.txn() as s:\n",
    "    s.show_ui_controls = True\n",
    "    s.show_panel_borders = True\n",
    "    s.viewer_size = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the view layout to show the segmentation side by side with the image, rather than overlayed.  This can also be done from the UI by dragging and dropping.  The side by side views by default have synchronized position, orientation, and zoom level, but this can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn() as s:\n",
    "    s.layout = neuroglancer.row_layout(\n",
    "        [neuroglancer.LayerGroupViewer(layers=['20160822_tp_bl6_crii_1500r_06', 'overlay']),\n",
    "         neuroglancer.LayerGroupViewer(layers=['rawatlas_20160822_tp_bl6_crii_1500r_06'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the overlay layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn() as s:\n",
    "    s.layout = neuroglancer.row_layout(\n",
    "        [neuroglancer.LayerGroupViewer(layers=['20160822_tp_bl6_crii_1500r_06']),\n",
    "         neuroglancer.LayerGroupViewer(layers=['20170207_db_bl6_crii_1300r_02']),\n",
    "        #neuroglancer.LayerGroupViewer(layers=['20161205_tp_bl6_lob45_1000r_01']),\n",
    "        neuroglancer.LayerGroupViewer(layers=['20170116_tp_bl6_lob6b_lpv_07']),\n",
    "        neuroglancer.LayerGroupViewer(layers=['20170411_db_bl6_crii_mid_53hr']),\n",
    "        neuroglancer.LayerGroupViewer(layers=['20170419_db_bl6_cri_rpv_53hr'])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lspy35",
   "language": "python",
   "name": "lspy35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
