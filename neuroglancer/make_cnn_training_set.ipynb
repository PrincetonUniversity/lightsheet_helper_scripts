{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import neuroglancer as ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the BRAIN CoGS client\n",
    "ng.set_static_content_source(url='https://neuroglancer-braincogs.appspot.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the raw data into neuroglancer and generate the link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host the data via cloudvolume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to host your cell channel with cloudvolume: e.g.\n",
    "```python\n",
    "import cloudvolume\n",
    "vol = cloudvolume.Cloudvolume('file:///jukebox/LightSheetData/lightserv/cz15/zimmerman_01/zimmerman_01-001/imaging_request_1/viz/rawdata/blendeddata_zimmerman_01-001')\n",
    "vol.viewer(port=1337) \n",
    "```\n",
    "Make sure to do this outside of this jupyter notebook in a separate python session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data into Neuroglancer and generate the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:42381/v/c48c3038430e7dd96e69788a6fbfba4fa4babc8c/\n"
     ]
    }
   ],
   "source": [
    "viewer = ng.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    s.layers['cell channel'] = ng.ImageLayer(source='precomputed://http://localhost:1337',\n",
    "                                     shader=\"\"\"void main() {emitGrayscale(1.0-toNormalized(getDataValue())*350.0);}\"\"\")\n",
    "print(viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the link to open Neuroglancer in a new tab\n",
    "\n",
    "If you want to load the atlas on top of this to help you find regions to annotate points, then host the raw-space atlas for this volume with cloudvolume (on port 1338, say) and load it in just like you loaded in the cell channel, but using SegmentationLayer instead of ImageLayer like: \n",
    "\n",
    "```python\n",
    "with viewer.txn() as s:\n",
    "    s.layers['raw-space atlas'] = ng.SegmentationLayer(source='precomputed://http://localhost:1338')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 1800.0, 'y': 1800.0, 'z': 2000.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changing anything. It creates a dictionary containing the voxel size in nm \n",
    "with viewer.txn() as s:\n",
    "    dim_dict = s.dimensions\n",
    "    nm_scale_dict = {}\n",
    "    for coord in ['x','y','z']:\n",
    "        dim_obj = dim_dict[coord]\n",
    "        unit = dim_obj.unit\n",
    "        assert unit == 'm'\n",
    "        nm_scale_dict[coord] = dim_obj.scale*1e9\n",
    "nm_scale_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subvolume cutout in which to do the annotations\n",
    "\n",
    "# Define the size in pixels, keeping in mind that 200x200x50 is a minimum\n",
    "# This corresponds to ~0.35 mm x 0.35 mm x 0.1 mm\n",
    "npix_subvol = [250,250,50] # x,y,z\n",
    "\n",
    "# Decide where to put the subvolume either in mm offset from the origin (the top, front, left of the brain, \n",
    "# typically), or in voxel coordinates\n",
    "# May take some experimenting\n",
    "use_mm_offset = False # if you want to specify the mm offset then switch this to True and update mm_offset below.\n",
    "# Otherwise update voxel-offset_origvol below to the voxel coordaintes at which you want the subvolume to start\n",
    "if use_mm_offset:\n",
    "    mm_offset = [4,8,2] # x y z\n",
    "else:\n",
    "    voxel_offset_origvol = [3200,3500,1800] # \n",
    "    mm_offset_x = voxel_offset_origvol[0]*nm_scale_dict['x']*1e-6 \n",
    "    mm_offset_y = voxel_offset_origvol[1]*nm_scale_dict['y']*1e-6 \n",
    "    mm_offset_z = voxel_offset_origvol[2]*nm_scale_dict['z']*1e-6 \n",
    "    mm_offset = [mm_offset_x,mm_offset_y,mm_offset_z]\n",
    "# mm_offset\n",
    "# Don't change below this\n",
    "voxel_offset = [int(round(mm_offset[0]*1e6/nm_scale_dict['x'])),\n",
    "                int(round(mm_offset[1]*1e6/nm_scale_dict['y'])),\n",
    "                int(round(mm_offset[2]*1e6/nm_scale_dict['z']))]\n",
    "subvol = np.zeros(npix_subvol, dtype=np.uint8)\n",
    "\n",
    "with viewer.txn() as s:\n",
    "    s.layers['training_set_overlay'] = ng.ImageLayer(\n",
    "        source=ng.LocalVolume(\n",
    "            subvol,\n",
    "            dimensions=ng.CoordinateSpace(\n",
    "                scales=[nm_scale_dict['x'], nm_scale_dict['y'], nm_scale_dict['z']], \n",
    "                units=['nm', 'nm', 'nm'],\n",
    "                names=['x', 'y', 'z']),\n",
    "            voxel_offset=voxel_offset), \n",
    "        shader=\"\"\"\n",
    "void main() {\n",
    "  emitGrayscale(1.0-toNormalized(getDataValue())*350.0);\n",
    "}\n",
    "\"\"\", \n",
    "    )\n",
    "    # Then pan to the location of this new volume\n",
    "    # To do that, need dimensions of your original data layer\n",
    "    \n",
    "    # find the position of the new subvolume in these units\n",
    "    x_subvol_orig = mm_offset[0]*1e6/nm_scale_dict['x']+10\n",
    "    y_subvol_orig = mm_offset[1]*1e6/nm_scale_dict['y']+10\n",
    "    z_subvol_orig = mm_offset[2]*1e6/nm_scale_dict['z']+1\n",
    "    # Pan/zoom to this location\n",
    "#     s.voxel_coordinates = [x_subvol_orig,y_subvol_orig,z_subvol_orig]\n",
    "#     s.cross_section_scale=0.5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a square show up in at least one of the 2D panels and in the 3d viewer panel. Right click it in any of the panels to pan to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an annotation layer so you can start making annotations\n",
    "with viewer.txn() as s:\n",
    "    s.layers['subvol_annotation']=ng.AnnotationLayer()\n",
    "    annot_layer = s.layers['subvol_annotation']\n",
    "    annot_layer.tool = \"annotatePoint\"\n",
    "    s.cross_section_depth=-40 # conrols the z-depth in microns over which the annotated points will persist. \n",
    "    # negative sign is necessary for some reason.\n",
    "    # Toggle this in Neuroglancer with alt+\"=\" or alt+\"-\" to decrease and increase the depth\n",
    "    s.selected_layer.layer = 'subvol_annotation' \n",
    "    s.selected_layer.visible = True\n",
    "with viewer.config_state.txn() as st:\n",
    "    st.status_messages['update'] = 'Start annotating points by ctrl+left clicking'   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ng",
   "language": "python",
   "name": "ng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
