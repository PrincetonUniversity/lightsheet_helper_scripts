{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this notebook\n",
    "An obstacle for getting started with Neuroglancer is the data formats you can feed it are somewhat unfamiliar. For example, it does not accept TIFF format. It does this for a good reason though which is to be more efficient. One of the formats it accepts is called \"precomputed\" format, and that is the one we will use in this notebook. Fortunately there is a python pipeline for making precomputed data from TIFF files. \n",
    "\n",
    "Neuroglancer also is not set up to read CSV files. Instead, it uses the JSON file format, which essentially look like python dictionaries.\n",
    "\n",
    "This notebook covers how to convert a custom annotated atlas volume (of the same format as the 'WHS_SD_rat_atlas_v3_annotation.tif' file on bucket) to precomputed format so that you can load it in to Neuroglancer. It also covers how to convert a CSV file formatted like the 'labels_v3.csv' file on bucket containing the region name mapping for the Rat MRI atlas into JSON format so that Neuroglancer can read it and display the region names in the bottom left corner when you hover over a region with your cursor.\n",
    "\n",
    "In the following I use the original MRI atlas annotation volume and CSV file as an example. To use custom annotation volume and/or custom CSV file, replace the variables at the top of the notebook with your custom ones. The rest of the notebook should not (hopefully!) need to be changed before running.\n",
    "\n",
    "## A quick note about Neuroglancer\n",
    "Neuroglancer loads in datasets in \"layers\". A layer can be of type \"image\" (like what you would get as output from the light sheet microscope) or type \"segmentation\" (like an atlas annotation volume). The naming is a little confusing because both layer types refer to volumes (3-d objects). In this notebook, we are only concerned with a single layer: the annotation volume, which is a segmentation layer. If you were to make multiple annotation volumes (with different boundaries, etc.), each one of those would be a different layer. In Neuroglancer, you can overlay multiple layers or view them side-by-side. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "In order to run the code in this notebook, you will need a conda environment with python3 and containing some additional libraries. This environment \"ng_mriatlas\" can be set up in the following way:\n",
    "In terminal:\n",
    "- conda create -n ng_mriatlas python=3.7.4 -y\n",
    "- conda activate ng_mriatlas # (or source activate ng_mriatlas, depending on which version of conda you have)\n",
    "- pip install cloud-volume\n",
    "- pip install SimpleITK\n",
    "- **pip install neuroglancer** <br>\n",
    "\n",
    "\\# To enable you to use jupyter notebooks to work with this environment as a kernel:\n",
    "- pip install --user ipykernel\n",
    "- python -m ipykernel install --user --name=ng_mriatlas\n",
    "\n",
    "Once this is all installed, make sure to select this conda environment as the kernel when running this notebook (you might have to restart the notebook server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,csv,json\n",
    "import numpy as np\n",
    "from cloudvolume import CloudVolume\n",
    "from cloudvolume.lib import mkdir, touch\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Point to the custom annotation volume file that you have modified (in this example we point to the original one) \n",
    "custom_annotation_vol_path = 'WHS_SD_rat_atlas_v3_annotation.tif'\n",
    "\n",
    "# Point to the csv file mapping region id to region name that you have modified (in this example we point to the original one)\n",
    "annotation_csv_file = 'labels_v3.csv'\n",
    "\n",
    "# Decide on a folder name where your layer for this volume is going to live. \n",
    "# IMPORTANT: For each custom atlas you need to change this so you don't \n",
    "# overwrite an old layer. You need the full path here. \n",
    "# In this example my layer folder will be 'my_custom_atlas1' saved in the current working directory\n",
    "cwd = os.getcwd()\n",
    "layer_dir = os.path.join(cwd,'my_custom_atlas1')\n",
    "# Make the layer directory\n",
    "if not os.path.exists(layer_dir):\n",
    "    os.mkdir(layer_dir)\n",
    "    print(f\"created {layer_dir}\")\n",
    "    \n",
    "# Finally, decide how many cpus you are willing and able to use for the parallelized conversion (see step 3)\n",
    "cpus_to_use = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Convert the CSV annotation file into JSON file that Neuroglancer can read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotation_json_file(csv_input_file):\n",
    "    \"\"\" Take a csv file of the format of labels_v3.csv \n",
    "    containing the MRI atlas labels and converts it to a \n",
    "    JSON file so that Neuroglancer can read it in\n",
    "    \n",
    "    The JSON file will be saved with the same filename as \n",
    "    the CSV file but with .json extension instead of .csv\n",
    "    \"\"\"\n",
    "    annotation_json_file = csv_input_file.replace('.csv','.json')\n",
    "\n",
    "    with open(csv_input_file,'r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        next(reader) # skips the header since we just want the data and we already know what the order of the columns is\n",
    "        annotation_dict = {row[2]:row[1] for row in reader}\n",
    "        annotation_dict_list = [{f\"regionId\":key,\"regionName\":f\"{key}: {annotation_dict[key]}\"} for key in annotation_dict.keys()]\n",
    "    with open(annotation_json_file,'w') as outfile:\n",
    "        outfile.write(\n",
    "            '[' +\n",
    "            ',\\n'.join(json.dumps(i) for i in annotation_dict_list) +\n",
    "            ']\\n')\n",
    "    # Now move the annotation file to the layer directory\n",
    "    annotation_rel_path = annotation_json_file.split('/')[-1]\n",
    "    dst = os.path.join(layer_dir,annotation_rel_path)\n",
    "    os.rename(annotation_json_file,dst)\n",
    "    print(f\"Wrote file {dst}\")\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function to convert to JSON\n",
    "annotation_json_fullpath=make_annotation_json_file(annotation_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Write the instructions (\"info\") file that will tell Neuroglancer about your annotation volume and the annotation label file you just created\n",
    "The info file is a required file for the precomputed data format. It is a JSON file containing things like the shape and physical resolution of your volume, and it will also be where we tell it to read in our labels for the different anatomical regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_info_file(resolution_xyz,volume_size_xyz,annotation_filename,layer_dir):\n",
    "    \"\"\" Make an JSON-formatted file called the \"info\" file\n",
    "    for use with the precomputed data format. \n",
    "    Precomputed is one of the formats that Neuroglancer can read in.  \n",
    "    --- parameters ---\n",
    "    resolution_xyz:      A tuple representing the size of the pixels (dx,dy,dz) \n",
    "                         in nanometers, e.g. (20000,20000,5000) for 20 micron x 20 micron x 5 micron\n",
    "    \n",
    "    volume_size_xyz:     A tuple representing the number of pixels in each dimension (Nx,Ny,Nz)\n",
    "    \n",
    "    annotation_filename: The relative path (from layer_dir) to the JSON-formatted file\n",
    "                         that we created above containing the labels of the different regions.\n",
    "                         \n",
    "    layer_dir:           The directory where the precomputed data will be\n",
    "                         saved\n",
    "    \"\"\"\n",
    "    info = CloudVolume.create_new_info(\n",
    "        num_channels = 1,\n",
    "        layer_type = 'segmentation', # 'image' or 'segmentation'\n",
    "        data_type = 'uint16', # 32 not necessary for atlases unless you have > 2^(32)-1 labels  \n",
    "        encoding = 'raw', # other options: 'jpeg', 'compressed_segmentation' (req. uint32 or uint64)\n",
    "        resolution = resolution_xyz, # X,Y,Z values in nanometers, 40 microns in each dim\n",
    "        voxel_offset = [ 0, 0, 0 ], # values X,Y,Z values in voxels\n",
    "        chunk_size = [ 1024, 1024, 1 ], # rechunk of image X,Y,Z in voxels.\n",
    "        volume_size = volume_size_xyz, # X,Y,Z size in voxels\n",
    "    )\n",
    "\n",
    "    vol = CloudVolume(f'file://{layer_dir}', info=info)\n",
    "    vol.provenance.description = \"A test info file\" # can change this if you want a description\n",
    "    vol.provenance.owners = [''] # list of contact email addresses\n",
    "    # Saves the info and provenance files for the first time\n",
    "    vol.commit_info() # generates file://bucket/dataset/layer/info json file\n",
    "    vol.commit_provenance() # generates file://bucket/dataset/layer/provenance json file\n",
    "    # add a key for the custom atlas annotations to the info file and overwrite it \n",
    "    info_dict = vol.info\n",
    "    info_dict['atlas_path'] = annotation_filename\n",
    "    info_filename = '/'.join(vol.info_cloudpath.split('/')[2:]) \n",
    "    with open(info_filename,'w') as outfile:\n",
    "        json.dump(info_dict,outfile,sort_keys=True,indent=2)\n",
    "    print(\"Created CloudVolume info file: \",vol.info_cloudpath)\n",
    "\n",
    "    return vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the info file\n",
    "\n",
    "# The MRI atlas has 39 micron isotropic resolution - we need nanometers though\n",
    "resolution_xyz = (39000,39000,39000)\n",
    "# Load the MRI annotation volume into memory and get its shape \n",
    "annotation_vol = np.array(sitk.GetArrayFromImage(\n",
    "    sitk.ReadImage(custom_annotation_vol_path)),dtype=np.uint16,order='F')\n",
    "z_dim,y_dim,x_dim = annotation_vol.shape\n",
    "volume_size_xyz = (x_dim,y_dim,z_dim)\n",
    "\n",
    "# Now get the relative path to the annotation file\n",
    "annotation_json_relpath = annotation_json_fullpath.split('/')[-1]\n",
    "\n",
    "# Write the info file\n",
    "vol = make_info_file(\n",
    "    resolution_xyz=resolution_xyz,\n",
    "    volume_size_xyz=volume_size_xyz,\n",
    "    annotation_filename=annotation_json_relpath,\n",
    "    layer_dir=layer_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Convert annotation volume to precomputed data format\n",
    "First we create a directory (the \"progress_dir\") at the same folder level as the layer directory to keep track of the progress of the conversion. \n",
    "All the conversion does is copy the numpy array representing the 3d volume to a new object \"vol\". This is done one plane at a time (although it is parallelized). As each plane is converted, an empty file is created in the progress_dir with the name of the plane. By the end of the conversion, there should be as many files in this progress_dir as there are z planes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = layer_dir.split('/')[-1]\n",
    "parent_dir = '/'.join(layer_dir.split('/')[:-1])\n",
    "# print(parent_dir)\n",
    "progress_dir = mkdir(parent_dir+ f'/progress_{layer_name}') # unlike os.mkdir doesn't crash on prexisting \n",
    "print(f\"created directory: {progress_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_slice(z):\n",
    "    \"\"\" This function copies a 2d image slice from the atlas volume\n",
    "    to the cloudvolume object, vol. We will run this in parallel over \n",
    "    all z planes\n",
    "    ---parameters---\n",
    "    z:    An integer representing the 0-indexed z plane to be converted\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(progress_dir, str(z))):\n",
    "        print(f\"Slice {z} already processed, skipping \")\n",
    "        return\n",
    "    if z >= z_dim: # z is zero indexed and runs from 0-(z_dim-1)\n",
    "        print(\"Index {z} >= z_dim of volume, skipping\")\n",
    "        return\n",
    "    print('Processing slice z=',z)\n",
    "    array = annotation_vol[z].reshape((1,y_dim,x_dim)).T\n",
    "    vol[:,:, z] = array\n",
    "    touch(os.path.join(progress_dir, str(z)))\n",
    "    return \"success\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the conversion in parallel. It's not a huge amount of processing but the more cores the better\n",
    "\n",
    "# First figure out if there are any planes that have already been converted \n",
    "# by checking the progress dir\n",
    "done_files = set([ int(z) for z in os.listdir(progress_dir) ])\n",
    "all_files = set(range(vol.bounds.minpt.z, vol.bounds.maxpt.z))\n",
    "# Figure out the ones we still need to convert \n",
    "to_upload = [ int(z) for z in list(all_files.difference(done_files)) ]\n",
    "to_upload.sort()\n",
    "print(f\"Have {len(to_upload)} planes to upload\")\n",
    "with ProcessPoolExecutor(max_workers=cpus_to_use) as executor:\n",
    "    for result in executor.map(process_slice,to_upload):\n",
    "        try:\n",
    "            print(result)\n",
    "        except Exception as exc:\n",
    "            print(f'generated an exception: {exc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Host the precomputed data on your machine so that Neuroglancer can see it\n",
    "This step is really easy! Note: Exectuing the code below will cause your jupyter notebook to hang, so it is better to run the following code in a new ipython terminal (make sure to have the ng_mriatlas conda environment activated in that python session) rather than the notebook. \n",
    "\n",
    "```python\n",
    "from cloudvolume import CloudVolume\n",
    "vol = CloudVolume(f'file://{layer_dir}')\n",
    "vol.viewer(port=1338)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: View your custom volume and labels in Neuroglancer\n",
    "Step 4 hosts your data via http on port 1338 of your local machine. To actually view your data in Neuroglancer, there are two ways to do this. You can either load the data in manually in the browser or load it in with python. \n",
    "\n",
    "For the manual method, open up the Braincogs Neuroglancer client: [https://nglancer.pni.princeton.edu](https://nglancer.pni.princeton.edu) (you must be using a Princeton VPN) and then click the \"+\" in the upper left hand corner of the screen once the black screen loads. To load in your data, type the following into the source text box:<br>\n",
    "> precomputed://http://localhost:1338 <br>\n",
    "\n",
    "Then hit tab and name your layer if you'd like. Hit enter or the \"add layer\" button and your layer should load into Neuroglancer. Hopefully the labels you added should be showing up in the bottom left when you hover over a region. \n",
    "\n",
    "For the python method, you can do this by executing the following cell. Make sure you have hosted the data in another python instance somewhere on your local machine at port 1338."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuroglancer\n",
    "\n",
    "# Set which client you want to use - use the BRAINCOGS client to get the latest features \n",
    "neuroglancer.set_static_content_source(url='https://nglancer.pni.princeton.edu')\n",
    "\n",
    "# Make a viewer object that represents your connection to the Neuroglancer window\n",
    "viewer = neuroglancer.Viewer()\n",
    "\n",
    "# Load in the layer with name \"my_custom_atlas\"\n",
    "with viewer.txn() as s:\n",
    "    s.layers['my_custom_atlas'] = neuroglancer.SegmentationLayer(source='precomputed://http://localhost:1338',\n",
    "    )\n",
    "print(viewer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ng_mriatlas",
   "language": "python",
   "name": "ng_mriatlas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
