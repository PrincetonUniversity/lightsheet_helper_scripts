{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this notebook\n",
    "In order to start using Neuroglancer, we will need to have our data in the right format. Neuroglancer does not accept the standard image formats, such as TIFF. It does this for a good reason which is to be more efficient for very large volumes. One of the formats it uses is called \"precomputed\" format, and that is the one we will use in this notebook. Fortunately there is a python pipeline for making precomputed data from TIFF files.\n",
    "\n",
    "This notebook covers how to convert a volumetric dataset obtained from Princeton's light sheet microscope to precomputed format so that it can be viewed in Neuroglancer. \n",
    "\n",
    "This notebook also covers how to host the precomputed data on your local machine so that Neuroglancer can load it. \n",
    "\n",
    "## A quick note about Neuroglancer\n",
    "Neuroglancer loads in datasets in \"layers\". Layers come in different types. The two most common types are \"image\" layers (like what you would get as output from the light sheet microscope) and \"segmentation\" layers (like an atlas annotation volume). The naming is a little confusing because both layer types refer to volumes (3-d objects). In this notebook, we will be only be using one layer, and it is of type \"image\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "In order to run the code in this notebook, you will need a conda environment with python3 and containing some additional libraries. This environment \"ng_demo\" can be set up in the following way:\n",
    "In terminal:\n",
    "- conda create -n ng_demo python=3.7.4 -y\n",
    "- conda activate ng_demo (or \"source activate ng_demo\", depending on which version of conda you have)\n",
    "- pip install cloud-volume\n",
    "- pip install SimpleITK <br>\n",
    "\n",
    "\\# To enable you to use jupyter notebooks to work with this environment as a kernel:\n",
    "- pip install --user ipykernel\n",
    "- python -m ipykernel install --user --name=ng_demo\n",
    "\n",
    "Once this is all installed, make sure to select this conda environment as the kernel when running this notebook (you might have to restart the notebook server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,csv,json\n",
    "import numpy as np\n",
    "from cloudvolume import CloudVolume\n",
    "from cloudvolume.lib import mkdir, touch\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# An example whole mouse brain volume is here: \n",
    "registered_brain_vol_path = '/jukebox/LightSheetData/lightserv_testing/lightserv-test/two_channels/two_channels-001/imaging_request_1/output/processing_request_1/resolution_1.3x/elastix/result.1.tif'\n",
    "# Decide on a folder name where your precomputed data for this layer is going to live. \n",
    "# In this example my layer folder will be 'my_registered_volume' saved on my local hard drive.\n",
    "# The output will take up ~120 MB of space\n",
    "ng_home_dir = '/home/ahoag/ngdemo/demo_bucket/ng_tutorial'\n",
    "layer_dir = os.path.join(ng_home_dir,'my_registered_volume')\n",
    "# Make the layer directory\n",
    "mkdir(layer_dir) # won't overwrite if already exists, also won't complain if already exists\n",
    "print(f\"Using {layer_dir}\")\n",
    "    \n",
    "# Finally, decide how many cpus you are willing and able to use for the parallelized conversion (see step 3)\n",
    "cpus_to_use = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Write the instructions (\"info\") file that will tell Neuroglancer about your volume \n",
    "The info file is a required file for the precomputed data format. It is a JSON-formatted file (looks like a nested python dictionary) containing things like the shape and physical resolution of your volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_info_file(resolution_xyz,volume_size_xyz,layer_dir):\n",
    "    \"\"\" Make an JSON-formatted file called the \"info\" file\n",
    "    for use with the precomputed data format. \n",
    "    Precomputed is one of the formats that Neuroglancer can read in.  \n",
    "    --- parameters ---\n",
    "    resolution_xyz:      A tuple representing the size of the pixels (dx,dy,dz) \n",
    "                         in nanometers, e.g. (20000,20000,5000) for 20 micron x 20 micron x 5 micron\n",
    "    \n",
    "    volume_size_xyz:     A tuple representing the number of pixels in each dimension (Nx,Ny,Nz)\n",
    "    \n",
    "                         \n",
    "    layer_dir:           The directory where the precomputed data will be\n",
    "                         saved\n",
    "    \"\"\"\n",
    "    info = CloudVolume.create_new_info(\n",
    "        num_channels = 1,\n",
    "        layer_type = 'image', # 'image' or 'segmentation'\n",
    "        data_type = 'uint16', # 32 not necessary for atlases unless you have > 2^(32)-1 labels  \n",
    "        encoding = 'raw', # other options: 'jpeg', 'compressed_segmentation' (req. uint32 or uint64)\n",
    "        resolution = resolution_xyz, # X,Y,Z values in nanometers, 40 microns in each dim\n",
    "        voxel_offset = [ 0, 0, 0 ], # values X,Y,Z values in voxels\n",
    "        chunk_size = [ 1024, 1024, 1 ], # rechunk of image X,Y,Z in voxels.\n",
    "        volume_size = volume_size_xyz, # X,Y,Z size in voxels\n",
    "    )\n",
    "\n",
    "    vol = CloudVolume(f'file://{layer_dir}', info=info)\n",
    "    vol.provenance.description = \"A test info file\" # can change this if you want a description\n",
    "    vol.provenance.owners = [''] # list of contact email addresses\n",
    "    # Saves the info and provenance files for the first time\n",
    "    vol.commit_info() # generates info json file\n",
    "    vol.commit_provenance() # generates provenance json file\n",
    "    print(\"Created CloudVolume info file: \",vol.info_cloudpath)\n",
    "\n",
    "    return vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the info file.\n",
    "\n",
    "# The volume is registered to the Princeton Mouse Atlas which has 20micron isotropic resolution \n",
    "resolution_xyz = (20000,20000,20000) # must be in nanometers\n",
    "# Load the volume into memory and get its shape - can be slow if using VPN from off-campus. \n",
    "# Might just be faster to copy the file to your local disk and then change the registered_brain_vol_path \n",
    "# to point to the local copy\n",
    "brain_vol = np.array(sitk.GetArrayFromImage(\n",
    "    sitk.ReadImage(registered_brain_vol_path)),dtype=np.uint16,order='F')\n",
    "z_dim,y_dim,x_dim = brain_vol.shape\n",
    "volume_size_xyz = (x_dim,y_dim,z_dim)\n",
    "\n",
    "\n",
    "# Write the info file\n",
    "vol = make_info_file(\n",
    "    resolution_xyz=resolution_xyz,\n",
    "    volume_size_xyz=volume_size_xyz,\n",
    "    layer_dir=layer_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Convert volume to precomputed data format\n",
    "First we create a directory (the \"progress_dir\") at the same level as the layer directory to keep track of the progress of the conversion. \n",
    "All the conversion does is copy the numpy array representing the 3d volume to a new object \"vol\". This is done one plane at a time (although it is parallelized). As each plane is converted, an empty file is created in the progress_dir with the name of the plane. By the end of the conversion, there should be as many files in this progress_dir as there are z planes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = layer_dir.split('/')[-1]\n",
    "parent_dir = '/'.join(layer_dir.split('/')[:-1])\n",
    "# print(parent_dir)\n",
    "progress_dir = mkdir(parent_dir+ f'/progress_{layer_name}') # unlike os.mkdir doesn't crash on prexisting \n",
    "print(f\"created directory: {progress_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_slice(z):\n",
    "    \"\"\" This function copies a 2d z-plane slice from the atlas volume\n",
    "    to the cloudvolume object, vol. We will run this in parallel over \n",
    "    all z planes\n",
    "    ---parameters---\n",
    "    z:    An integer representing the 0-indexed z plane to be converted\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(progress_dir, str(z))):\n",
    "        print(f\"Slice {z} already processed, skipping \")\n",
    "        return\n",
    "    if z >= z_dim: # z is zero indexed and runs from 0-(z_dim-1)\n",
    "        print(\"Index {z} >= z_dim of volume, skipping\")\n",
    "        return\n",
    "    print('Processing slice z=',z)\n",
    "    array = brain_vol[z].reshape((1,y_dim,x_dim)).T\n",
    "    vol[:,:, z] = array\n",
    "    touch(os.path.join(progress_dir, str(z)))\n",
    "    return \"success\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the conversion in parallel. It's not a huge amount of processing but the more cores the better\n",
    "\n",
    "# First figure out if there are any planes that have already been converted \n",
    "# by checking the progress dir\n",
    "done_files = set([ int(z) for z in os.listdir(progress_dir) ])\n",
    "all_files = set(range(vol.bounds.minpt.z, vol.bounds.maxpt.z))\n",
    "# Figure out the ones we still need to convert \n",
    "to_upload = [ int(z) for z in list(all_files.difference(done_files)) ]\n",
    "to_upload.sort()\n",
    "print(f\"Have {len(to_upload)} planes to upload\")\n",
    "with ProcessPoolExecutor(max_workers=cpus_to_use) as executor:\n",
    "    for result in executor.map(process_slice,to_upload):\n",
    "        try:\n",
    "            print(result)\n",
    "        except Exception as exc:\n",
    "            print(f'generated an exception: {exc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Host the precomputed data on your machine so that Neuroglancer can see it\n",
    "This step is really easy! Note: Executing the cell below will cause your jupyter notebook to hang (expected). This is the last cell you need to run in the notebook, but if for some reason you want to use the same notebook for something else then run the code in the cell below elsewhere (e.g. a new terminal window). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = CloudVolume(f'file://{layer_dir}')\n",
    "vol.viewer(port=1338)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: View your custom volume and labels in Neuroglancer!\n",
    "Step 3 hosts your data via http on port 1338 of your local machine. When you run Neuroglancer in your browser, you can tell it to look for data hosted at a particular port. To do this, open up the Braincogs Neuroglancer client: [https://braincogs00.pni.princeton.edu/nglancer_viewer](https://braincogs00.pni.princeton.edu/nglancer_viewer) (you must be using a Princeton VPN) and then click the \"+\" in the upper left hand corner of the screen once the black screen loads. To load in your data, type the following into the source text box:<br>\n",
    "> precomputed://http://localhost:1338 <br>\n",
    "\n",
    "Then hit tab and name your layer if you'd like. Hit enter or the \"add layer\" button and your layer should load into Neuroglancer. The first thing you will notice if it works is the image is all black within the yellow bounding box. Your data are there but you need to change the contrast to see it. To do that, use the \"d\" and \"f\" keys until you have the contrast you like. The \"i\" key inverts the colormap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
