{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this notebook\n",
    "In order to display data in Neuroglancer it needs to be converted to one of several formats. The one we will use is called \"precomputed\". TIFF is not one of the accepted formats, but there is a python package for converting TIFF data into the \"precomputed\" data format that Neuroglancer reads. \n",
    "\n",
    "This notebook covers how to convert a registered mouse brain TIFF volume to precomputed format so that you can load it into Neuroglancer. It also goes through how to load your registered data and the atlas in Neuroglancer so you can see them overlaid.  \n",
    "\n",
    "\n",
    "## A quick note about Neuroglancer\n",
    "Neuroglancer loads in datasets in \"layers\". A layer can be of type \"image\" (like what you would get as output from the light sheet microscope) or type \"segmentation\" (like the atlas annotation volume). The naming is a little confusing because both layer types refer to volumes (3-d objects). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "In order to run the code in this notebook, you will need an anaconda installation (or some other virtual environment manager) that is able to create environments running python3 and containing some additional libraries. \n",
    "\n",
    "In this example, I will create a virtual environment using anaconda called \"ng_lightsheet\".\n",
    "\n",
    "In the terminal:\n",
    "- conda create -n ng_lightsheet python=3.7.4 -y # creates the anaconda environment\n",
    "- conda activate ng_lightsheet # (or \"source activate ng_lightsheet\", depending on which version of conda you have)\n",
    "- pip install neuroglancer --use-feature=2020-resolver\n",
    "- pip install cloud-volume --use-feature=2020-resolver\n",
    "- pip install SimpleITK --use-feature=2020-resolver\n",
    "\n",
    "\n",
    "\\# To enable you to use jupyter notebooks to work with this environment as a kernel:\n",
    "- pip install --user ipykernel --use-feature=2020-resolver\n",
    "- python -m ipykernel install --user --name=ng_lightsheet\n",
    "\n",
    "Once this is all installed, make sure to select this conda environment as the kernel when running this notebook (you might have to restart jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created /home/ahoag/progs/fiber_tracking_example/20190510_fiber_placement/registered_data_m360_dorsal_up\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from cloudvolume import CloudVolume\n",
    "from cloudvolume.lib import mkdir, touch\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import neuroglancer\n",
    "\n",
    "\n",
    "# Point to the registered data volume file. This is the output\n",
    "# from running elastix to register your data to the Paxinos atlas\n",
    "dataset = '20190510_fiber_placement'\n",
    "animal_id = 'm360_dorsal_up'\n",
    "reg_vol_path = os.path.join(\n",
    "    '/jukebox/wang/willmore/lightsheet',\n",
    "    dataset,\n",
    "    animal_id,\n",
    "    'paxinos_registration/elastix/result.1.tif'\n",
    ")\n",
    "\n",
    "# Decide on a folder name where your layer for this volume is going to live. \n",
    "basedir = '/home/ahoag/progs/fiber_tracking_example' # change to somewhere on your filesystem or bucket\n",
    "\n",
    "layer_dir = os.path.join(basedir,dataset,f'registered_data_{animal_id}')\n",
    "# Make the layer directory (mkdir won't overwrite)\n",
    "mkdir(layer_dir)\n",
    "print(f\"created {layer_dir}\")\n",
    "    \n",
    "# Finally, decide how many cpus you are willing and able to use for the parallelized conversion (see step 2)\n",
    "cpus_to_use = 8 # 6 or more should be perfectly fast for the task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Write the instructions (\"info\") file that will tell Neuroglancer about your data volume \n",
    "This info file is a JSON file (looks like a python dictionary but saved in a readable file) and it contains general things about our layer like the shape and physical resolution of the volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_info_file(resolution_xyz,volume_size_xyz,layer_dir):\n",
    "    \"\"\" Make an JSON-formatted file called the \"info\" file\n",
    "    for use with the precomputed data format that Neuroglancer can read.  \n",
    "    --- parameters ---\n",
    "    resolution_xyz:      A tuple representing the size of the pixels (dx,dy,dz) \n",
    "                         in nanometers, e.g. (20000,20000,5000) for 20 micron x 20 micron x 5 micron\n",
    "    \n",
    "    volume_size_xyz:     A tuple representing the number of pixels in each dimension (Nx,Ny,Nz)\n",
    "\n",
    "                         \n",
    "    layer_dir:           The directory where the precomputed data will be\n",
    "                         saved\n",
    "    \"\"\"\n",
    "    info = CloudVolume.create_new_info(\n",
    "        num_channels = 1,\n",
    "        layer_type = 'image', # 'image' here since this layer represents imaging data\n",
    "        data_type = 'uint16', # 32-bit not necessary for data usually. Use smallest possible  \n",
    "        encoding = 'raw', # other options: 'jpeg', 'compressed_segmentation' (req. uint32 or uint64)\n",
    "        resolution = resolution_xyz, # X,Y,Z values in nanometers, 40 microns in each dim\n",
    "        voxel_offset = [ 0, 0, 0 ], # values X,Y,Z values in voxels\n",
    "        chunk_size = [ 1024, 1,1024 ], # rechunk of image X,Y,Z in voxels. We want fast access to y (coronal) planes\n",
    "        volume_size = volume_size_xyz, # X,Y,Z size in voxels\n",
    "    )\n",
    "\n",
    "    vol = CloudVolume(f'file://{layer_dir}', info=info)\n",
    "    vol.provenance.description = \"A test info file\" # can change this if you want a description\n",
    "    vol.provenance.owners = [''] # list of contact email addresses\n",
    "    # Saves the info and provenance files for the first time\n",
    "    vol.commit_info() # generates file://bucket/dataset/layer/info json file\n",
    "    vol.commit_provenance() # generates file://bucket/dataset/layer/provenance json file\n",
    "    print(\"Created CloudVolume info file: \",vol.info_cloudpath)\n",
    "\n",
    "    return vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the info file \n",
    "\n",
    "# The resolution of our registered data volume is governed\n",
    "# by the resolution of the Paxinos atlas, which is 10x100x10 micron resolution,\n",
    "# but we need to specify it in nanometers here\n",
    "resolution_xyz = (10000,100000,10000) # 10x100x10 micron resolution)\n",
    "# Load the registered data volume and get its shape. \n",
    "# This can take a ~10-20 seconds to load\n",
    "reg_vol = np.array(sitk.GetArrayFromImage(\n",
    "    sitk.ReadImage(reg_vol_path)),dtype=np.uint16,order='F')\n",
    "z_dim,y_dim,x_dim = reg_vol.shape\n",
    "volume_size_xyz = (x_dim,y_dim,z_dim)\n",
    "\n",
    "# Write the info file\n",
    "vol = make_info_file(\n",
    "    resolution_xyz=resolution_xyz,\n",
    "    volume_size_xyz=volume_size_xyz,\n",
    "    layer_dir=layer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Convert volume to precomputed data format\n",
    "First we create a directory (the \"progress_dir\") at the same folder level as the layer directory to keep track of the progress of the conversion. \n",
    "All the conversion does is copy the numpy array representing the 3d volume to a new object \"vol\". This is done one plane at a time (although it is parallelized). As each plane is converted, an empty file is created in the progress_dir with the name of the plane. By the end of the conversion, there should be as many files in this progress_dir as there are z planes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created directory: /home/ahoag/progs/fiber_tracking_example/20190510_fiber_placement/progress_registered_data_m360_dorsal_up\n"
     ]
    }
   ],
   "source": [
    "layer_name = layer_dir.split('/')[-1]\n",
    "parent_dir = '/'.join(layer_dir.split('/')[:-1])\n",
    "progress_dir = mkdir(parent_dir+ f'/progress_{layer_name}') # unlike os.mkdir doesn't crash on prexisting \n",
    "print(f\"created directory: {progress_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_slice(y):\n",
    "    \"\"\" This function copies a 2d image slice from the atlas volume\n",
    "    to the cloudvolume object, vol. We will run this in parallel over \n",
    "    all y (coronal) planes\n",
    "    ---parameters---\n",
    "    y:    An integer representing the 0-indexed coronal slice to be converted\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(progress_dir, str(y))):\n",
    "        print(f\"Slice {y} already processed, skipping \")\n",
    "        return\n",
    "    if y >= y_dim: # y is zero indexed and runs from 0-(y_dim-1)\n",
    "        print(\"Index {y} >= y_dim of volume, skipping\")\n",
    "        return\n",
    "    print('Processing slice y=',y)\n",
    "    array = reg_vol[:,y,:].reshape((z_dim,1,x_dim)).T\n",
    "    vol[:,y,:] = array\n",
    "    touch(os.path.join(progress_dir, str(y)))\n",
    "    return \"success\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the conversion in parallel. It's not a huge amount of processing but the more cores the better\n",
    "\n",
    "# First we check to see if there are any planes that have already been converted \n",
    "# by checking the progress dir\n",
    "done_files = set([ int(y) for y in os.listdir(progress_dir) ])\n",
    "all_files = set(range(vol.bounds.minpt.y, vol.bounds.maxpt.y)) \n",
    "\n",
    "to_upload = [ int(y) for y in list(all_files.difference(done_files)) ]\n",
    "to_upload.sort()\n",
    "print(\"Remaining slices to upload are:\",to_upload)\n",
    "with ProcessPoolExecutor(max_workers=cpus_to_use) as executor:\n",
    "    for result in executor.map(process_slice,to_upload):\n",
    "        try:\n",
    "            print(result)\n",
    "        except Exception as exc:\n",
    "            print(f'generated an exception: {exc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Host the precomputed data on your machine so that Neuroglancer can see it\n",
    "This step is really easy! Note: Exectuing the code below will cause your jupyter notebook to hang, so it is better to run the following code in a new ipython terminal or a script (make sure to have the ng_lightsheet conda environment activated in that python session) rather than this notebook. \n",
    "\n",
    "```python\n",
    "from cloudvolume import CloudVolume\n",
    "vol = CloudVolume(f'file://{layer_dir}') # need to set layer_dir to the layer_dir you defined above\n",
    "vol.viewer(port=1338)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: View your registered data and the Paxinos atlas in Neuroglancer\n",
    "Step 3 hosts your data via http on port 1338 of your local machine. To actually view your data in Neuroglancer, there are two ways to do this. You can either load the data in manually in the browser or load it in with Python. The Python method is a lot more convenient and gives you a lot more power. However, it is useful to know how to manually add a layer for debugging the Python method. \n",
    "\n",
    "For the manual method, open up the Braincogs Neuroglancer client: [https://nglancer.pni.princeton.edu](https://nglancer.pni.princeton.edu) (you must be using a Princeton VPN or on campus) in your browser of choice (Chrome preferred) and then click the \"+\" in the upper left hand corner of the screen once the black screen loads. To load in your data, type the following into the source text box:<br>\n",
    "> precomputed://http://localhost:1338 <br>\n",
    "\n",
    "Then hit tab and name your layer if you'd like. Hit enter or the \"add layer\" button and your layer should load into Neuroglancer. Hopefully the labels you added should be showing up in the bottom left when you hover over a region. \n",
    "\n",
    "For the Python method, you can do this by executing the following cells. Make sure you have hosted the data in another Python instance somewhere on your local machine at port 1338, as described in step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set which client you want to use - use the BRAINCOGS client to get the latest features.\n",
    "# Need to be in the Princeton VPN or on campus to use this\n",
    "neuroglancer.set_static_content_source(url='https://nglancer.pni.princeton.edu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:46840/v/d3a194b2ec12d71b3deafc9028bf385211537b52/\n"
     ]
    }
   ],
   "source": [
    "# Make a viewer object that represents your connection to a Neuroglancer session\n",
    "viewer = neuroglancer.Viewer()\n",
    "\n",
    "# Load in the layer we just made and the Paxinos mouse atlas \n",
    "# and the layer showing the Paxinos boundaries\n",
    "\n",
    "# This cell generates a link, which when clicked brings you to the\n",
    "# Neuroglancer browser interface with your data loaded in as one layer, the\n",
    "# Paxinos atlas loaded in as a second layer (colored regions),\n",
    "# and the Paxinos atlas boundaries as a third layer\n",
    "with viewer.txn() as s:\n",
    "    s.layers[layer_name] = neuroglancer.ImageLayer(\n",
    "        source='precomputed://http://localhost:1338')\n",
    "    s.layers['Paxinos Mouse Atlas'] = neuroglancer.SegmentationLayer(\n",
    "        source='precomputed://gs://wanglab-pma/kimatlas')\n",
    "    s.layers['Paxinos Boundaries'] = neuroglancer.SegmentationLayer(\n",
    "        source='precomputed://gs://wanglab-pma/kimatlas_boundaries')\n",
    "\n",
    "print(viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clicking the link, if all went well, you should see your data, the Paxinos colored regions and the boundaries in sagittal, coronal and horizontal views.\n",
    "\n",
    "The coronal sections are all we care about for Paxinos (since the atlas is too low resolution to be useful in the other cross sectional views), and we also only care about the boundaries. The following code changes the layout so we only see the coronal view and rotates the brain so it is in the usual orientation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn() as s:\n",
    "    s.layout = 'xz'\n",
    "    s.crossSectionOrientation = [\n",
    "    0,\n",
    "    0.7071067690849304,\n",
    "    0,\n",
    "    0.7071067690849304\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to turn down the transparency of the colored regions so we can only see the boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn() as s:\n",
    "    atlas_layer = s.layers['Paxinos Mouse Atlas']\n",
    "    atlas_layer.selected_alpha = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we turned the transparency of this layer all the way down, when we hover over the middle area of a region, we can still see the region name in the layer box at the top of the viewer. If you don't care about this feature you can remove the \"Paxinos Mouse Atlas\" layer entirely and just work only with the boundaries. The name of the region will still appear when you hover over a boundary, but because they are so thin it can be more convenient to keep this other hidden layer on.  \n",
    "\n",
    "You will notice that the boundaries are still colored. Assuming you want to make these all black or white, you first should check the \"Set all segments black/white\" checkbox in Neuroglancer under the \"Render\" tab of the \"Paxinos Boundaries\" side panel. This will by defaul make the segments white. If you want them to be black, you can use the \"Saturation\" slidebar manually and set it to 0 or you can do that with the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn() as s:\n",
    "    boundaries_layer = s.layers['Paxinos Boundaries']\n",
    "    boundaries_layer.saturation = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transparency of the boundaries can also be controlled via the Opacity (on) slider or via this example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn() as s:\n",
    "    boundaries_layer = s.layers['Paxinos Boundaries']\n",
    "    boundaries_layer.selected_alpha = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The colormap of your data can be adjusted via the \"d\" and \"f\" keys and the colors can be inverted using the \"i\" key. This is especially helpful for getting a better contrast against black boundaries. These can also be controlled more precisely via code, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn() as s:\n",
    "    boundaries_layer = s.layers['Paxinos Boundaries']\n",
    "    boundaries_layer.shader = \"void main() {emitGrayscale(1.0-toNormalized(getDataValue())*40.0);}\"\n",
    "    # The \"1.0-\" part inverts the colormap and the factor of 40.0 at the end controls the brightness. \n",
    "    # Feel free to change the factor to suit whatever works "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ng_lightsheet",
   "language": "python",
   "name": "ng_lightsheet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
