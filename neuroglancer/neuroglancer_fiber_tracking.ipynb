{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this notebook\n",
    "In order to display data in Neuroglancer it needs to be converted to one of several formats. The one we will use is called \"precomputed\". TIFF is not one of the accepted formats, but there is a python package for converting TIFF data into the \"precomputed\" data format that Neuroglancer reads. \n",
    "\n",
    "This notebook covers how to convert a registered mouse brain TIFF volume to precomputed format so that you can load it into Neuroglancer. It also goes through how to load your registered data and the atlas boundaries in Neuroglancer so you can see them overlaid. Finally, I show some examples of other manipulations you can make using Python. \n",
    "\n",
    "Steps 1-3 should work no matter what browser you are using. However, step 4 I have only tested using Google Chrome. There are probably other browsers that will work but you will most likely get an error when you try to use the \"webdriver\". You might be able to download the driver for your browser and still get this all to work, but be warned. \n",
    "\n",
    "\n",
    "## A quick note about Neuroglancer\n",
    "Neuroglancer loads in datasets in \"layers\". A layer can be of type \"image\" (like what you would get as output from the light sheet microscope) or type \"segmentation\" (like the atlas annotation volume). The naming is a little confusing because both layer types refer to volumes (3-d objects). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "In order to run the code in this notebook, you will need an anaconda installation (or some other virtual environment manager) that is able to create environments running python3 and containing some additional libraries. \n",
    "\n",
    "In this example, I will create a virtual environment using anaconda called \"ng_lightsheet\".\n",
    "\n",
    "In the terminal:\n",
    "- conda create -n ng_lightsheet python=3.7.4 -y # creates the anaconda environment\n",
    "- conda activate ng_lightsheet # (or \"source activate ng_lightsheet\", depending on which version of conda you have)\n",
    "- pip install neuroglancer --use-feature=2020-resolver\n",
    "- pip install cloud-volume --use-feature=2020-resolver\n",
    "- pip install SimpleITK --use-feature=2020-resolver\n",
    "\n",
    "\n",
    "\\# To enable you to use jupyter notebooks to work with this environment as a kernel:\n",
    "- pip install --user ipykernel --use-feature=2020-resolver\n",
    "- python -m ipykernel install --user --name=ng_lightsheet\n",
    "\n",
    "Once this is all installed, make sure to select this conda environment as the kernel when running this notebook (you might have to restart jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created /home/ahoag/progs/fiber_tracking_example/20190510_fiber_placement/registered_data_m360_dorsal_up\n",
      "http://127.0.0.1:46840/v/d3a194b2ec12d71b3deafc9028bf385211537b52/main.bundle.js 60601 performance warning: READ-usage buffer was written, then fenced, but written again before being read back. This discarded the shadow copy that was created to accelerate readback.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from cloudvolume import CloudVolume\n",
    "from cloudvolume.lib import mkdir, touch\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import neuroglancer\n",
    "from neuroglancer import webdriver as webd\n",
    "\n",
    "\n",
    "# Point to the registered data volume file. This is the output\n",
    "# from running elastix to register your data to the Paxinos atlas\n",
    "dataset = '20190510_fiber_placement'\n",
    "animal_id = 'm360_dorsal_up'\n",
    "reg_vol_path = os.path.join(\n",
    "    '/jukebox/wang/willmore/lightsheet',\n",
    "    dataset,\n",
    "    animal_id,\n",
    "    'paxinos_registration/elastix/result.1.tif'\n",
    ")\n",
    "\n",
    "# Decide on a folder name where your layer for this volume is going to live. \n",
    "basedir = '/home/ahoag/progs/fiber_tracking_example' # change to somewhere on your filesystem or bucket\n",
    "\n",
    "layer_dir = os.path.join(basedir,dataset,f'registered_data_{animal_id}')\n",
    "# Make the layer directory (mkdir won't overwrite)\n",
    "mkdir(layer_dir)\n",
    "print(f\"created {layer_dir}\")\n",
    "    \n",
    "# Finally, decide how many cpus you are willing and able to use for the parallelized conversion (see step 2)\n",
    "cpus_to_use = 8 # 6 or more should be perfectly fast for the task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Write the instructions (\"info\") file that will tell Neuroglancer about your data volume \n",
    "This info file is a JSON file (looks like a python dictionary but saved in a readable file) and it contains general things about our layer like the shape and physical resolution of the volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_info_file(resolution_xyz,volume_size_xyz,layer_dir):\n",
    "    \"\"\" Make an JSON-formatted file called the \"info\" file\n",
    "    for use with the precomputed data format that Neuroglancer can read.  \n",
    "    --- parameters ---\n",
    "    resolution_xyz:      A tuple representing the size of the pixels (dx,dy,dz) \n",
    "                         in nanometers, e.g. (20000,20000,5000) for 20 micron x 20 micron x 5 micron\n",
    "    \n",
    "    volume_size_xyz:     A tuple representing the number of pixels in each dimension (Nx,Ny,Nz)\n",
    "\n",
    "                         \n",
    "    layer_dir:           The directory where the precomputed data will be\n",
    "                         saved\n",
    "    \"\"\"\n",
    "    info = CloudVolume.create_new_info(\n",
    "        num_channels = 1,\n",
    "        layer_type = 'image', # 'image' here since this layer represents imaging data\n",
    "        data_type = 'uint16', # 32-bit not necessary for data usually. Use smallest possible  \n",
    "        encoding = 'raw', # other options: 'jpeg', 'compressed_segmentation' (req. uint32 or uint64)\n",
    "        resolution = resolution_xyz, # X,Y,Z values in nanometers, 40 microns in each dim\n",
    "        voxel_offset = [ 0, 0, 0 ], # values X,Y,Z values in voxels\n",
    "        chunk_size = [ 1024, 1,1024 ], # rechunk of image X,Y,Z in voxels. We want fast access to y (coronal) planes\n",
    "        volume_size = volume_size_xyz, # X,Y,Z size in voxels\n",
    "    )\n",
    "\n",
    "    vol = CloudVolume(f'file://{layer_dir}', info=info)\n",
    "    vol.provenance.description = \"A test info file\" # can change this if you want a description\n",
    "    vol.provenance.owners = [''] # list of contact email addresses\n",
    "    # Saves the info and provenance files for the first time\n",
    "    vol.commit_info() # generates file://bucket/dataset/layer/info json file\n",
    "    vol.commit_provenance() # generates file://bucket/dataset/layer/provenance json file\n",
    "    print(\"Created CloudVolume info file: \",vol.info_cloudpath)\n",
    "\n",
    "    return vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the info file \n",
    "\n",
    "# The resolution of our registered data volume is governed\n",
    "# by the resolution of the Paxinos atlas, which is 10x100x10 micron resolution,\n",
    "# but we need to specify it in nanometers here\n",
    "resolution_xyz = (10000,100000,10000) # 10x100x10 micron resolution)\n",
    "# Load the registered data volume and get its shape. \n",
    "# This can take a ~10-20 seconds to load\n",
    "reg_vol = np.array(sitk.GetArrayFromImage(\n",
    "    sitk.ReadImage(reg_vol_path)),dtype=np.uint16,order='F')\n",
    "z_dim,y_dim,x_dim = reg_vol.shape\n",
    "volume_size_xyz = (x_dim,y_dim,z_dim)\n",
    "\n",
    "# Write the info file\n",
    "vol = make_info_file(\n",
    "    resolution_xyz=resolution_xyz,\n",
    "    volume_size_xyz=volume_size_xyz,\n",
    "    layer_dir=layer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Convert volume to precomputed data format\n",
    "First we create a directory (the \"progress_dir\") at the same folder level as the layer directory to keep track of the progress of the conversion. \n",
    "All the conversion does is copy the numpy array representing the 3d volume to a new object \"vol\". This is done one plane at a time (although it is parallelized). As each plane is converted, an empty file is created in the progress_dir with the name of the plane. By the end of the conversion, there should be as many files in this progress_dir as there are z planes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created directory: /home/ahoag/progs/fiber_tracking_example/20190510_fiber_placement/progress_registered_data_m360_dorsal_up\n"
     ]
    }
   ],
   "source": [
    "layer_name = layer_dir.split('/')[-1]\n",
    "parent_dir = '/'.join(layer_dir.split('/')[:-1])\n",
    "progress_dir = mkdir(parent_dir+ f'/progress_{layer_name}') # unlike os.mkdir doesn't crash on prexisting \n",
    "print(f\"created directory: {progress_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_slice(y):\n",
    "    \"\"\" This function copies a 2d image slice from the atlas volume\n",
    "    to the cloudvolume object, vol. We will run this in parallel over \n",
    "    all y (coronal) planes\n",
    "    ---parameters---\n",
    "    y:    An integer representing the 0-indexed coronal slice to be converted\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(progress_dir, str(y))):\n",
    "        print(f\"Slice {y} already processed, skipping \")\n",
    "        return\n",
    "    if y >= y_dim: # y is zero indexed and runs from 0-(y_dim-1)\n",
    "        print(\"Index {y} >= y_dim of volume, skipping\")\n",
    "        return\n",
    "    print('Processing slice y=',y)\n",
    "    array = reg_vol[:,y,:].reshape((z_dim,1,x_dim)).T\n",
    "    vol[:,y,:] = array\n",
    "    touch(os.path.join(progress_dir, str(y)))\n",
    "    return \"success\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the conversion in parallel. It's not a huge amount of processing but the more cores the better\n",
    "\n",
    "# First we check to see if there are any planes that have already been converted \n",
    "# by checking the progress dir\n",
    "done_files = set([ int(y) for y in os.listdir(progress_dir) ])\n",
    "all_files = set(range(vol.bounds.minpt.y, vol.bounds.maxpt.y)) \n",
    "\n",
    "to_upload = [ int(y) for y in list(all_files.difference(done_files)) ]\n",
    "to_upload.sort()\n",
    "print(\"Remaining slices to upload are:\",to_upload)\n",
    "with ProcessPoolExecutor(max_workers=cpus_to_use) as executor:\n",
    "    for result in executor.map(process_slice,to_upload):\n",
    "        try:\n",
    "            print(result)\n",
    "        except Exception as exc:\n",
    "            print(f'generated an exception: {exc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Host the precomputed data on your machine so that Neuroglancer can see it\n",
    "This step is really easy! Note: Exectuing the code below will cause your jupyter notebook to hang, so it is better to run the following code in a new ipython terminal or a script (make sure to have the ng_lightsheet conda environment activated in that python session) rather than this notebook. \n",
    "\n",
    "```python\n",
    "from cloudvolume import CloudVolume\n",
    "vol = CloudVolume(f'file://{layer_dir}') # need to set layer_dir to the layer_dir you defined above\n",
    "vol.viewer(port=1338)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: View your registered data and the Paxinos atlas in Neuroglancer\n",
    "Step 3 hosts your data via http on port 1338 of your local machine. To actually view your data in Neuroglancer, there are two ways to do this. You can either load the data in manually in the browser or load it in with Python. The Python method is a lot more convenient and gives you a lot more power. However, it is useful to know how to manually add a layer for debugging the Python method. \n",
    "\n",
    "For the manual method, open up the Braincogs Neuroglancer client: [https://nglancer.pni.princeton.edu](https://nglancer.pni.princeton.edu) (you must be using a Princeton VPN or on campus) in your browser of choice (Chrome preferred) and then click the \"+\" in the upper left hand corner of the screen once the black screen loads. To load in your data, type the following into the source text box:<br>\n",
    "> precomputed://http://localhost:1338 <br>\n",
    "\n",
    "Then hit tab and name your layer if you'd like. Hit enter or the \"add layer\" button and your layer should load into Neuroglancer. Hopefully the labels you added should be showing up in the bottom left when you hover over a region. \n",
    "\n",
    "For the Python method, you can do this by executing the following cells. Make sure you have hosted the data in another Python instance somewhere on your local machine at port 1338, as described in step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set which client you want to use - use the BRAINCOGS client to get the latest features.\n",
    "# Need to be in the Princeton VPN or on campus to use this\n",
    "neuroglancer.set_static_content_source(url='https://nglancer.pni.princeton.edu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8080/sockjs-node/info?t=1604960750686 - Failed to load resource: net::ERR_CONNECTION_REFUSED\n",
      "http://127.0.0.1:46840/v/e23cdb51dadeec13fc85448f7d55b9b3a42698fe/main.bundle.js 36877:8 \"[WDS] Disconnected!\"\n",
      "http://127.0.0.1:46840/favicon.ico - Failed to load resource: the server responded with a status of 404 (Not Found)\n",
      "http://127.0.0.1:46840/v/e23cdb51dadeec13fc85448f7d55b9b3a42698fe/chunk_worker.bundle.js 26235 [WDS] Disconnected!\n",
      "http://127.0.0.1:8080/sockjs-node/info?t=1604960751798 - Failed to load resource: net::ERR_CONNECTION_REFUSED\n",
      "http://127.0.0.1:8080/sockjs-node/info?t=1604960753866 - Failed to load resource: net::ERR_CONNECTION_REFUSED\n",
      "http://127.0.0.1:8080/sockjs-node/info?t=1604960757906 - Failed to load resource: net::ERR_CONNECTION_REFUSED\n",
      "http://127.0.0.1:8080/sockjs-node/info?t=1604960766003 - Failed to load resource: net::ERR_CONNECTION_REFUSED\n",
      "http://127.0.0.1:8080/sockjs-node/info?t=1604960782056 - Failed to load resource: net::ERR_CONNECTION_REFUSED\n",
      "http://127.0.0.1:8080/sockjs-node/info?t=1604960814101 - Failed to load resource: net::ERR_CONNECTION_REFUSED\n",
      "http://127.0.0.1:8080/sockjs-node/info?t=1604960878193 - Failed to load resource: net::ERR_CONNECTION_REFUSED\n"
     ]
    }
   ],
   "source": [
    "# Make a viewer object that represents your connection to a Neuroglancer session\n",
    "viewer = neuroglancer.Viewer()\n",
    "\n",
    "# Load in the layer we just made and the Paxinos mouse atlas \n",
    "# and the layer showing the Paxinos boundaries\n",
    "\n",
    "# This cell should automatically open up a new window showing the \n",
    "# Neuroglancer browser interface with your data loaded in as one layer, the\n",
    "# Paxinos atlas loaded in as a second layer (colored regions),\n",
    "# and the Paxinos atlas boundaries as a third layer\n",
    "with viewer.txn() as s:\n",
    "    s.layers[layer_name] = neuroglancer.ImageLayer(\n",
    "        source='precomputed://http://localhost:1338')\n",
    "    s.layers['Paxinos Mouse Atlas'] = neuroglancer.SegmentationLayer(\n",
    "        source='precomputed://gs://wanglab-pma/kimatlas')\n",
    "    s.layers['Paxinos Boundaries'] = neuroglancer.SegmentationLayer(\n",
    "        source='precomputed://gs://wanglab-pma/kimatlas_boundaries')\n",
    "# First set up something called a webdriver which opens an entirely new window that you have more control over\n",
    "webdriver = webd.Webdriver(viewer, headless=False,) \n",
    "# print(viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error saying something about \"This version of ChromeDriver only supports chrome version X, then go to: https://chromedriver.chromium.org/downloads and download the Chromedriver for your version of Google Chrome and operating system, then put the \"chromedriver\" executable in your python path, then retry running the above cell. \n",
    "\n",
    "If you are in a hurry and don't want to worry about installing this chromedriver thing right now, you can comment out the \"webdriver = webd...\" line above and uncomment the line below it: \"print(viewer)\". Click the link that appears and then proceed from there. Most of the rest of the notebook will run fine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, a new window should open, showing the Neuroglancer page with your data, the Paxinos colored regions and the boundaries already loaded in. First let's make the window bigger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the size of the window that appeared\n",
    "webdriver.driver.set_window_size(1200,800) # comment this out if you are skipping the webdriver portion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the sagittal, coronal and horizontal views are all shown. The coronal sections are all we care about for Paxinos (the atlas is too low resolution to be useful in the other cross sectional views anyway). The following code changes the layout so we only see the coronal view and rotates the brain so it is in the usual orientation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn() as s:\n",
    "    s.layout = 'xz'\n",
    "    s.crossSectionOrientation = [\n",
    "    0,\n",
    "    0.7071067690849304,\n",
    "    0,\n",
    "    0.7071067690849304\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above cell, check the Neuroglancer window and you should see that the change has taken place. Any change you want to make to the viewer must take place inside of one of these \"with viewer.txn() as s\" blocks. \n",
    "\n",
    "You also may only care to show the boundaries. Next, let's turn down the transparency of the colored regions so we can only see the boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8080/sockjs-node/info?t=1604962234082 - Failed to load resource: net::ERR_CONNECTION_REFUSED\n"
     ]
    }
   ],
   "source": [
    "with viewer.txn() as s:\n",
    "    atlas_layer = s.layers['Paxinos Mouse Atlas']\n",
    "    atlas_layer.selected_alpha = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we turned the transparency of this layer all the way down, when we hover over the middle area of a region, we can still see the region name in the layer box at the top of the viewer. If you don't care about this feature you can remove the \"Paxinos Mouse Atlas\" layer entirely and just work only with the boundaries. The name of the region will still appear when you hover over a boundary, but because the boundaries are so thin it can be more convenient to keep this other hidden layer on.  \n",
    "\n",
    "You will notice that the boundaries are still colored. Assuming you want to make these all black or white, you can run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:46840/v/e23cdb51dadeec13fc85448f7d55b9b3a42698fe/main.bundle.js 60601 performance warning: READ-usage buffer was written, then fenced, but written again before being read back. This discarded the shadow copy that was created to accelerate readback.\n",
      "http://127.0.0.1:46840/v/e23cdb51dadeec13fc85448f7d55b9b3a42698fe/main.bundle.js 60601 performance warning: READ-usage buffer was written, then fenced, but written again before being read back. This discarded the shadow copy that was created to accelerate readback.\n",
      "http://127.0.0.1:46840/v/e23cdb51dadeec13fc85448f7d55b9b3a42698fe/main.bundle.js 60601 performance warning: READ-usage buffer was written, then fenced, but written again before being read back. This discarded the shadow copy that was created to accelerate readback.\n"
     ]
    }
   ],
   "source": [
    "with viewer.txn() as s:\n",
    "    s.selectedLayer.layer = 'Paxinos Boundaries'\n",
    "    s.selectedLayer.visible = True\n",
    "option = webdriver.driver.find_element_by_class_name('neuroglancer-segmentation-dropdown-set-segments-black')\n",
    "option.click()\n",
    "# This automatically checks the \"Set all segments black/white\" checkbox in the \"Render\" tab \n",
    "# for the boundaries layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transparency of the boundaries can also be controlled via the Opacity (on) slider or via this example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8080/sockjs-node/info?t=1604961209998 - Failed to load resource: net::ERR_CONNECTION_REFUSED\n"
     ]
    }
   ],
   "source": [
    "with viewer.txn() as s:\n",
    "    boundaries_layer = s.layers['Paxinos Boundaries']\n",
    "    boundaries_layer.selected_alpha = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually, the colormap of your data can be adjusted via the \"d\" and \"f\" keys and the colors can be inverted using the \"i\" key. This is especially helpful for getting a better contrast against black boundaries. These can also be controlled more precisely via code, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8080/sockjs-node/info?t=1604961262341 - Failed to load resource: net::ERR_CONNECTION_REFUSED\n"
     ]
    }
   ],
   "source": [
    "with viewer.txn() as s:\n",
    "    data_layer = s.layers['registered_data_m360_dorsal_up']\n",
    "    data_layer.shader = \"void main() {emitGrayscale(1.0-toNormalized(getDataValue())*40.0);}\"\n",
    "    # The \"1.0-\" part inverts the colormap, and the factor of 40.0 at the end controls the brightness.\n",
    "    # Remove the \"1.0-\" and re-run to restore the colormap to original colors\n",
    "    # Change the \"40.0\" factor to suit whatever works best for you. Values between 10 and 100 typically \n",
    "    # work best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also make your data any color instead of grayscale. See here for more information if interested: https://github.com/google/neuroglancer/blob/831798e8ab91f40cccb53882d2b1e381d7251847/src/neuroglancer/sliceview/image_layer_rendering.md \n",
    "\n",
    "There is a lot more customization of the screen that you can do with Python. Here are some more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off the red/green/blue coordinate axis (\"a\" key in Neuroglancer)\n",
    "with viewer.txn() as s:\n",
    "    s.show_axis_lines = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400.5  61.5 570.5]\n"
     ]
    }
   ],
   "source": [
    "# Print out the coordinates that we are centered on in the viewer \n",
    "# These three numbers are the x,y,z position and are also displayed at the top of the Neuroglancer screen\n",
    "# The numbers you see below might be fractions but they are rounded down to the nearest slice \n",
    "with viewer.txn() as s:\n",
    "    print(s.position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the position by modifying the position variable.\n",
    "# For example let's change which coronal slice we are looking at (the y coordinate)\n",
    "with viewer.txn() as s:\n",
    "    s.position = [400.5,34.0,570.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the zoom level \n",
    "with viewer.txn() as s:\n",
    "    s.cross_section_scale = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn on the sidebar for the selected layer\n",
    "with viewer.txn() as s:\n",
    "    s.selectedLayer.layer = 'Paxinos Boundaries' # change this to the other layer names to select them\n",
    "    s.selectedLayer.visible = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off the sidebar for the selected layer\n",
    "with viewer.txn() as s:\n",
    "    s.selectedLayer.layer = 'Paxinos Boundaries'\n",
    "    s.selectedLayer.visible = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8080/sockjs-node/info?t=1604961774358 - Failed to load resource: net::ERR_CONNECTION_REFUSED\n"
     ]
    }
   ],
   "source": [
    "# Take a high quality screenshot.\n",
    "savename = './test_screenshot.png'\n",
    "webdriver.driver.save_screenshot('./test_screenshot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above worked, it should print \"True\" and you should see a PNG file called test_screenshot in this directory (wherever you are running the jupyter notebook server). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ng",
   "language": "python",
   "name": "ng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
