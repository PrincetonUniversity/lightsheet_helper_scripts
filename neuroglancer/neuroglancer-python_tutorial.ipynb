{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this notebook\n",
    "The goal of this notebook is to introduce you to the Python interface to Neuroglancer. There are several advantages for using the Python interface over using the visualization links generated at the braincogs00.pni.princeton.edu site. The main advantage is that it gives you programmatic control over your Neuroglancer session. This helps for making reproducible figures via the screenshot feature. It also allows you to keep the Neuroglancer session open indefinitely, whereas the links on braincogs00 expire after a few hours of inactivity. If you are making annotations and that takes longer than a single sitting, you could lose your progress if the Neuroglancer session is closed. This notebook prevents that from happening. \n",
    "\n",
    "Here is what we will cover in this notebook:\n",
    "- Start a Neuroglancer session from Python and load in public data \n",
    "- Manipulate the Neuroglancer session from Python\n",
    "- Configure a screenshot reproducibly\n",
    "- Load in your private light-sheet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "In order to run the code in this notebook, you will need a conda environment with python3 and containing some additional libraries. This environment, which I call \"ng\" below but you could call whatever you want, can be set up in the following way:\n",
    "\n",
    "In terminal:\n",
    "- conda create -n ng python=3.8 -y\n",
    "- conda activate ng # (or source activate ng, depending on which version of conda you have)\n",
    "- pip install cloud-volume\n",
    "- pip install neuroglancer <br>\n",
    "\n",
    "\\# To enable you to use jupyter notebooks to work with this environment as a kernel:\n",
    "In terminal:\n",
    "- conda activate ng\n",
    "- pip install --user ipykernel\n",
    "- python -m ipykernel install --user --name=ng\n",
    "\n",
    "Once this is all installed, make sure to select this conda environment as the kernel when running this notebook via Kernel -> Change Kernel (you may have to restart the jupyter notebook server if you just created the conda environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuroglancer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Start a Neuroglancer session from Python and load in public data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set which client you want to use \n",
    "# This uses the BRAINCOGS client to get the latest features.\n",
    "neuroglancer.set_static_content_source(url='https://neuroglancer-braincogs.appspot.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a viewer object that represents your connection to a new Neuroglancer session\n",
    "viewer = neuroglancer.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:40607/v/0bcad0017036c62ced2423238ca5cc998015bdaf/\n"
     ]
    }
   ],
   "source": [
    "# Load in the Allen Mouse Brain Atlas \n",
    "# This is hosted publicly in Google Cloud -- hence the \"gs://\" in the source address below\n",
    "# wanglab-pma is our public Google Cloud bucket where we host the atlases\n",
    " \n",
    "with viewer.txn() as s:\n",
    "    s.layers[\"Allen Mouse Brain Atlas\"] = neuroglancer.SegmentationLayer(\n",
    "        source='precomputed://gs://wanglab-pma/allenatlas_2017',\n",
    "    )\n",
    "print(viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the link above and it should bring you to a Neuroglancer session with the Allen Mouse Brain Brain Atlas loaded. That link will be active for as long as this jupyter notebook session is active. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate the Neuroglancer session from Python\n",
    "Pretty much all of the things you can do in the browser with your mouse and keyboard you can also do programmatically from Python. After each code cell look back at your Neuroglancer session to see the changes that were made. I recommend arranging the windows so you can see this jupyter notebook and the Neuroglancer window simultaneously. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViewerState({\"dimensions\": {\"x\": [2.5e-05, \"m\"], \"y\": [2.5e-05, \"m\"], \"z\": [2.5e-05, \"m\"]}, \"position\": [160.5, 264.5, 228.5], \"crossSectionScale\": 1, \"projectionScale\": 1024, \"layers\": [{\"type\": \"segmentation\", \"source\": \"precomputed://gs://wanglab-pma/allenatlas_2017\", \"name\": \"Allen Mouse Brain Atlas\"}], \"layout\": \"4panel\"})"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the entire configuration of the viewer -- these are all things you can change\n",
    "viewer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom way out\n",
    "with viewer.txn() as s:\n",
    "    s.crossSectionScale = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom back in\n",
    "with viewer.txn() as s:\n",
    "    s.crossSectionScale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the position you are looking at\n",
    "with viewer.txn() as s:\n",
    "    s.position = [135,514,144] # Somewhere in Crus I in the Cerebellum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only look at sagittal planes\n",
    "with viewer.txn() as s:\n",
    "    s.layout = 'xy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate brain\n",
    "with viewer.txn() as s:\n",
    "    s.cross_section_orientation = [0, 0, -np.cos(45*np.pi/180), np.cos(45*np.pi/180)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select segments by ID \n",
    "with viewer.txn() as s:\n",
    "    s.layers[\"Allen Mouse Brain Atlas\"].layer.segments = {512, 91, 1025, 1033, 1041, 153, \n",
    "                                  1049, 1056, 1064, 936, 944, 951,\n",
    "                                  957, 1091, 968, 846, 976,\n",
    "                                  728, 984, 989, 96,\n",
    "                                  101, 744, 1134, 1007} # only the Cerebellum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the right hand control panel and show the selected segments\n",
    "with viewer.txn() as s:\n",
    "    s.selectedLayer.layer = \"Allen Mouse Brain Atlas\"\n",
    "    s.selectedLayer.visible = True\n",
    "    s.layers[\"Allen Mouse Brain Atlas\"].tab = \"segments\"\n",
    "    s.selectedLayer.size=500 # sets the width of the right hand panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrease saturation\n",
    "with viewer.txn() as s:\n",
    "    s.layers[0].saturation = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change color of one of the segments\n",
    "with viewer.txn() as s:\n",
    "    s.layers[\"Allen Mouse Brain Atlas\"].segment_colors[1007] = \"#a83c32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all of the segments shown so they all have the same color\n",
    "with viewer.txn() as s:\n",
    "    s.layers[\"Allen Mouse Brain Atlas\"].equivalences = [(512, 91, 1025, 1033, 1041, 153, \n",
    "                                  1049, 1056, 1064, 936, 944, 951,\n",
    "                                  957, 1091, 968, 846, 976,\n",
    "                                  728, 984, 989, 96,\n",
    "                                  101, 744, 1134, 1007)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undo the merge and reset the segments\n",
    "with viewer.txn() as s:\n",
    "    s.layers[\"Allen Mouse Brain Atlas\"].equivalences = []\n",
    "    s.layers[\"Allen Mouse Brain Atlas\"].layer.segments = {512, 91, 1025, 1033, 1041, 153, \n",
    "                                  1049, 1056, 1064, 936, 944, 951,\n",
    "                                  957, 1091, 968, 846, 976,\n",
    "                                  728, 984, 989, 96,\n",
    "                                  101, 744, 1134, 1007}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle the yellow bounding box, RGB coordinate axes, scale bar\n",
    "with viewer.txn() as s:\n",
    "    s.showDefaultAnnotations = False # turn off yellow bounding box\n",
    "    s.show_axis_lines=False # turn off axes\n",
    "    s.show_scale_bar = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change background color\n",
    "with viewer.txn() as s:\n",
    "    s.cross_section_background_color = \"#ffffff\" # white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring saturation back up\n",
    "with viewer.txn() as s:\n",
    "    s.layers[0].saturation = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide the side panel\n",
    "with viewer.txn() as s:\n",
    "    s.selectedLayer.visible = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in some custom point annotations\n",
    "counter=0\n",
    "annotation_layer_name = 'my_annotation_layer'\n",
    "points = [\n",
    "    [85,488,144,1056], # x,y,z,segment_id\n",
    "    [106, 501, 144,1064],\n",
    "    [137, 516, 144,1025],\n",
    "    [188,500,144,1033],\n",
    "    [142,449,144,728],\n",
    "   \n",
    "]\n",
    "with viewer.txn() as s:\n",
    "    s.layers[annotation_layer_name]=neuroglancer.AnnotationLayer()\n",
    "    # Link the annotation layer to the atlas layer \n",
    "    s.layers[annotation_layer_name].linkedSegmentationLayer['segments'] = \"Allen Mouse Brain Atlas\"\n",
    "    annotations=s.layers[annotation_layer_name].annotations\n",
    "    \n",
    "    for (x,y,z,segid) in points:\n",
    "        pt=neuroglancer.PointAnnotation(point=[x, y, z],id=f'point{counter}',segments=[[segid]])\n",
    "        annotations.append(pt)\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"point\": [85, 488, 144], \"type\": \"point\", \"id\": \"point0\", \"segments\": [[\"1056\"]]}, {\"point\": [106, 501, 144], \"type\": \"point\", \"id\": \"point1\", \"segments\": [[\"1064\"]]}, {\"point\": [137, 516, 144], \"type\": \"point\", \"id\": \"point2\", \"segments\": [[\"1025\"]]}, {\"point\": [188, 500, 144], \"type\": \"point\", \"id\": \"point3\", \"segments\": [[\"1033\"]]}, {\"point\": [142, 449, 144], \"type\": \"point\", \"id\": \"point4\", \"segments\": [[\"728\"]]}]\n"
     ]
    }
   ],
   "source": [
    "# Load in a custom line annotation\n",
    "counter=0\n",
    "annotation_layer_name = 'my_annotation_layer'\n",
    "lines = [\n",
    "    [(62,458,142),(182,490,143)], # (x1,y1,z1),(x2,y2,z2)   \n",
    "]\n",
    "with viewer.txn() as s:\n",
    "\n",
    "    annotations=s.layers[annotation_layer_name].annotations\n",
    "    print(annotations)\n",
    "    for (pointA,pointB) in lines:\n",
    "        pt=neuroglancer.LineAnnotation(pointA=pointA,\n",
    "                                       pointB=pointB,id=f'line{counter}')\n",
    "        annotations.append(pt)\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViewerState({\"dimensions\": {\"x\": [2.5e-05, \"m\"], \"y\": [2.5e-05, \"m\"], \"z\": [2.5e-05, \"m\"]}, \"position\": [135, 514, 144], \"crossSectionOrientation\": [0, 0, -0.7071067690849304, 0.7071067690849304], \"crossSectionScale\": 1, \"projectionScale\": 1024, \"layers\": [{\"type\": \"segmentation\", \"source\": \"precomputed://gs://wanglab-pma/allenatlas_2017\", \"tab\": \"segments\", \"segmentColors\": {\"91\": \"#a83c32\"}, \"segments\": [\"1007\", \"101\", \"1025\", \"1033\", \"1041\", \"1049\", \"1056\", \"1064\", \"1091\", \"1134\", \"153\", \"512\", \"728\", \"744\", \"846\", \"91\", \"936\", \"944\", \"951\", \"957\", \"96\", \"968\", \"976\", \"984\", \"989\"], \"name\": \"Allen Mouse Brain Atlas\"}, {\"type\": \"annotation\", \"source\": {\"url\": \"local://annotations\", \"transform\": {\"outputDimensions\": {\"x\": [2.5e-05, \"m\"], \"y\": [2.5e-05, \"m\"], \"z\": [2.5e-05, \"m\"]}}}, \"annotations\": [{\"point\": [85, 488, 144], \"type\": \"point\", \"id\": \"point0\", \"segments\": [[\"1056\"]]}, {\"point\": [106, 501, 144], \"type\": \"point\", \"id\": \"point1\", \"segments\": [[\"1064\"]]}, {\"point\": [137, 516, 144], \"type\": \"point\", \"id\": \"point2\", \"segments\": [[\"1025\"]]}, {\"point\": [188, 500, 144], \"type\": \"point\", \"id\": \"point3\", \"segments\": [[\"1033\"]]}, {\"point\": [142, 449, 144], \"type\": \"point\", \"id\": \"point4\", \"segments\": [[\"728\"]]}, {\"pointA\": [62, 458, 142], \"pointB\": [182, 490, 143], \"type\": \"line\", \"id\": \"line0\"}], \"linkedSegmentationLayer\": {\"segments\": \"Allen Mouse Brain Atlas\"}, \"name\": \"my_annotation_layer\"}], \"showAxisLines\": false, \"showScaleBar\": false, \"showDefaultAnnotations\": false, \"selectedLayer\": {\"layer\": \"Allen Mouse Brain Atlas\", \"size\": 500}, \"crossSectionBackgroundColor\": \"#ffffff\", \"layout\": \"xy\"})"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show brain regions and points in 3D \n",
    "# Hide the side panel\n",
    "with viewer.txn() as s:\n",
    "    s.layout = \"xy-3d\"\n",
    "    s.showSlices=True\n",
    "    s.projectionOrientation = [-0.2832767367362976, -0.26628902554512024, -0.696902871131897, 0.6026365756988525]\n",
    "    s.projection_scale = 700\n",
    "    s.cross_section_scale = 0.5\n",
    "    s.layers[0].object_alpha=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a screenshot using the webdriver\n",
    "from neuroglancer import webdriver as webd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:40607/favicon.ico - Failed to load resource: the server responded with a status of 404 (Not Found)\n",
      "http://127.0.0.1:40607/v/0bcad0017036c62ced2423238ca5cc998015bdaf/chunk_worker.bundle.js 110 Error retrieving chunk 1134: HttpError: Fetching \"https://www.googleapis.com/storage/v1/b/wanglab-pma/o/allenatlas_2017%2Fmesh_mip_0_err_40%2F1134%3A0?alt=media\" resulted in HTTP error 404.\n"
     ]
    }
   ],
   "source": [
    "# Start the webdriver which should open a new window which clones your viewer\n",
    "webdriver = webd.Webdriver(viewer, headless=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get errors starting the webdriver you may need to do \"pip install selenium\" into your ng conda environment. You may also need to download the \"chromedriver\" program if you are using Google Chrome. https://chromedriver.chromium.org/downloads. Put the chromedriver exectuable in your path and then retry running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdriver.driver.set_window_size(1200,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide the top control bar and layer names\n",
    "with viewer.config_state.txn() as s:\n",
    "    s.show_ui_controls = False\n",
    "#     s.show_panel_borders = True\n",
    "#     s.show_layer_panel = True\n",
    "#     s.show_help_button= True\n",
    "#     s.show_location= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the screenshot of the webdriver window\n",
    "screenshot_filename = './test_screenshot.png'\n",
    "webdriver.driver.save_screenshot(screenshot_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViewerState({\"dimensions\": {\"x\": [2.5e-05, \"m\"], \"y\": [2.5e-05, \"m\"], \"z\": [2.5e-05, \"m\"]}, \"position\": [180.0, 290.0, 30.0], \"crossSectionOrientation\": [0, 0, -0.7071067690849304, 0.7071067690849304], \"crossSectionScale\": 0.8494661543092754, \"projectionOrientation\": [-0.04922858998179436, -0.18042683601379395, -0.7317871451377869, 0.6553703546524048], \"projectionScale\": 700, \"layers\": [{\"type\": \"segmentation\", \"source\": \"precomputed://gs://wanglab-pma/allenatlas_2017\", \"tab\": \"segments\", \"objectAlpha\": 0.2, \"segmentColors\": {\"91\": \"#a83c32\"}, \"segments\": [\"1007\", \"101\", \"1025\", \"1033\", \"1041\", \"1049\", \"1056\", \"1064\", \"1091\", \"1134\", \"153\", \"512\", \"728\", \"744\", \"846\", \"91\", \"936\", \"944\", \"951\", \"957\", \"96\", \"968\", \"976\", \"984\", \"989\"], \"name\": \"Allen Mouse Brain Atlas\"}, {\"type\": \"annotation\", \"source\": {\"url\": \"local://annotations\", \"transform\": {\"outputDimensions\": {\"x\": [2.5e-05, \"m\"], \"y\": [2.5e-05, \"m\"], \"z\": [2.5e-05, \"m\"]}}}, \"annotations\": [{\"point\": [85, 488, 144], \"type\": \"point\", \"id\": \"point0\", \"segments\": [[\"1056\"]]}, {\"point\": [106, 501, 144], \"type\": \"point\", \"id\": \"point1\", \"segments\": [[\"1064\"]]}, {\"point\": [137, 516, 144], \"type\": \"point\", \"id\": \"point2\", \"segments\": [[\"1025\"]]}, {\"point\": [188, 500, 144], \"type\": \"point\", \"id\": \"point3\", \"segments\": [[\"1033\"]]}, {\"point\": [142, 449, 144], \"type\": \"point\", \"id\": \"point4\", \"segments\": [[\"728\"]]}, {\"pointA\": [62, 458, 142], \"pointB\": [182, 490, 143], \"type\": \"line\", \"id\": \"line0\"}], \"linkedSegmentationLayer\": {\"segments\": \"Allen Mouse Brain Atlas\"}, \"name\": \"my_annotation_layer\"}], \"showAxisLines\": false, \"showScaleBar\": false, \"showDefaultAnnotations\": false, \"selectedLayer\": {\"layer\": \"Allen Mouse Brain Atlas\", \"size\": 500}, \"crossSectionBackgroundColor\": \"#ffffff\", \"layout\": \"xy-3d\"})"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a screen \"video\" (sequence of pngs) of moving through the brain sagittal plane by plane\n",
    "counter = 0    \n",
    "for z in range(30,500,10):\n",
    "    screenshot_filename = './screenshot_{}.png'.format(str(counter).zfill(5))\n",
    "#     print(screenshot_filename)\n",
    "    with webdriver.viewer.txn() as s:\n",
    "        s.position=[113,477,z]\n",
    "    webdriver.driver.save_screenshot(screenshot_filename)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make a gif from the sequence of pngs. An easy way to do that with Python is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./screenshot_00000.png', './screenshot_00001.png', './screenshot_00002.png', './screenshot_00003.png', './screenshot_00004.png', './screenshot_00005.png', './screenshot_00006.png', './screenshot_00007.png', './screenshot_00008.png', './screenshot_00009.png', './screenshot_00010.png', './screenshot_00011.png', './screenshot_00012.png', './screenshot_00013.png', './screenshot_00014.png', './screenshot_00015.png', './screenshot_00016.png', './screenshot_00017.png', './screenshot_00018.png', './screenshot_00019.png', './screenshot_00020.png', './screenshot_00021.png', './screenshot_00022.png', './screenshot_00023.png', './screenshot_00024.png', './screenshot_00025.png', './screenshot_00026.png', './screenshot_00027.png', './screenshot_00028.png', './screenshot_00029.png', './screenshot_00030.png', './screenshot_00031.png', './screenshot_00032.png', './screenshot_00033.png', './screenshot_00034.png', './screenshot_00035.png', './screenshot_00036.png', './screenshot_00037.png', './screenshot_00038.png', './screenshot_00039.png', './screenshot_00040.png', './screenshot_00041.png', './screenshot_00042.png', './screenshot_00043.png', './screenshot_00044.png', './screenshot_00045.png', './screenshot_00046.png']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob, os\n",
    " \n",
    "savename = 'atlasmovie.gif'\n",
    "# Create the frames\n",
    "pngs = sorted(glob.glob('./screenshot_00???.png'))\n",
    "print(pngs)\n",
    "frames = [Image.open(png) for png in pngs]\n",
    "\n",
    "# Save into a GIF file that loops forever\n",
    "frames[0].save(savename, format='GIF', \n",
    "               append_images=frames[1:],\n",
    "               save_all=True,\n",
    "               duration=200, # milliseconds\n",
    "               loop=0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should create the following gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"atlasmovie.gif\" width=500px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://neuroglancer-braincogs.appspot.com#!%7B%22dimensions%22:%7B%22x%22:%5B2.5e-05,%22m%22%5D,%22y%22:%5B2.5e-05,%22m%22%5D,%22z%22:%5B2.5e-05,%22m%22%5D%7D,%22position%22:%5B180.0,290.0,490.0%5D,%22crossSectionOrientation%22:%5B0,0,-0.7071067690849304,0.7071067690849304%5D,%22crossSectionScale%22:1,%22projectionOrientation%22:%5B-0.4709406793117523,-0.42826876044273376,-0.575935959815979,0.5129312872886658%5D,%22projectionScale%22:1200,%22layers%22:%5B%7B%22type%22:%22segmentation%22,%22source%22:%22precomputed://gs://wanglab-pma/allenatlas_2017%22,%22tab%22:%22segments%22,%22segmentColors%22:%7B%2291%22:%22#a83c32%22%7D,%22segments%22:%5B%221007%22,%22101%22,%221025%22,%221033%22,%221041%22,%221049%22,%221056%22,%221064%22,%221091%22,%221134%22,%22153%22,%22512%22,%22728%22,%22744%22,%22846%22,%2291%22,%22936%22,%22944%22,%22951%22,%22957%22,%2296%22,%22968%22,%22976%22,%22984%22,%22989%22%5D,%22name%22:%22Allen%20Mouse%20Brain%20Atlas%22%7D,%7B%22type%22:%22annotation%22,%22source%22:%7B%22url%22:%22local://annotations%22,%22transform%22:%7B%22outputDimensions%22:%7B%22x%22:%5B2.5e-05,%22m%22%5D,%22y%22:%5B2.5e-05,%22m%22%5D,%22z%22:%5B2.5e-05,%22m%22%5D%7D%7D%7D,%22annotations%22:%5B%7B%22point%22:%5B85,488,144%5D,%22type%22:%22point%22,%22id%22:%22point0%22,%22segments%22:%5B%5B%221056%22%5D%5D%7D,%7B%22point%22:%5B106,501,144%5D,%22type%22:%22point%22,%22id%22:%22point1%22,%22segments%22:%5B%5B%221064%22%5D%5D%7D,%7B%22point%22:%5B137,516,144%5D,%22type%22:%22point%22,%22id%22:%22point2%22,%22segments%22:%5B%5B%221025%22%5D%5D%7D,%7B%22point%22:%5B188,500,144%5D,%22type%22:%22point%22,%22id%22:%22point3%22,%22segments%22:%5B%5B%221033%22%5D%5D%7D,%7B%22point%22:%5B142,449,144%5D,%22type%22:%22point%22,%22id%22:%22point4%22,%22segments%22:%5B%5B%22728%22%5D%5D%7D%5D,%22linkedSegmentationLayer%22:%7B%22segments%22:%22Allen%20Mouse%20Brain%20Atlas%22%7D,%22name%22:%22my_annotation_layer%22%7D%5D,%22showAxisLines%22:false,%22showScaleBar%22:false,%22showDefaultAnnotations%22:false,%22selectedLayer%22:%7B%22layer%22:%22Allen%20Mouse%20Brain%20Atlas%22,%22size%22:500%7D,%22crossSectionBackgroundColor%22:%22#ffffff%22,%22layout%22:%22xy-3d%22%7D'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a publicly sharable URL with the current viewer state\n",
    "neuroglancer.to_url(viewer.state,prefix=\"https://neuroglancer-braincogs.appspot.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the above link into a new tab in your browser -- the result should be identical to our first link. Note that this is a static link - if you make more changes to the viewer you will have to regenerate the link.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load your private light-sheet data into Neuroglancer\n",
    "So far we have been using a public dataset (the Allen Mouse Brain Atlas) that is hosted in the cloud. To view private data in Neuroglancer (such as your light-sheet data that lives on bucket), you need to host it yourself (or move it to Google Cloud Storage). Fortunately there is a python package called \"cloud-volume\" that makes hosting your private data easy. You should have already installed this into your \"ng\" conda environment as per the instructions at the beginning of the notebook.\n",
    "\n",
    "Before your data can be hosted, they need to have been converted to the \"precomputed\" format. This is part of the light-sheet pipeline that we run for you, so if you have a request at braincogs00.pni.princeton.edu we more than likely have already done this for your data. We store the precomputed data in the \"viz/\" subfolder for each of your samples on bucket. This path can be found for your request/sample under:\n",
    "\n",
    "```python\n",
    "/jukebox/LightSheetData/lightserv/{netid}/{request_name}/{sample_name}/imaging_request_1/viz\n",
    "```\n",
    "Replace {netid}, {request_name}, {sample_name} with your information to find your data.\n",
    "\n",
    "The precomputed data folder for your raw data are stored in a `raw/` subfolder and a `processing_request_1/` subfolder for the other products, like blended and atlas-registered volumes. For example the precomputed layer for blended channel 488 data the filepath would be:\n",
    "\n",
    "```python\n",
    "/jukebox/LightSheetData/lightserv/{netid}/{request_name}/{sample_name}/imaging_request_1/viz/processing_request_1/blended/channel_488/channel488_blended\n",
    "```\n",
    " \n",
    "The precomputed data folders have file structures that looks like this:\n",
    "```\n",
    "├── 14400_14400_2000\n",
    "├── 1800_1800_2000\n",
    "├── 28800_28800_2000\n",
    "├── 3600_3600_2000\n",
    "├── 57600_57600_2000\n",
    "├── 7200_7200_2000\n",
    "├── info\n",
    "└── provenance\n",
    "```\n",
    "The key thing is that there needs to be an `info` file inside the precomputed folder. That is how you know you have found the right folder.\n",
    "\n",
    "If you are not sure whether we have created the precomputed layer for you yet, or you cannot find a folder that has contents like what is shown above, please contact lightservhelper@gmail.com or use the #lightsheet-software slack channel for help. Note that if you know how to use braincogs00.pni.princeton.edu to visualize your data (see: https://braincogs00.pni.princeton.edu/FAQ if you don't), when you generate the Neuroglancer link there is a table that shows the filepaths to your precomputed layers once you have filled out the visualization setup form.\n",
    "\n",
    "Once you have found the path to your precomputed data, you  need to host it. To do this, open up a Python or iPython session outside of this jupyter notebook and run the following code, but change the `layer_dir` to your light sheet data precomputed layer folder on bucket that you found above.  \n",
    "\n",
    "```python\n",
    "from cloudvolume import CloudVolume\n",
    "layer_dir = \"ENTER THE PATH TO YOUR LAYER HERE\"\n",
    "vol = CloudVolume(f'file://{layer_dir}')\n",
    "vol.viewer(port=1338)\n",
    "```\n",
    "\n",
    "**Note:** Make sure to have the ng conda environment activated before running python, otherwise you will get some \"module not found\" errors. You don't want to run this code in this notebook because it will cause it to hang and you won't be able to run the rest of the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your data are hosted you can make a new Neuroglancer python instance and load them in like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a new viewer object that is separate from the one we were working with above \n",
    "viewer = neuroglancer.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dir = \"ENTER THE PATH TO YOUR LAYER HERE\" # use the same path as you used when you hosted the data \n",
    "layer_name = \"MY CHANNEL NAME\" # change this to whatever you want. \n",
    "with viewer.txn() as s:\n",
    "    s.layers[layer_name] = neuroglancer.ImageLayer(\n",
    "        source='precomputed://http://localhost:1338',\n",
    "    )\n",
    "print(viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click that link and it will bring you to Neuroglancer with your data loaded in.\n",
    "\n",
    "You might notice that the contrast is poor. To change that execute this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change image contrast so the data are more visible\n",
    "# This inverts the color map (the 1.0- part) and increases the contrast (the factor of 300). \n",
    "# Use the d and f keys in the browser to update the contrast to your liking\n",
    "with viewer.txn() as s:\n",
    "    imagelayer = s.layers[layer_name]\n",
    "    imagelayer.shader = \"\"\" void main() {emitGrayscale(1.0-toNormalized(getDataValue())*300.0);} \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ng",
   "language": "python",
   "name": "ng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
