{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import neuroglancer as ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Set the neuroglancer client to be one that is hosted locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ng.set_static_content_source(url='http://localhost:8080')\n",
    "ng.set_static_content_source(url='https://neuroglancer-braincogs.appspot.com')\n",
    "# ng.set_static_content_source(url='https://alpine-dogfish-285314.appspot.com')\n",
    "# ng.set_static_content_source(url='https://neuromancer-seung-import.appspot.com')\n",
    "# ng.set_static_content_source(url='https://neuroglancer-demo.appspot.com')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the raw data into neuroglancer and generate the link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host the raw-space allen atlas cloudvolume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to host this with cloudvolume as was done for the raw data above. Use a different port though, e.g.\n",
    "```python\n",
    "import cloudvolume\n",
    "vol = cloudvolume.Cloudvolume('file:///jukebox/LightSheetTransfer/kelly/neuroglancer/201908_cfos/m61468_observ_rawatlas')\n",
    "vol.viewer(port=1338) # must be a different port than the raw data\n",
    "```\n",
    "Again, make sure to do this outside of this jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the atlas\n",
    "Load in a segmentation layer (e.g. the raw-space allen atlas) that is being hosted with cloudvolume using the same viewer object that you already made. If you go back to the neuroglancer window after running this it should be there in a new layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:40741/v/886bc92e23fc9084a963964b94f060ed148b254c/\n"
     ]
    }
   ],
   "source": [
    "viewer = ng.Viewer()\n",
    "with viewer.txn() as s:\n",
    "#     s.layers['seg'] = ng.SegmentationLayer(source='precomputed://gs://wanglab-pma/allenatlas_2017_16bit_hierarch_labels_fillmissing/'\n",
    "#     )\n",
    "    s.layers['cfos'] = ng.ImageLayer(source='precomputed://http://localhost:1337',\n",
    "                                     shader=\"\"\"void main() {emitGrayscale(1.0-toNormalized(getDataValue())*25.0);}\"\"\")\n",
    "print(viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1mm x 1mm x 1mm  subvolume in the center of the brain in which to do the annotations\n",
    "# This code colors the \n",
    "npix = 1 # we only need an empty cube with just a single voxel\n",
    "nm_per_pix = 1000000 # each voxel is 1mm or 1e6 nm\n",
    "# Decide where to put the subvolume in mm offset from the origin (the top, front, left of the brain, typically)\n",
    "# May take some experimenting\n",
    "mm_offset = [3,4,5] # x y z\n",
    "# Don't change below\n",
    "voxel_offset = [int(x*1e6/nm_per_pix) for x in mm_offset]\n",
    "a = np.zeros((npix, npix, npix), dtype=np.uint8)\n",
    "\n",
    "with viewer.txn() as s:\n",
    "    s.layers['training_set_overlay'] = ng.ImageLayer(\n",
    "        source=ng.LocalVolume(\n",
    "            a,\n",
    "            dimensions=ng.CoordinateSpace(\n",
    "                scales=[nm_per_pix, nm_per_pix, nm_per_pix], # 10 micron isotropic\n",
    "                units=['nm', 'nm', 'nm'],\n",
    "                names=['x', 'y', 'z']),\n",
    "            voxel_offset=voxel_offset), # This \n",
    "        shader=\"\"\"\n",
    "void main() {\n",
    "  emitRGB(vec3(toNormalized(getDataValue(0)),\n",
    "               toNormalized(getDataValue(1)),\n",
    "               toNormalized(getDataValue(2))));\n",
    "}\n",
    "\"\"\",\n",
    "    )\n",
    "    # Then pan to the location of this new volume\n",
    "    # To do that, need dimensions of your original data layer\n",
    "    dim_dict = s.dimensions\n",
    "    nm_scale_dict = {}\n",
    "    for coord in ['x','y','z']:\n",
    "        dim_obj = dim_dict[coord]\n",
    "        unit = dim_obj.unit\n",
    "        assert unit == 'm'\n",
    "        nm_scale_dict[coord] = dim_obj.scale*1e9\n",
    "    # find the position of the new subvolume in these units\n",
    "    x_subvol_orig = mm_offset[0]*1e6/nm_scale_dict['x']\n",
    "    y_subvol_orig = mm_offset[1]*1e6/nm_scale_dict['y']\n",
    "    z_subvol_orig = mm_offset[2]*1e6/nm_scale_dict['z']\n",
    "    # Pan/zoom to this location\n",
    "    s.voxel_coordinates = [x_subvol_orig,y_subvol_orig,z_subvol_orig]\n",
    "    s.cross_section_scale=0.5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a square show up in at least one of the 2D panels and in the 3d viewer panel. Right click it in any of the panels to pan to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn() as s:\n",
    "    # Swap layers to make subvolume pop out\n",
    "    s.layers[1],s.layers[0] = s.layers[0],s.layers[1]\n",
    "    s.layers[1].opacity=0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should make the subvolume look something like this against the background image:\n",
    "<img src=\"screenshots/subvolume_popout.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should make it easier to keep track of whether you are in the subvolume while annotating. Adjust the opacity of the original data layer as well as the image contrast (\"d\" and \"f\" keys) to get the best contrast between the subvolume and the background image. You can turn the bounding box of the subvolume off with the \"shift+b\" keystroke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an annotation layer so you can start making annotations\n",
    "with viewer.txn() as s:\n",
    "    s.layers['subvol_annotation']=ng.AnnotationLayer()\n",
    "    annot_layer = s.layers['subvol_annotation']\n",
    "    annot_layer.tool = \"annotatePoint\"\n",
    "    s.cross_section_depth=-80 # the z-depth in microns over which the annotated points will persist. negative sign is necessary for some reason\n",
    "    s.selected_layer.layer = 'subvol_annotation' \n",
    "    s.selected_layer.visible = True\n",
    "with viewer.config_state.txn() as st:\n",
    "    st.status_messages['update'] = 'Start annotating points by ctrl+left clicking'   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViewerState({\"dimensions\": {\"x\": [5e-06, \"m\"], \"y\": [5e-06, \"m\"], \"z\": [1e-05, \"m\"]}, \"position\": [638.9795532226562, 865.268798828125, 506.5], \"crossSectionScale\": 0.1019628058671067, \"crossSectionDepth\": -0.15625, \"projectionScale\": 4096, \"layers\": [{\"type\": \"image\", \"source\": \"python://volume/886bc92e23fc9084a963964b94f060ed148b254c.9c9d903498b482deade4924fbbebad642d72a398\", \"opacity\": 1, \"shader\": \"void main() {emitGrayscale(1.0-toNormalized(getDataValue())*25.0);}\", \"name\": \"training_set_overlay\"}, {\"type\": \"image\", \"source\": \"precomputed://http://localhost:1337\", \"opacity\": 0.6, \"shader\": \"void main() {emitGrayscale(1.0-toNormalized(getDataValue())*25.0);}\", \"name\": \"cfos\"}, {\"type\": \"annotation\", \"source\": {\"url\": \"local://annotations\", \"transform\": {\"outputDimensions\": {\"x\": [5e-06, \"m\"], \"y\": [5e-06, \"m\"], \"z\": [1e-05, \"m\"]}}}, \"tool\": \"annotatePoint\", \"annotations\": [{\"point\": [638.1748046875, 844.96044921875, 505.5], \"type\": \"point\", \"id\": \"8cc043a2eb64d7b40bcae0f0c56f68a791c25e3d\"}], \"name\": \"subvol_annotation\"}], \"showAxisLines\": false, \"showDefaultAnnotations\": false, \"selectedLayer\": {\"layer\": \"subvol_annotation\", \"visible\": true}, \"layout\": \"4panel\", \"selection\": {\"layers\": {\"subvol_annotation\": {\"annotationId\": \"8cc043a2eb64d7b40bcae0f0c56f68a791c25e3d\", \"annotationSource\": 0, \"annotationSubsource\": \"default\"}}}})\n"
     ]
    }
   ],
   "source": [
    "with viewer.txn() as s:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn() as s:\n",
    "    s.cross_section_depth=-80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn() as s:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ng",
   "language": "python",
   "name": "ng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
