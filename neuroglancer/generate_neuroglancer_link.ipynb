{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuroglancer\n",
    "import cloudvolume\n",
    "import pandas as pd, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README\n",
    "This example assumes you have precomputed datasets already made and hosted with cloudvolume.\n",
    "To host the raw data in this example that I already converted to precomputed format, open an ipython terminal and do:\n",
    "```python\n",
    "import cloudvolume\n",
    "vol = cloudvolume.CloudVolume('file:///home/wanglab/Documents/neuroglancer/20170204_tp_bl6_cri_1750r_03/647')\n",
    "vol.viewer(port=1337) # You can choose the port here, but remember it for later\n",
    "```\n",
    "The last command will cause the window to hang -- that is the expected behavior. Do not try to do this in this jupyter notebook as it will cause your session to hang and you won't be able to run any of the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the neuroglancer client you want to use. \n",
    "Default is Seung lab's which should work fine for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuroglancer.set_static_content_source(url='https://neuromancer-seung-import.appspot.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the raw data into neuroglancer and generate the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:39341/v/f814f7b4ec39396554f989ab2673af4d47be283c/\n"
     ]
    }
   ],
   "source": [
    "# This volume handle can be used to notify the viewer that the data has changed.\n",
    "viewer = neuroglancer.Viewer()\n",
    "\n",
    "# Load in a segmentation layer (e.g. the raw-space allen atlas) that is being hosted with cloudvolume\n",
    "with viewer.txn() as s:\n",
    "    s.layers['20170204_tp_bl6_cri_1750r_03'] = neuroglancer.ImageLayer(source='precomputed://http://localhost:1337',\n",
    "    shader = '''\n",
    "    void main() {\n",
    "  float v = toNormalized(getDataValue(0)) * 20.0;\n",
    "  emitRGBA(vec4(v, v, v, v));\n",
    "}\n",
    "'''                                                 \n",
    "    )\n",
    "print(viewer) # Click the link that is generated to see your volume displayed in Neuroglancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host the raw-space allen atlas cloudvolume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to host this with cloudvolume as was done for the raw data above. Use a different port though, e.g.\n",
    "```python\n",
    "import cloudvolume\n",
    "vol = cloudvolume.CloudVolume('file:///home/wanglab/Documents/neuroglancer/20170204_tp_bl6_cri_1750r_03/atlas')\n",
    "vol.viewer(port=1338) # must be a different port than the raw data\n",
    "```\n",
    "Again, make sure to do this outside of this jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the atlas\n",
    "Load in a segmentation layer (e.g. the raw-space allen atlas) that is being hosted with cloudvolume using the same viewer object that you already made. If you go back to the neuroglancer window after running this it should be there in a new layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn() as s:\n",
    "    s.layers['rawatlas'] = neuroglancer.SegmentationLayer(source='precomputed://http://localhost:1338'\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the look-up table to make a key binding to the Neuroglancer session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got my-action 1\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 1078, \"rawatlas\": 655})\n",
      "Got my-action 2\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 1278, \"rawatlas\": 663})\n",
      "Got my-action 3\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 1957, \"rawatlas\": 403})\n",
      "Got my-action 4\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 614, \"rawatlas\": 1105})\n",
      "Got my-action 5\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 604, \"rawatlas\": 477})\n",
      "Got my-action 6\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 443, \"rawatlas\": 559})\n",
      "Got my-action 7\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 432, \"rawatlas\": 551})\n",
      "Got my-action 8\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 411, \"rawatlas\": 544})\n",
      "Got my-action 9\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 426, \"rawatlas\": 131})\n",
      "Got my-action 10\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 906, \"rawatlas\": 961})\n",
      "Got my-action 11\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 1357, \"rawatlas\": 788})\n",
      "Got my-action 12\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 622, \"rawatlas\": 966})\n",
      "Got my-action 13\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 700, \"rawatlas\": 403})\n",
      "Got my-action 14\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 1385, \"rawatlas\": 663})\n",
      "Got my-action 15\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 1424, \"rawatlas\": 655})\n",
      "Got my-action 16\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 349, \"rawatlas\": 12349})\n",
      "Got my-action 17\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 353, \"rawatlas\": 889})\n",
      "Got my-action 18\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 301, \"rawatlas\": 1038})\n",
      "Got my-action 19\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 291, \"rawatlas\": 12354})\n",
      "Got my-action 20\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 282, \"rawatlas\": 12353})\n",
      "Got my-action 21\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 1739, \"rawatlas\": 945})\n",
      "Got my-action 22\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 427, \"rawatlas\": 882})\n",
      "Got my-action 23\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 482, \"rawatlas\": 844})\n",
      "Got my-action 24\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 6577, \"rawatlas\": 1020})\n",
      "Got my-action 25\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 319, \"rawatlas\": 733})\n",
      "Got my-action 26\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 7104, \"rawatlas\": 262})\n",
      "Got my-action 27\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 363, \"rawatlas\": 718})\n",
      "Got my-action 28\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 1782, \"rawatlas\": 1020})\n",
      "Got my-action 29\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 301, \"rawatlas\": 362})\n",
      "Got my-action 30\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 2456, \"rawatlas\": 989})\n",
      "Got my-action 31\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 721, \"rawatlas\": 91})\n",
      "Got my-action 32\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 2875, \"rawatlas\": 262})\n",
      "Got my-action 33\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 492, \"rawatlas\": 303})\n",
      "Got my-action 34\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 569, \"rawatlas\": 311})\n",
      "Got my-action 35\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 609, \"rawatlas\": 334})\n",
      "Got my-action 36\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 495, \"rawatlas\": 303})\n",
      "Got my-action 37\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 1370, \"rawatlas\": 403})\n",
      "Got my-action 38\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 526, \"rawatlas\": 559})\n",
      "Got my-action 39\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 900, \"rawatlas\": 403})\n",
      "Got my-action 40\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 1050, \"rawatlas\": 194})\n",
      "Got my-action 41\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 889, \"rawatlas\": 436})\n",
      "Got my-action 42\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 14294, \"rawatlas\": 194})\n",
      "Got my-action 43\n",
      "  Layer selected values: Map({\"20170204_tp_bl6_cri_1750r_03\": 12600, \"rawatlas\": 88})\n"
     ]
    }
   ],
   "source": [
    "# First you need to create a dictionary mapping the atlas id to the region name. \n",
    "# Here's my example\n",
    "\n",
    "csv_file = '/jukebox/LightSheetTransfer/atlas/ls_id_table_w_voxelcounts_16bit.xlsx'\n",
    "\n",
    "df = pd.read_excel(csv_file)\n",
    "\n",
    "atlas_df = df.copy()\n",
    "\n",
    "atlas_dict = {}\n",
    "for ind, row in atlas_df.iterrows():\n",
    "    atlas_dict[int(row['id'])] = row['name']\n",
    "    \n",
    "# Here is the actual code for making the key binding. Copy and paste this\n",
    "# into your jupyter notebook where you have already made the viewer() object\n",
    "\n",
    "num_actions = 0\n",
    "def my_action3(s):\n",
    "    global num_actions\n",
    "    num_actions += 1\n",
    "    with viewer.config_state.txn() as st:\n",
    "        region_id = s.selected_values['rawatlas']\n",
    "        region_name = atlas_dict.get(region_id)\n",
    "        st.status_messages['hello'] = ('%i:%s' %\n",
    "                                    (region_id, region_name))\n",
    "\n",
    "    print('Got my-action %i' % num_actions)\n",
    "#     print('  Mouse position: %s' % (s.mouse_voxel_coordinates,))\n",
    "    print('  Layer selected values: %s' % (s.selected_values,))\n",
    "viewer.actions.add('my-action3', my_action3)\n",
    "\n",
    "with viewer.config_state.txn() as s:\n",
    "    s.input_event_bindings.viewer['keyp'] = 'my-action3'\n",
    "    s.status_messages['hello'] = 'Welcome to the segment labeling example. Press \"p\" to see region name under cursor.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the viewer and take a screenshot. Good for videos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdst = '/home/wanglab/Documents/neuroglancer/screenshots/20161205_tp_bl6_lob45_1000r_01/amygdala'\n",
    "if not os.path.exists(svdst): os.mkdir(svdst)\n",
    "for i in range(443,763):\n",
    "    with viewer.txn() as s:\n",
    "        s.voxel_coordinates = [4468,3424,i]\n",
    "    ss = neuroglancer.ScreenshotSaver(viewer, svdst)\n",
    "    ss.capture(index=i)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the view layout to show the segmentation side by side with the image, rather than overlayed.  This can also be done from the UI by dragging and dropping.  The side by side views by default have synchronized position, orientation, and zoom level, but this can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn() as s:\n",
    "    s.layout = neuroglancer.row_layout(\n",
    "        [neuroglancer.LayerGroupViewer(layers=['20161205_tp_bl6_lob45_1000r_01', 'overlay']),\n",
    "         neuroglancer.LayerGroupViewer(layers=['rawatlas'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the overlay layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn() as s:\n",
    "    s.layout = neuroglancer.row_layout(\n",
    "        [neuroglancer.LayerGroupViewer(layers=['20161205_tp_bl6_lob45_1000r_01']),\n",
    "         neuroglancer.LayerGroupViewer(layers=['rawatlas'])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lspy35] *",
   "language": "python",
   "name": "conda-env-lspy35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
