{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,struct\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import json, glob\n",
    "import datajoint as dj\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor,ThreadPoolExecutor\n",
    "from brain_atlas_toolkit import graph_tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in all neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_dir = '/home/ahoag/progs/mouselight/public/json30'\n",
    "json_files = sorted(glob.glob(neuron_dir + '/*json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write all neuron skeletons to same directory\n",
    "This will reduce the number of layers created when someone wants to view multiple neurons. The only thing we'll have to worry about here is making it clear what integer ids correspond to what neurons. Can just use 0,1,2 for soma, axon, dendrite of first neuron, 3,4,5 for soma, axon and dendrite of second neuron and so on. Use segment properties to keep all of this straight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_skeletons_singledir(neuron,ID_soma, debug=False):\n",
    "    \"\"\" Given a neuron dictionary and the segment ID for its soma,\n",
    "    create skeletons for the soma, axon and dendrite and save them in the \n",
    "    public Princeton neuroglancer bucket\"\"\"\n",
    "    \n",
    "    # Make directory if it does not exist already\n",
    "    precomputed_dir = \"/jukebox/LightSheetData/neuroglancer/public/mouselight/allen_ccfv3_25um/ccfv3_25um_mouselight\"\n",
    "    \n",
    "    ### Soma\n",
    "    coordinates_soma = []\n",
    "    soma = neuron['soma']\n",
    "    \n",
    "    # Downsample to 25 micron / voxels and swap axes to conform with our Allen Atlas\n",
    "    newx = soma['y']/10/2.5\n",
    "    newy = soma['x']/10/2.5\n",
    "    newz = soma['z']/10/2.5 \n",
    "    coordinates_soma= [[newx,newy,newz]]\n",
    "    vertex_positions_soma = np.array(coordinates_soma,dtype='<f4')\n",
    "    edges_soma = np.array([],dtype='<u4')\n",
    "    \n",
    "    filename_soma = os.path.join(precomputed_dir,str(ID_soma))\n",
    "    with open(filename_soma,'wb') as outfile:\n",
    "        buf = struct.pack('<II', vertex_positions_soma.shape[0], edges_soma.shape[0])\n",
    "        buf += vertex_positions_soma.tobytes()\n",
    "        buf += edges_soma.tobytes()\n",
    "        outfile.write(buf)\n",
    "    if debug:\n",
    "        print(\"Wrote \",filename_soma)\n",
    "    \n",
    "    ### Axon\n",
    "    ID_axon = ID_soma + 1\n",
    "    axon = neuron['axon']\n",
    "    \n",
    "    coordinates_axon = []\n",
    "    edges_axon = []\n",
    "    axon_branches_dict = {}\n",
    "    axon_terminals_dict = {}\n",
    "    for pt_index,pt in enumerate(axon):\n",
    "       # Downsample to to 25 micron / voxels and swap their axes\n",
    "        newx = pt['y']/10/2.5\n",
    "        newy = pt['x']/10/2.5\n",
    "        newz = pt['z']/10/2.5 \n",
    "        sample_number = pt['sampleNumber']-1\n",
    "        parent_number = pt['parentNumber']-1\n",
    "        coordinates_axon.append([newx,newy,newz])\n",
    "        if parent_number >= 0:\n",
    "            edges_axon.append([parent_number,sample_number])\n",
    "            # Have to deal with axon splits and terminals\n",
    "            if (sample_number != parent_number + 1):\n",
    "                # then the parent is at a branch split and child is at an endpoint\n",
    "                atlas_id = pt['allenId']\n",
    "                atlas_id_parent = axon[pt_index-1]['allenId']\n",
    "                try:\n",
    "                    axon_branches_dict[atlas_id_parent] +=1\n",
    "                except: \n",
    "                    axon_branches_dict[atlas_id_parent] = 1\n",
    "                try:\n",
    "                    axon_terminals_dict[atlas_id] += 1\n",
    "                except:\n",
    "                    axon_terminals_dict[atlas_id] = 1\n",
    "\n",
    "    vertex_positions_axon = np.array(coordinates_axon,dtype='<f4')\n",
    "    edges_axon = np.array(edges_axon,dtype='<u4')\n",
    "    \n",
    "    # Encode into binary skeleton file\n",
    "    filename_axon = os.path.join(precomputed_dir,str(ID_axon))\n",
    "    with open(filename_axon,'wb') as outfile:\n",
    "        buf = struct.pack('<II', vertex_positions_axon.shape[0], edges_axon.shape[0])\n",
    "        buf += vertex_positions_axon.tobytes()\n",
    "        buf += edges_axon.tobytes()\n",
    "        outfile.write(buf)\n",
    "    if debug:\n",
    "        print(\"Wrote \",filename_axon)\n",
    "\n",
    "    ### Dendrite\n",
    "    ID_dendrite = ID_soma + 2\n",
    "    dendrite = neuron['dendrite']\n",
    "    \n",
    "    coordinates_dendrite = []\n",
    "    edges_dendrite = []\n",
    "    dendrite_branches_dict = {}\n",
    "    dendrite_terminals_dict = {}\n",
    "    for pt_index,pt in enumerate(dendrite):\n",
    "       # Downsample to to 25 micron / voxels and swap their axes\n",
    "        newx = pt['y']/10/2.5\n",
    "        newy = pt['x']/10/2.5\n",
    "        newz = pt['z']/10/2.5 \n",
    "        coordinates_dendrite.append([newx,newy,newz])\n",
    "        sample_number = pt['sampleNumber']-1\n",
    "        parent_number = pt['parentNumber']-1\n",
    "        if parent_number >= 0:\n",
    "            edges_dendrite.append([parent_number,sample_number])\n",
    "            if (sample_number != parent_number + 1):\n",
    "                # then the parent is at a branch split and child is at an endpoint\n",
    "                atlas_id = pt['allenId']\n",
    "                atlas_id_parent = dendrite[pt_index-1]['allenId']\n",
    "                try:\n",
    "                    dendrite_branches_dict[atlas_id_parent] +=1\n",
    "                except: \n",
    "                    dendrite_branches_dict[atlas_id_parent] = 1\n",
    "                try:\n",
    "                    dendrite_terminals_dict[atlas_id] += 1\n",
    "                except:\n",
    "                    dendrite_terminals_dict[atlas_id] = 1\n",
    "\n",
    "    vertex_positions_dendrite = np.array(coordinates_dendrite,dtype='<f4')\n",
    "    edges_dendrite = np.array(edges_dendrite,dtype='<u4')\n",
    "    \n",
    "    # Encode into binary skeleton file\n",
    "\n",
    "    filename_dendrite = os.path.join(precomputed_dir,str(ID_dendrite))\n",
    "    with open(filename_dendrite,'wb') as outfile:\n",
    "        buf = struct.pack('<II', vertex_positions_dendrite.shape[0], edges_dendrite.shape[0])\n",
    "        buf += vertex_positions_dendrite.tobytes()\n",
    "        buf += edges_dendrite.tobytes()\n",
    "        outfile.write(buf)\n",
    "    if debug:\n",
    "        print(\"Wrote \",filename_dendrite)\n",
    "    \n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_singledir(neuron_json_file):\n",
    "    \"\"\" Function that will be run in parallel to write the skeleton files for a single neuron\"\"\"\n",
    "    with open(neuron_json_file,'r') as infile:\n",
    "        neuron_data = json.load(infile)\n",
    "    neuron = neuron_data['neuron']\n",
    "    ID_soma = json_files.index(neuron_json_file)*3\n",
    "    print(\"ID:\")\n",
    "    print(ID_soma)\n",
    "    return write_skeletons_singledir(neuron,ID_soma,debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:ID:ID:\n",
      "\n",
      "2127\n",
      "\n",
      "\n",
      "15\n",
      "ID:ID:\n",
      "ID:\n",
      "012\n",
      "ID:\n",
      "\n",
      "ID:18\n",
      "\n",
      "\n",
      "333\n",
      "\n",
      "ID:\n",
      "30\n",
      "ID:ID:\n",
      "\n",
      "246\n",
      "\n",
      "ID:\n",
      "9\n",
      "ID:\n",
      "36\n",
      "ID:\n",
      "39\n",
      "ID:\n",
      "42\n",
      "True\n",
      "ID:\n",
      "45\n",
      "ID:\n",
      "48\n",
      "ID:\n",
      "51\n",
      "ID:\n",
      "54\n",
      "ID:\n",
      "57\n",
      "ID:\n",
      "60\n",
      "ID:\n",
      "63\n",
      "ID:\n",
      "66\n",
      "ID:\n",
      "69\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "ID:\n",
      "72\n",
      "ID:\n",
      "75\n",
      "True\n",
      "ID:\n",
      "78\n",
      "ID:\n",
      "81\n",
      "ID:\n",
      "84\n",
      "True\n",
      "True\n",
      "True\n",
      "ID:\n",
      "87\n",
      "ID:\n",
      "90\n",
      "ID:\n",
      "93\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "ID:\n",
      "96\n",
      "ID:\n",
      "99\n",
      "True\n",
      "True\n",
      "ID:\n",
      "102\n",
      "ID:\n",
      "105\n",
      "True\n",
      "True\n",
      "ID:\n",
      "108\n",
      "ID:\n",
      "111\n",
      "ID:\n",
      "114\n",
      "True\n",
      "ID:\n",
      "117\n",
      "True\n",
      "ID:\n",
      "120\n",
      "ID:\n",
      "123\n",
      "True\n",
      "True\n",
      "ID:\n",
      "126\n",
      "ID:\n",
      "129\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "ID:\n",
      "132\n",
      "True\n",
      "ID:\n",
      "135\n",
      "ID:\n",
      "138\n",
      "True\n",
      "True\n",
      "ID:\n",
      "141\n",
      "True\n",
      "ID:\n",
      "144\n",
      "True\n",
      "ID:\n",
      "147\n",
      "True\n",
      "ID:\n",
      "150\n",
      "ID:\n",
      "153\n",
      "True\n",
      "True\n",
      "ID:\n",
      "156\n",
      "ID:\n",
      "159\n",
      "ID:\n",
      "162\n",
      "ID:\n",
      "165\n",
      "ID:\n",
      "168\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "ID:\n",
      "171\n",
      "ID:\n",
      "174\n",
      "True\n",
      "True\n",
      "ID:\n",
      "177\n",
      "True\n",
      "ID:\n",
      "180\n",
      "True\n",
      "ID:\n",
      "183\n",
      "ID:\n",
      "186\n",
      "True\n",
      "True\n",
      "ID:\n",
      "189\n",
      "ID:\n",
      "192\n",
      "ID:\n",
      "195\n",
      "True\n",
      "True\n",
      "True\n",
      "ID:\n",
      "198\n",
      "ID:\n",
      "201\n",
      "True\n",
      "ID:\n",
      "204\n",
      "True\n",
      "True\n",
      "ID:\n",
      "207\n",
      "ID:\n",
      "210\n",
      "True\n",
      "ID:\n",
      "213\n",
      "True\n",
      "True\n",
      "ID:\n",
      "216\n",
      "True\n",
      "ID:\n",
      "219\n",
      "True\n",
      "ID:\n",
      "222\n",
      "ID:\n",
      "225\n",
      "ID:\n",
      "228\n",
      "True\n",
      "True\n",
      "ID:\n",
      "231\n",
      "ID:\n",
      "234\n",
      "True\n",
      "True\n",
      "True\n",
      "ID:\n",
      "237\n",
      "ID:\n",
      "240\n",
      "True\n",
      "True\n",
      "ID:\n",
      "243\n",
      "True\n",
      "ID:\n",
      "246\n",
      "True\n",
      "ID:\n",
      "249\n",
      "True\n",
      "ID:\n",
      "252\n",
      "ID:\n",
      "255\n",
      "ID:\n",
      "258\n",
      "True\n",
      "ID:\n",
      "261\n",
      "ID:\n",
      "264\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "ID:\n",
      "267\n",
      "ID:\n",
      "270\n",
      "ID:\n",
      "273\n",
      "True\n",
      "True\n",
      "ID:\n",
      "276\n",
      "ID:\n",
      "279\n",
      "ID:\n",
      "282\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "ID:\n",
      "285\n",
      "True\n",
      "ID:\n",
      "288\n",
      "ID:\n",
      "291\n",
      "True\n",
      "ID:\n",
      "294\n",
      "ID:\n",
      "297\n",
      "ID:\n",
      "300\n",
      "True\n",
      "True\n",
      "ID:\n",
      "303\n",
      "ID:\n",
      "306\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "ID:\n",
      "309\n",
      "ID:\n",
      "312\n",
      "True\n",
      "ID:\n",
      "315\n",
      "True\n",
      "True\n",
      "ID:\n",
      "318\n",
      "ID:\n",
      "321\n",
      "True\n",
      "True\n",
      "ID:\n",
      "324\n",
      "ID:\n",
      "327\n",
      "ID:\n",
      "330\n",
      "ID:\n",
      "333\n",
      "ID:\n",
      "336\n",
      "ID:\n",
      "339\n",
      "ID:\n",
      "342\n",
      "ID:\n",
      "345\n",
      "ID:\n",
      "348\n",
      "ID:\n",
      "351\n",
      "ID:\n",
      "354\n",
      "ID:\n",
      "357\n",
      "ID:\n",
      "360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-35:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahoag/anaconda3/envs/djversion0p13/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ahoag/anaconda3/envs/djversion0p13/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ahoag/anaconda3/envs/djversion0p13/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/ahoag/anaconda3/envs/djversion0p13/lib/python3.8/multiprocessing/queues.py\", line 97, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/home/ahoag/anaconda3/envs/djversion0p13/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ahoag/anaconda3/envs/djversion0p13/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ahoag/anaconda3/envs/djversion0p13/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/djversion0p13/lib/python3.8/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \"\"\"\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/djversion0p13/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/djversion0p13/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/djversion0p13/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/djversion0p13/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/djversion0p13/lib/python3.8/concurrent/futures/process.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue_management_thread_wakeup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue_management_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0;31m# To reduce the risk of opening too many files, remove references to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;31m# objects that use file descriptors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/djversion0p13/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/djversion0p13/lib/python3.8/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with ProcessPoolExecutor(max_workers=12) as executor:\n",
    "    for res in executor.map(process_singledir,json_files):\n",
    "        try:\n",
    "            print(res)\n",
    "        except Exception as exc:\n",
    "            print(f'generated an exception: {exc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /jukebox/LightSheetData/neuroglancer/public/mouselight/all_neurons/segment_properties/info\n",
      "Saved  /jukebox/LightSheetData/neuroglancer/public/mouselight/all_neurons/info\n"
     ]
    }
   ],
   "source": [
    "### Segment properties\n",
    "# Save segment properties \n",
    "segment_props_neuron = {}\n",
    "segment_props_neuron['@type'] = 'neuroglancer_segment_properties'\n",
    "segment_props_neuron['inline'] = {\n",
    "    'ids': [],\n",
    "    'properties':[\n",
    "        {\n",
    "            'id':'label',\n",
    "            'type':'label',\n",
    "            'values': []\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "skeleton_ids = [str(x) for x in range(len(json_files)*3)]\n",
    "neuron_names = [os.path.basename(x).split('.')[0] for x in json_files]\n",
    "skeleton_names = [name + ': ' + feature for name in neuron_names for feature in ['soma','axon','dendrite']]\n",
    "\n",
    "segment_props_neuron['inline']['ids'] = skeleton_ids\n",
    "segment_props_neuron['inline']['properties'][0]['values'] = skeleton_names \n",
    "\n",
    "segment_props_neuron_dir = '/jukebox/LightSheetData/neuroglancer/public/mouselight/allen_ccfv3_25um/ccfv3_25um_mouselight/segment_properties'\n",
    "os.makedirs(segment_props_neuron_dir,exist_ok=True)\n",
    "\n",
    "segment_props_neuron_filename = os.path.join(segment_props_neuron_dir,'info')\n",
    "with open(segment_props_neuron_filename,'w') as outfile:\n",
    "    json.dump(segment_props_neuron,outfile,indent=2)\n",
    "print(f\"Saved {segment_props_neuron_filename}\")\n",
    "\n",
    "# Save info file\n",
    "info = {\n",
    "    \"@type\": \"neuroglancer_skeletons\",\n",
    "    \"transform\": [25000, 0.0, 0.0, 0.0, 0.0, 25000, 0.0, 0.0, 0.0, 0.0, 25000, 0.0],\n",
    "    \"vertex_attributes\":[],\n",
    "    \"segment_properties\":\"segment_properties\"\n",
    "}\n",
    "info_filename = '/jukebox/LightSheetData/neuroglancer/public/mouselight/all_neurons/info'\n",
    "with open(info_filename,'w') as outfile:\n",
    "    json.dump(info,outfile,indent=2)\n",
    "\n",
    "print(\"Saved \",info_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "djversion0p13",
   "language": "python",
   "name": "djversion0p13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
