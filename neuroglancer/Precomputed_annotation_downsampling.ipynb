{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precomputed_annotation_downsampling\n",
    "\n",
    "The purpose of this notebook is to provide an implementation for creating multiple spatial indexes for a precomputed annotation layer. This is useful when the number of annotations reaches > 10^5 because at that point using only a single spatial index (see: [Precomputed_annotation_format.ipynb](Precomputed_annotation_format.ipynb)) results in poor performance in Neuroglancer. \n",
    "\n",
    "For a better understanding of precomputed annotation layers and how to create them, refer to this document: https://github.com/google/neuroglancer/blob/master/src/neuroglancer/datasource/precomputed/annotations.md in particular the section: https://github.com/google/neuroglancer/blob/master/src/neuroglancer/datasource/precomputed/annotations.md#spatial-index for how spatial indexing works.\n",
    "\n",
    "Spatial indexing provides a way to split up annotation volumes into chunks (a.k.a. \"cells\") at multiple resolution levels. In this notebook, we will create a spatially-indexed precomputed annotation layer from ~10^6 object coordinates within a volume of shape: [2160,2560,687]. This volume represents a whole mouse brain imaged at 5x5x10 microns (x,y,z). Each spatial index level contains smaller chunks of the volume than the previous level. Level 0 consists of a single chunk the size of the whole volume. At each successive level, the chunk size is decreased either by a factor of 2 or 1 along each dimension depending on whether that would make the chunk size more or less isotropic. The first few levels of the downsampling scheme for an example volume are shown in the figure below. After level 2, each dimension can be reduced by a factor of 2 since x,y, and z chunk sizes are comparable. The actual implementation of this is in the function `calculate_factors()` below. If your volume requires a different downsampling scheme than this to achieve chunk size isotropy, then modify that function. \n",
    "\n",
    "The spatial index levels are created until there are no more coordinates to fill up the cells at the next level.\n",
    "\n",
    "This implementation does not currently allow writing annotation properties, but such an extension would be straightforward by modifying the function `save_cellfile()` to write out the properties in addition to the coordinates. Some other minor modifications may be necessary to ensure that the properties are subsetted along with the coordinates throughout the main() function and the utility functions (see below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/Annotation_volume_chunking_figure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to widen the notebook cells to full screen width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import struct\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the raw-space cell coordinates file and load it in - this can take ~10 seconds if you are not on campus\n",
    "animal_id = 5\n",
    "pth=os.path.join('/jukebox/wang/Jess/lightsheet_output',\n",
    "        '201904_ymaze_cfos','processed',f'an{animal_id}','clearmap_cluster_output',\n",
    "        'cells.npy')\n",
    "converted_points = np.load(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1388798"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(converted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1388798, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "unique_coordinates = np.unique(converted_points,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1388798, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_coordinates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our case there were no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the order so that when we subsample we get a representative subset\n",
    "np.random.shuffle(unique_coordinates) # shuffles in place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_factors(level):\n",
    "    \"\"\" \n",
    "    ---PURPOSE---\n",
    "    Calculate the downsampling factor to apply to the grid_shape/chunk size at a given spatial index level.\n",
    "    This is chosen to make the chunks as isotropic as possible, change as needed for your volume\n",
    "    ---INPUT---\n",
    "    level     - 0-indexed integer representing the spatial index level\n",
    "    ---OUTPUT---\n",
    "    d[level]  - The downsampling factor to apply to the level to get to the next level\n",
    "    \"\"\"\n",
    "    # \n",
    "    d = {}\n",
    "    d[0] = [1,1,1]\n",
    "    d[1] = [2,2,1]\n",
    "    d[2] = [2,2,1]\n",
    "    for i in range(3,20):\n",
    "        d[i] = [2,2,2]\n",
    "    return d[level]\n",
    "\n",
    "def make_cells(grid_shape):\n",
    "    \"\"\" \n",
    "    ---PURPOSE---\n",
    "    Make a list of grid cells e.g. [\"0_0_0\",\"1_0_0\", ...] given a grid shape\n",
    "    ---INPUT---\n",
    "    grid_shape  - number of cells at a given level in each coordinate as a list,\n",
    "                  e.g. [4,4,2] means 4x4x2 grid in x,y,z\n",
    "    ---OUTPUT---\n",
    "    cells       - A list of strings representing the cells, \n",
    "                  e.g. ['0_0_0', '0_1_0', '1_0_0', '1_1_0']\n",
    "    \"\"\"\n",
    "    cells = []\n",
    "    for x in range(grid_shape[0]):\n",
    "        for y in range(grid_shape[1]):\n",
    "            for z in range(grid_shape[2]):\n",
    "                cell = f\"{x}_{y}_{z}\"\n",
    "                cells.append(cell)\n",
    "    return cells\n",
    "\n",
    "def get_child_cells(cell,factor):\n",
    "    \"\"\" \n",
    "    ---PURPOSE---\n",
    "    Given a cell string e.g. 1_2_3 and a downsampling factor, e.g. [2,2,1]\n",
    "    figure out all of the child cells of this cell in the next spatial index level \n",
    "    ---INPUT---\n",
    "    grid_shape  - number of cells at a given level in each coordinate as a list,\n",
    "                  e.g. [4,4,2] means 4x4x2 grid in x,y,z\n",
    "    ---OUTPUT---\n",
    "    cells       - A list of strings representing the cells, \n",
    "                  e.g. ['0_0_0', '0_1_0', '1_0_0', '1_1_0']\n",
    "    \"\"\"\n",
    "   \n",
    "    child_cells = []\n",
    "    xcell,ycell,zcell = [int(x) for x in cell.split('_')] # n,m,p\n",
    "    xfactor,yfactor,zfactor = factor # x,y,z\n",
    "    for xf in range(0,xfactor):\n",
    "        x_child = xcell*xfactor + xf\n",
    "        for yf in range(0,yfactor):\n",
    "            y_child = ycell*yfactor + yf\n",
    "            for zf in range(0,zfactor):\n",
    "                z_child = zcell*zfactor + zf\n",
    "                child_cell = f\"{x_child}_{y_child}_{z_child}\"\n",
    "                child_cells.append(child_cell)\n",
    "    return child_cells\n",
    "\n",
    "def save_cellfile(level,cell,coordinates,debug=False):\n",
    "    \"\"\" \n",
    "    ---PURPOSE---\n",
    "    Save the binary spatially indexed grid cell file,\n",
    "    e.g. if level=1 and cell=\"1_1_0\", then the file will be: spatial1/1_1_0 \n",
    "    Assumes the global variable layer_dir is defined which is the \n",
    "    directory in which to save the spatial index directories\n",
    "    ---INPUT---\n",
    "    level       - 0-indexed integer representing the spatial index level\n",
    "    cell        - a string like \"0_0_0\" representing the x,y,z grid location at a given level \n",
    "                  in which you want to extract a subset\n",
    "    coordinates - a 2D array of coordinates like array([x0,y0,z0],...[xN,yN,zN])\n",
    "    debug       - if True prints out that it saved the file\n",
    "    ---OUTPUT---\n",
    "    Writes the file, but does not return anything\n",
    "    \"\"\"\n",
    "    # We already know how to encode just the coordinates. Do it like so for the first 100 points\n",
    "    spatial_dir = os.path.join(layer_dir,f\"spatial{level}\")\n",
    "    if not os.path.exists(spatial_dir):\n",
    "        os.mkdir(spatial_dir)\n",
    "    filename = os.path.join(spatial_dir,cell)\n",
    "    total_count = len(coordinates)\n",
    "    with open(filename,'wb') as outfile:\n",
    "        buf = struct.pack('<Q',total_count)\n",
    "        pt_buf = b''.join(struct.pack('<3f',x,y,z) for (x,y,z) in coordinates)\n",
    "        buf += pt_buf\n",
    "        id_buf = struct.pack('<%sQ' % len(coordinates), *range(len(coordinates)))\n",
    "        buf += id_buf\n",
    "        outfile.write(buf)\n",
    "    if debug:\n",
    "        print(f\"wrote {filename}\")\n",
    "    \n",
    "def find_intersecting_coordinates(coordinates,lower_bounds,upper_bounds):\n",
    "    \"\"\" \n",
    "    ---PURPOSE---\n",
    "    Find the subset of coordinates that fall within lower and upper bounds in x,y,z\n",
    "    ---INPUT---\n",
    "    coordinates  - a 2D array of coordinates like array([x0,y0,z0],...[xN,yN,zN])\n",
    "    lower_bounds - a tuple or list of x,y,z lower bounds like [0,0,0]\n",
    "    upper_bounds - a tuple or list of x,y,z upper bounds like [2160,2560,617]\n",
    "    ---OUTPUT---\n",
    "    coordinates[mask] - the subset of coordinates that fall \n",
    "                        within the lower and upper bounds\n",
    "    \"\"\"\n",
    "    mask = (coordinates[:,0]>=lower_bounds[0]) & (coordinates[:,0]<upper_bounds[0]) & \\\n",
    "           (coordinates[:,1]>=lower_bounds[1]) & (coordinates[:,1]<upper_bounds[1]) & \\\n",
    "           (coordinates[:,2]>=lower_bounds[2]) & (coordinates[:,2]<upper_bounds[2])\n",
    "    return coordinates[mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function to create the whole precomputed layer with multiple spatial indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(unique_coordinates,layer_dir,grid_shape = [1,1,1],\n",
    "         chunk_size=[2160,2560,687],dimensions_m=[5e-06,5e-06,1e-05],\n",
    "         limit=10000,debug=False):\n",
    "    \"\"\" \n",
    "    ---PURPOSE---\n",
    "    Create the multiple spatial index levels and save out the cell files at each level.\n",
    "    Also create, save and return the info file for this layer.\n",
    "    ---INPUT---\n",
    "    unique_coordinates - A 2D array of all coordinates representing your point annotations\n",
    "                         that you want to spatially index. Duplicates should be removed already.\n",
    "    layer_dir          - Base precomputed layer directory in which to save the info file\n",
    "                         and spatial index directories\n",
    "    grid_shape         - The grid shape of level 0. Typically this is [1,1,1].\n",
    "    chunk_size         - The chunk size of level 0. If grid_shape = [1,1,1] then this is \n",
    "                         the dimensions of the entire volume, e.g. [2160,2560,617]\n",
    "    dimensions_m       - The x,y,z dimensions in meters in a tuple or list\n",
    "    limit              - The maximum number of annotations you wish to display \n",
    "                         in any cell at any level in Neuroglancer\n",
    "    debug              - Set to True to print out various quantities to help with debugging\n",
    "             \n",
    "    ---OUTPUT---\n",
    "    Writes out each spatialX/X_Y_Z spatial index file in layer_dir\n",
    "    Writes out the info file in layer_dir\n",
    "    info    - a dictionary containing the precomputed info JSON information\n",
    "    \"\"\"\n",
    "    # Complete all of the info file except for the spatial part\n",
    "    info = {}\n",
    "    info['@type'] = \"neuroglancer_annotations_v1\"\n",
    "    info['annotation_type'] = \"POINT\"\n",
    "    info['by_id'] = {'key':'by_id'}\n",
    "    info['dimensions'] = {'x':[str(dimensions_m[0]),'m'],\n",
    "                          'y':[str(dimensions_m[1]),'m'],\n",
    "                          'z':[str(dimensions_m[2]),'m']}\n",
    "    info['lower_bound'] = [0,0,0]\n",
    "    info['upper_bound'] = chunk_size\n",
    "    info['properties'] = []\n",
    "    info['relationships'] = []\n",
    "    info['spatial'] = []\n",
    "    # Create layer dir if it doesn't exist yet\n",
    "    if not os.path.exists(layer_dir):\n",
    "        os.mkdir(layer_dir)\n",
    "    # initialize some variables\n",
    "    level=0\n",
    "    cell=\"0_0_0\"\n",
    "    \n",
    "    total_annotations = len(unique_coordinates)\n",
    "    remaining_annotations = {} # will hold the arrays of coordinates in each cell at each level\n",
    "    remaining_annotations[level] = {cell:unique_coordinates}\n",
    "\n",
    "    maxCount = {} # will hold the maximum remaining annotations at each level\n",
    "    \n",
    "    # Iterate over levels until there are no more annotations to assign to child cells\n",
    "    while True:\n",
    "        if debug:\n",
    "            print(\"##############\")\n",
    "            print(f\"Level: {level}\")\n",
    "            print(\"##############\")\n",
    "        \n",
    "        # Figure out maxCount to see if we are out of cells\n",
    "        N_annotations_this_level = [len(x) for x in remaining_annotations[level].values()]\n",
    "        maxCount[level] = max(N_annotations_this_level)\n",
    "        if maxCount[level] == 0:\n",
    "            print(\"Finished! Writing info file:\")\n",
    "            info_path = os.path.join(layer_dir,\"info\")\n",
    "            print(info_path)\n",
    "            with open(info_path,'w') as outfile:\n",
    "                json.dump(info,outfile,indent=2)\n",
    "            break\n",
    "        # If we made it past there then we have cells left to assign\n",
    "    \n",
    "        # Use utility functions to figure out grid_shape and chunk_size for this level\n",
    "        factor = calculate_factors(level)\n",
    "        grid_shape = [a*b for a,b in zip(grid_shape,factor)]\n",
    "        chunk_size = [a/b for a,b in zip(chunk_size,factor)]\n",
    "        # Make the spatial dict for the info file\n",
    "        spatial_dict_this_level = {\n",
    "        'key':f'spatial{level}',\n",
    "        'grid_shape':grid_shape,\n",
    "        'chunk_size':chunk_size,\n",
    "        'limit':limit\n",
    "        }\n",
    "        info['spatial'].append(spatial_dict_this_level)\n",
    "        \n",
    "        cells = make_cells(grid_shape)\n",
    "            \n",
    "        if debug:\n",
    "            print(f\"chunk_size={chunk_size}, maxCount = {maxCount[level]}\")\n",
    "            print(\"Have these cells:\", cells)\n",
    "        \n",
    "        # Figure out the probability of extracting each annotation based on the limit\n",
    "        if maxCount[level] > limit:\n",
    "            prob = limit/maxCount[level]\n",
    "        else:\n",
    "            prob = 1\n",
    "            \n",
    "        # Loop over each cell at this level\n",
    "        for cell in cells:\n",
    "            if debug:\n",
    "                print(\"In cell: \", cell)\n",
    "            \n",
    "            # Look up the remaining annotations in this cell, which was computed during the last iteration\n",
    "            annotations_this_cell = remaining_annotations[level][cell]            \n",
    "            N_annotations_this_cell = len(annotations_this_cell)\n",
    "            if debug:\n",
    "                print(f\"started with {N_annotations_this_cell} annotations\")\n",
    "            \n",
    "            # Need to know the child cells and the size of each so we can figure out the \n",
    "            # remaining counts in each\n",
    "            next_factor = calculate_factors(level+1)\n",
    "            child_cells = get_child_cells(cell,next_factor)\n",
    "            next_chunk_size = [a/b for a,b in zip(chunk_size,next_factor)]\n",
    "\n",
    "            # If we have annotations in this cell, then save the spatial index file for this level and cell\n",
    "            # If not, don't save the file since it would be empty\n",
    "            if N_annotations_this_cell != 0:\n",
    "                # Figure out the subset of cells based on the probability calculated above\n",
    "                N_subset = int(round(N_annotations_this_cell*prob))\n",
    "                \n",
    "                # figure out list of indices of the remaining array to grab \n",
    "                subset_indices = np.random.choice(range(N_annotations_this_cell),size=N_subset,replace=False)\n",
    "                # Use these indices to get the subset of annotations\n",
    "                subset_cells = np.take(annotations_this_cell,subset_indices,axis=0)\n",
    "                \n",
    "                if debug:\n",
    "                    print(f\"subsetted {len(subset_cells)} annotations\")\n",
    "\n",
    "                # save these cells to a spatial index file\n",
    "                save_cellfile(level,cell,subset_cells,debug=debug)\n",
    "                \n",
    "                # Figure out the leftover annotations that weren't included in the subset\n",
    "                indices_annotations_this_cell = range(len(annotations_this_cell))\n",
    "                leftover_annotation_indices = set(indices_annotations_this_cell)-set(subset_indices)\n",
    "                leftover_annotations = np.take(annotations_this_cell,list(leftover_annotation_indices),axis=0)\n",
    "                if debug:\n",
    "                    print(f\"have {len(leftover_annotations)} annotations leftover\")\n",
    "            else:\n",
    "                leftover_annotations = np.array([])\n",
    "            # Initialize the next level in the remaining_annotations dictionary\n",
    "            if level+1 not in remaining_annotations.keys():\n",
    "                remaining_annotations[level+1] = {}\n",
    "            \n",
    "            if debug:\n",
    "                print(\"Looping over child cells: \", child_cells)\n",
    "            \n",
    "            # Intiailize a variable to keep track of how many annotations total are in each child cell\n",
    "            n_annotations_in_child_cells = 0\n",
    "            \n",
    "            # Loop over child cells and figure out how many of the remaining annotations \n",
    "            # fall in each child cell region\n",
    "            for child_cell in child_cells:\n",
    "                if N_annotations_this_cell == 0:\n",
    "                    remaining_annotations[level+1][child_cell] = np.array([])\n",
    "                    continue\n",
    "                \n",
    "                if debug:\n",
    "                    print(f\"Child cell: {child_cell}\")\n",
    "                \n",
    "                # figure out which of the leftover annotations fall within this child cell\n",
    "                child_cell_indices = [int(x) for x in child_cell.split('_')]\n",
    "                child_lower_bounds = [a*b for a,b in zip(child_cell_indices,next_chunk_size)]\n",
    "                child_upper_bounds = [a+b for a,b, in zip(child_lower_bounds,next_chunk_size)]\n",
    "                \n",
    "                if debug:\n",
    "                    print(\"Child lower and upper bounds\")\n",
    "                    print(child_lower_bounds)\n",
    "                    print(child_upper_bounds)\n",
    "\n",
    "                # Now use the bounds to find intersecting annotations in this child cell\n",
    "                intersecting_annotations_this_child = find_intersecting_coordinates(\n",
    "                    leftover_annotations,child_lower_bounds,child_upper_bounds)\n",
    "                \n",
    "                if debug:\n",
    "                    print(f\"Have {len(intersecting_annotations_this_child)} in this child cell\")\n",
    "                \n",
    "                # Assign the remaining annotations for the child cell in the dictionary\n",
    "                remaining_annotations[level+1][child_cell] = intersecting_annotations_this_child\n",
    "                \n",
    "                n_annotations_in_child_cells+=len(intersecting_annotations_this_child)\n",
    "            \n",
    "            # Make sure that the sum of all annotations in all child cells equals the total for this cell\n",
    "            if debug:\n",
    "                print(\"Leftover annotations this cell vs. sum in child cells\")\n",
    "                print(len(leftover_annotations),n_annotations_in_child_cells)\n",
    "        assert len(leftover_annotations) == n_annotations_in_child_cells\n",
    "        \n",
    "        # increment to the next level before next iteration in while loop\n",
    "        level+=1\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! Writing info file:\n",
      "/home/ahoag/ngdemo/demo_bucket/test_annotations/test_multispatialindex3/info\n"
     ]
    }
   ],
   "source": [
    "# Call the main() function to produce the multi-spatial index precomputed layer and info file \n",
    "# Define where you want to save this layer. Parent folder must exist\n",
    "layer_dir = \"/home/ahoag/ngdemo/demo_bucket/test_annotations/test_multispatialindex3\"\n",
    "info = main(unique_coordinates,layer_dir,grid_shape = [1,1,1],\n",
    "         chunk_size=[2160,2560,687],limit=10000,debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@type': 'neuroglancer_annotations_v1',\n",
       " 'annotation_type': 'POINT',\n",
       " 'by_id': {'key': 'by_id'},\n",
       " 'dimensions': {'x': ['5e-06', 'm'], 'y': ['5e-06', 'm'], 'z': ['1e-05', 'm']},\n",
       " 'lower_bound': [0, 0, 0],\n",
       " 'upper_bound': [2160, 2560, 687],\n",
       " 'properties': [],\n",
       " 'relationships': [],\n",
       " 'spatial': [{'key': 'spatial0',\n",
       "   'grid_shape': [1, 1, 1],\n",
       "   'chunk_size': [2160.0, 2560.0, 687.0],\n",
       "   'limit': 10000},\n",
       "  {'key': 'spatial1',\n",
       "   'grid_shape': [2, 2, 1],\n",
       "   'chunk_size': [1080.0, 1280.0, 687.0],\n",
       "   'limit': 10000},\n",
       "  {'key': 'spatial2',\n",
       "   'grid_shape': [4, 4, 1],\n",
       "   'chunk_size': [540.0, 640.0, 687.0],\n",
       "   'limit': 10000},\n",
       "  {'key': 'spatial3',\n",
       "   'grid_shape': [8, 8, 2],\n",
       "   'chunk_size': [270.0, 320.0, 343.5],\n",
       "   'limit': 10000},\n",
       "  {'key': 'spatial4',\n",
       "   'grid_shape': [16, 16, 4],\n",
       "   'chunk_size': [135.0, 160.0, 171.75],\n",
       "   'limit': 10000}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 spatial index levels were created. So in the folder `layer_dir` there should be an `info` file and the folders: `spatial0`, `spatial1`, ..., `spatial4`. In each spatial folder there will be lots of files like `X_Y_Z` which are binary files which contain the coordinates in that grid cell (a.k.a. chunk)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ng",
   "language": "python",
   "name": "ng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
