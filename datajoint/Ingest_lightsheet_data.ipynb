{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest_lightsheet_data\n",
    "The idea here is to use the google sheets API to ingest the information stored in the core facility clearing google sheets into the braincogs00 database. We will be making a new database table for each clearing protocol spreadsheet. These will be linked to the Experiments() table in the database by the experiment_id primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path, sys\n",
    "from datetime import datetime\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datajoint as dj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Spreadsheet API setup\n",
    "\n",
    "We will be using the Google Sheets API: https://developers.google.com/sheets/api/quickstart/python\n",
    "\n",
    "For details on how to set this up on your machine, see this notebook: [Microscope_management.ipynb](Microscope_management.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPREADSHEET_ID = '15NmKBIPfSSpjTFoHS6K2jREsbMZHueyQ5psub-bctjI' # The copy of the clearing spreadsheet, where I made some formatting changes to make it more consistent.\n",
    "# Set the scope to be read only since we are not adding anything to the google sheet, just reading it\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets.readonly']\n",
    "\n",
    "secrets_file = 'credentials.json' # Has to be called \"credentials.json\"  \n",
    "assert os.path.exists(secrets_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_google_sheet(spreadsheet_id,range_query):\n",
    "    \"\"\"\n",
    "    ---PURPOSE---\n",
    "    Gets the data from the range of cells in the google sheet specified.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                secrets_file, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "    # Call the Sheets API\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().get(spreadsheetId=spreadsheet_id,\n",
    "                                range=range_query).execute()\n",
    "    values = result.get('values', [])\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter DataJoint username: ahoag\n",
      "Please enter DataJoint password: ········\n",
      "Connecting ahoag@datajoint00.pni.princeton.edu:3306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataJoint connection (connected) ahoag@datajoint00.pni.princeton.edu:3306"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.config['database.host'] = 'datajoint00.pni.princeton.edu'\n",
    "dj.conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to port forward 3306 from jtb3-dev@pni.princeton.edu first (set this up externally on the machine)\n",
    "db_lightsheet = dj.create_virtual_module('ahoag_lightsheet_demo','ahoag_lightsheet_demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Form Responses sheet and the various clearing sheets from google sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are in readonly (see SCOPES) so we cannot destroy it by accident.\n",
    "form_responses_range_query = 'Form Responses!A4:T54' # Syntax is Sheet_name!cell_start:cell_end, e.g. \n",
    "form_responses_values = retrieve_google_sheet(spreadsheet_id=SPREADSHEET_ID,range_query=form_responses_range_query)\n",
    "df_form_responses = pd.DataFrame(form_responses_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iDISCO+\n",
    "idisco_plus_range_query = 'iDISCO+!A3:BF20' # Syntax is Sheet_name!cell_start:cell_end, e.g. \n",
    "idisco_plus_values = retrieve_google_sheet(spreadsheet_id=SPREADSHEET_ID,range_query=idisco_plus_range_query)\n",
    "df_idisco_plus = pd.DataFrame(idisco_plus_values)\n",
    "# Assign column names based on first row, then remove top two rows as first is header and second is an example row\n",
    "new_header = df_idisco_plus.iloc[0]\n",
    "df_idisco_plus = df_idisco_plus[2:]\n",
    "df_idisco_plus.columns = new_header\n",
    "\n",
    "# iDISCO abbreviated clearing\n",
    "idisco_abbrev_range_query = 'iDISCO_NOF!A2:T27' # Syntax is Sheet_name!cell_start:cell_end, e.g. \n",
    "idisco_abbrev_values = retrieve_google_sheet(spreadsheet_id=SPREADSHEET_ID,range_query=idisco_abbrev_range_query)\n",
    "df_idisco_abbrev = pd.DataFrame(idisco_abbrev_values)\n",
    "# Assign column names based on first row, then remove top two rows as first is header and second is an example row\n",
    "new_header = df_idisco_abbrev.iloc[0]\n",
    "df_idisco_abbrev = df_idisco_abbrev[3:]\n",
    "df_idisco_abbrev.columns = new_header\n",
    "\n",
    "# iDISCO abbreviated clearing (rat)\n",
    "idisco_abbrev_rat_range_query = 'iDISCO_NOF_Rat!A2:X12' # Syntax is Sheet_name!cell_start:cell_end, e.g. \n",
    "idisco_abbrev_rat_values = retrieve_google_sheet(spreadsheet_id=SPREADSHEET_ID,range_query=idisco_abbrev_rat_range_query)\n",
    "df_idisco_abbrev_rat = pd.DataFrame(idisco_abbrev_rat_values)\n",
    "# Assign column names based on first row, then remove top two rows as first is header and second is an example row\n",
    "new_header = df_idisco_abbrev_rat.iloc[0]\n",
    "df_idisco_abbrev_rat = df_idisco_abbrev_rat[2:]\n",
    "df_idisco_abbrev_rat.columns = new_header\n",
    "\n",
    "# uDISCO \n",
    "udisco_range_query = 'uDISCO!A3:O8' # Syntax is Sheet_name!cell_start:cell_end, e.g. \n",
    "udisco_values = retrieve_google_sheet(spreadsheet_id=SPREADSHEET_ID,range_query=udisco_range_query)\n",
    "df_udisco = pd.DataFrame(udisco_values)\n",
    "# Assign column names based on first row, then remove top two rows as first is header and second is an example row\n",
    "new_header = df_udisco.iloc[0]\n",
    "df_udisco = df_udisco[2:]\n",
    "df_udisco.columns = new_header\n",
    "\n",
    "# iDISCO+_EdU \n",
    "idisco_edu_range_query = 'iDISCO+_EdU!A3:AV6' # Syntax is Sheet_name!cell_start:cell_end, e.g. \n",
    "idisco_edu_values = retrieve_google_sheet(spreadsheet_id=SPREADSHEET_ID,range_query=idisco_edu_range_query)\n",
    "df_idisco_edu = pd.DataFrame(idisco_edu_values)\n",
    "# Assign column names based on first row, then remove top two rows as first is header and second is an example row\n",
    "new_header = df_idisco_edu.iloc[0]\n",
    "df_idisco_edu = df_idisco_edu[3:]\n",
    "df_idisco_edu.columns = new_header\n",
    "\n",
    "# iDISCO+_rat \n",
    "idisco_rat_range_query = 'iDISCO+_Rat!A3:BH6' # Syntax is Sheet_name!cell_start:cell_end, e.g. \n",
    "idisco_rat_values = retrieve_google_sheet(spreadsheet_id=SPREADSHEET_ID,range_query=idisco_rat_range_query)\n",
    "df_idisco_rat = pd.DataFrame(idisco_rat_values)\n",
    "# Assign column names based on first row, then remove top two rows as first is header and second is an example row\n",
    "new_header = df_idisco_rat.iloc[0]\n",
    "df_idisco_rat = df_idisco_rat[1:]\n",
    "df_idisco_rat.columns = new_header\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some setup\n",
    "clearing_protocol_dict = {'iDISCO+ (immunostaining)':'iDISCO+_immuno',\n",
    "                         'iDISCO for non-oxidizable fluorophores (abbreviated clearing)':'iDISCO abbreviated clearing',\n",
    "                         'uDISCO':'uDISCO',\n",
    "                         'Wang Lab iDISCO Protocol-EdU':'iDISCO_EdU'}\n",
    "clearing_protocol_link_dict = {'iDISCO+_immuno':'https://docs.google.com/spreadsheets/d/1A83HVyy1bEhctqArwt4EiT637M8wBxTFodobbt1jrXI/edit#gid=0',\n",
    "                              'iDISCO abbreviated clearing':'https://docs.google.com/spreadsheets/d/1A83HVyy1bEhctqArwt4EiT637M8wBxTFodobbt1jrXI/edit#gid=895577002',\n",
    "                              'iDISCO abbreviated clearing (rat)': 'https://docs.google.com/spreadsheets/d/1A83HVyy1bEhctqArwt4EiT637M8wBxTFodobbt1jrXI/edit#gid=782871049',\n",
    "                              'uDISCO':'https://docs.google.com/spreadsheets/d/1A83HVyy1bEhctqArwt4EiT637M8wBxTFodobbt1jrXI/edit#gid=1195842433',\n",
    "                              'iDISCO+_rat':'https://docs.google.com/spreadsheets/d/1A83HVyy1bEhctqArwt4EiT637M8wBxTFodobbt1jrXI/edit#gid=1114714575',\n",
    "                              'iDISCO_EdU':'https://docs.google.com/spreadsheets/d/1A83HVyy1bEhctqArwt4EiT637M8wBxTFodobbt1jrXI/edit#gid=746482133'}\n",
    "def request_fxn(x,length):\n",
    "    \"\"\" A function to be applied to a pandas column\n",
    "    for getting the 0:length characters in a modified string\n",
    "    of the column\"\"\"\n",
    "    return '_'.join(x[0:64].split(' '))[0:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jverpeut DREADDymaze\n",
      "blank row, skipping.\n",
      "willmore 20190304_LW_m340\n",
      "jverpeut an1-31\n",
      "apv2 20190313_IBL_DiI_1\n",
      "marlies.oostland 20181217_marlies, 20181013_marlies_M12\n",
      "jverpeut EAAT4- 14 samples\n",
      "mioffe a1_20190327 a2_20190327 a3_20190327\n",
      "jverpeut LindsayCrusI_ymaze_cfos (25 samples)\n",
      "jverpeut AdultChronicD_MLI_Lawrence (1-12 each batch)\n",
      "pbibawi 20190405_pb_X015, 20190405_pb_X045, 20190405_pb_A233,20190405_pb\n",
      "willmore 20190510_lw_059\n",
      "marlies.oostland not sure - check when samples are given to us?\n",
      "ejdennis 201905_atlas00x where x=1:n\n",
      "mioffe five samples, h1 and h2, r1,r2,r3\n",
      "soline Mouse_058\n",
      "ejdennis 20190606_atlas00x where x=11-20\n",
      "rmallarino 171\n",
      "marlies.oostland Marlies_190614_M21\n",
      "jverpeut cruslat_ymaze_TiffanyP_6.20.19 (12 samples)\n",
      "jverpeut cruslat_ymaze_TiffanyP_6.20.19 (13 samples)\n",
      "marlies.oostland Marlies_190618_M30\n",
      "jverpeut opto_ai27D_lobVI\n",
      "soline 190725 _Target_Practice\n",
      "ejdennis W118, K292, K293, K295, K301, K302, K303, K304, K305, K306, K307\n",
      "ejdennis 10-13 brains, names TBD\n",
      "willmore 20190809_lw_32\n",
      "marlies.oostland 190809_M27\n",
      "apv2 ibl_witten_04\n",
      "marlies.oostland M18, M19, M23, M26, M28, M29, M31\n",
      "zhihaoz 20190903_gp5p3_gcamp6f_20dg\n",
      "zhihaoz 190912_zz34\n",
      "mioffe M10, M11\n",
      "ejdennis W122, W128, E111\n",
      "willmore 20191001_LW_10, 11,12, 13, 54, 55, 60, 61, 70, 71, 392\n",
      "pbibawi 20191001_pb_Z265_ctb647_rPPC, 20191001_pb_Z266_ctb647_lPPC, 2019\n",
      "jverpeut 10032019_CNOtest\n",
      "jverpeut CDymaze_1-10\n",
      "jverpeut TPcrusI_lat_cfos (34 samples perfused 3 days): 11.14.19 5-14, 11\n",
      "afalkner MFNP2, MFNP3, MMNP4, MMNP5, MMNP6, FMNP4, FMNP5, FMNP6\n",
      "soline Mouse092, MouseK01\n",
      "lpinto sp4_20200106_albumin-FITC\n",
      "jverpeut mcherryDymaze_1-14\n",
      "ejdennis X050\n",
      "ejdennis X042\n",
      "ejdennis X013\n",
      "soline Mouse_817\n",
      "lpinto sp1_20200210\n",
      "ejdennis K310 (CM-diI), K315 (CM-diI), K320, K321, K323, K327, K333, K334\n",
      "ejdennis E112, E126, E137\n"
     ]
    }
   ],
   "source": [
    "def ingest_request_form(insert=False):\n",
    "    user_insert_list = [{'username':'ahoag','princeton_email':'ahoag@princeton.edu'},\n",
    "                        {'username':'zmd','princeton_email':'zmd@princeton.edu'},\n",
    "                        {'username':'jduva','princeton_email':'jduva@princeton.edu'},\n",
    "                        {'username':'kellyms','princeton_email':'kellyms@princeton.edu'}] # admins\n",
    "    request_insert_list = []\n",
    "    clearing_batch_insert_list = []\n",
    "    sample_insert_list = []\n",
    "    imaging_request_insert_list = []\n",
    "    imaging_resolution_request_insert_list = []\n",
    "    processing_request_insert_list = []\n",
    "    for row in df_form_responses.values.tolist():\n",
    "        if not any(row): \n",
    "            print(\"blank row, skipping.\")\n",
    "            # skip blank rows\n",
    "            continue\n",
    "        request_insert_dict = {}\n",
    "        # User() table\n",
    "        # handle email and user \n",
    "        email = row[-4]\n",
    "        requested_by = ''\n",
    "        if not email.endswith('princeton.edu'):\n",
    "            if email == 'zahra.dhanerawala@gmail.com':\n",
    "                email = 'marlies.oostland@princeton.edu'\n",
    "                requested_by = 'zmd'\n",
    "            elif email == 'emilyjanedennis@gmail.com':\n",
    "                email = 'ejdennis@princeton.edu'\n",
    "            else:\n",
    "                continue\n",
    "        email = email.lower()\n",
    "        username = email.split('@')[0]\n",
    "        user_insert_dict = {'username':username,'princeton_email':email}\n",
    "        user_insert_list.append(user_insert_dict)\n",
    "        request_insert_dict['username'] = username\n",
    "        # Request() table\n",
    "        # handle request_name\n",
    "        request_name = row[3][0:64].strip()\n",
    "        print(username,request_name)\n",
    "        request_name = '_'.join(request_name.split(' '))\n",
    "        request_insert_dict['request_name'] = request_name\n",
    "        # requested_by\n",
    "        if requested_by != 'zmd':\n",
    "            requested_by = username\n",
    "        request_insert_dict['requested_by'] = requested_by\n",
    "        # date_submitted and time_submitted\n",
    "        date_input,time_input = row[0].split(' ')\n",
    "        date_submitted = datetime.strptime(date_input,'%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "        time_submitted = time_input\n",
    "        request_insert_dict['date_submitted'] = date_submitted\n",
    "        request_insert_dict['time_submitted'] = time_submitted\n",
    "        # labname\n",
    "        labname = row[-1]\n",
    "        if not labname:\n",
    "            labname = 'not provided'\n",
    "        request_insert_dict['labname'] = labname\n",
    "        # subject_fullname\n",
    "        request_insert_dict['subject_fullname'] = ''\n",
    "        # correspondence email\n",
    "        request_insert_dict['correspondence_email'] = email\n",
    "        # description\n",
    "        description = row[5][0:250]\n",
    "        request_insert_dict['description'] = description\n",
    "        # species\n",
    "        species = row[1].lower()\n",
    "        request_insert_dict['species'] = species\n",
    "        # number_of_samples\n",
    "        number_of_samples = row[4]\n",
    "        try:\n",
    "            number_of_samples = int(number_of_samples)\n",
    "        except:\n",
    "            number_of_samples = 1\n",
    "        request_insert_dict['number_of_samples'] = number_of_samples\n",
    "        # flag this request as an archival request \n",
    "        request_insert_dict['is_archival'] = True\n",
    "        request_insert_list.append(request_insert_dict)\n",
    "        \n",
    "        # ClearingBatch() table\n",
    "        clearing_batch_insert_dict = {\n",
    "            'username':username,\n",
    "            'request_name':request_name}\n",
    "        # clearing protocol\n",
    "        clearing_protocol_sheet = row[2]\n",
    "        clearing_protocol = clearing_protocol_dict[clearing_protocol_sheet]\n",
    "        if species == 'rat':\n",
    "            if clearing_protocol == 'iDISCO abbreviated clearing':\n",
    "                clearing_protocol = 'iDISCO abbreviated clearing (rat)'\n",
    "            elif clearing_protocol == 'iDISCO+_immuno':\n",
    "                clearing_protocol = 'iDISCO+_rat'\n",
    "\n",
    "        clearing_batch_insert_dict['clearing_protocol'] = clearing_protocol\n",
    "        \n",
    "        if clearing_protocol == 'iDISCO+_immuno':\n",
    "            df_clearing = df_idisco_plus\n",
    "            df_clearing = df_clearing.rename(columns={'Sample Name/#':'Sample Name'})\n",
    "            mask = df_clearing['Sample Name'].apply(request_fxn,args=(len(request_name),)) == request_name\n",
    "            assert len(df_clearing[mask]) == 1\n",
    "            df_clearing_this_request = df_clearing[mask]\n",
    "            antibody1,antibody2 = df_clearing_this_request.iloc[0][['Primary antibody+conc','Secondary antibody+conc']].to_numpy()\n",
    "        elif clearing_protocol == 'iDISCO abbreviated clearing':\n",
    "            df_clearing = df_idisco_abbrev\n",
    "            antibody1,antibody2 = '',''\n",
    "        elif clearing_protocol == 'iDISCO abbreviated clearing (rat)':\n",
    "            df_clearing = df_idisco_abbrev_rat\n",
    "        elif clearing_protocol == 'iDISCO+_rat':\n",
    "            df_clearing = df_idisco_rat\n",
    "        elif clearing_protocol == 'uDISCO':\n",
    "            df_clearing = df_udisco\n",
    "        elif clearing_protocol == 'iDISCO_EdU':\n",
    "            df_clearing = df_idisco_edu\n",
    "        else:\n",
    "            sys.exit(f'Clearing protocol {clearing_protocol} is not accepted')\n",
    "        # link to clearing sheet\n",
    "        clearing_link = clearing_protocol_link_dict[clearing_protocol]\n",
    "        clearing_batch_insert_dict['link_to_clearing_spreadsheet'] = clearing_link\n",
    "        clearing_batch_insert_dict['antibody1'] = antibody1\n",
    "        clearing_batch_insert_dict['antibody2'] = antibody2\n",
    "        # Find the row in the clearing sheet corresponding to this request\n",
    "        \n",
    "        # clearing batch number\n",
    "        clearing_batch_number = 1 # always only 1 batch if submitted from the google form\n",
    "        clearing_batch_insert_dict['clearing_batch_number'] = clearing_batch_number\n",
    "        # clearing progress\n",
    "        clearing_progress = 'complete' # if it is in the spreadsheet\n",
    "        clearing_batch_insert_dict['clearing_progress'] = clearing_progress\n",
    "        # number in batch \n",
    "        clearing_batch_insert_dict['number_in_batch'] = number_of_samples\n",
    "        # perfusion date\n",
    "        perfusion_date_input = row[-3]\n",
    "        if perfusion_date_input: # can be NULL, so just don't insert if it is empty in sheet\n",
    "            perfusion_date_submitted = datetime.strptime(perfusion_date_input,'%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "            clearing_batch_insert_dict['perfusion_date_submitted'] = perfusion_date_submitted\n",
    "        # handoff date \n",
    "        handoff_date_input = row[-2]\n",
    "        if handoff_date_input: # can be NULL, so just don't insert if it is empty in sheet\n",
    "            handoff_date_submitted = datetime.strptime(handoff_date_input,'%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "            clearing_batch_insert_dict['handoff_date_submitted'] = handoff_date_submitted\n",
    "        # clearer - not currently possible to tell so leave blank - OK since NULLable column\n",
    "        \n",
    "        # notes for clearer - there was not a space for users to submit this so leaving blank.\n",
    "        notes_for_clearer = ''\n",
    "        clearing_batch_insert_dict['notes_for_clearer'] = notes_for_clearer\n",
    "        \n",
    "        clearing_batch_insert_list.append(clearing_batch_insert_dict)\n",
    "        \n",
    "        # Sample(), ImagingRequest(), ImagingResolutionRequest(), ProcessingRequest() tables \n",
    "        notes_for_imager = row[11]\n",
    "        was_imaged = int(row[12])\n",
    "        processed_data_location_bucket = row[13]\n",
    "        if processed_data_location_bucket:\n",
    "            notes_from_imaging = f'Processed files are here: {processed_data_location_bucket}'\n",
    "        else:\n",
    "            notes_from_imaging = ''\n",
    "        sample_master_dict = {\n",
    "            'username':username,'request_name':request_name,\n",
    "            'clearing_protocol':clearing_protocol,\n",
    "            'antibody1':antibody1,'antibody2':antibody2,\n",
    "            'clearing_batch_number':clearing_batch_number,\n",
    "        }\n",
    "        \n",
    "        imaging_request_master_dict = {\n",
    "            'username':username,'request_name':request_name,\n",
    "            'imaging_request_number':1,\n",
    "            'imaging_progress':'complete',\n",
    "            'imaging_request_date_submitted':date_submitted,\n",
    "            'imaging_request_time_submitted':time_submitted\n",
    "        }\n",
    "        imaging_resolution_request_master_dict = {\n",
    "            'username':username,'request_name':request_name,\n",
    "            'imaging_request_number':1,\n",
    "            'notes_for_imager':notes_for_imager,\n",
    "            'notes_from_imaging':notes_from_imaging\n",
    "        }\n",
    "        processing_request_master_dict = {\n",
    "            'username':username,'request_name':request_name,\n",
    "            'imaging_request_number':1,\n",
    "            'processing_request_number':1,\n",
    "            'processor':'zmd',\n",
    "            'processing_request_date_submitted':date_submitted,\n",
    "            'processing_request_time_submitted':time_submitted,\n",
    "            'processing_progress':'complete'\n",
    "        }\n",
    "       \n",
    "        image_resolutions = []\n",
    "        imaging_resolution_input_str = row[9]\n",
    "        if '1.3x' in imaging_resolution_input_str:\n",
    "            image_resolutions.append('1.3x')\n",
    "        if '1.1x' in imaging_resolution_input_str:\n",
    "            image_resolutions.append('1.1x')\n",
    "        if '4x' in imaging_resolution_input_str:\n",
    "            image_resolutions.append('4x')\n",
    "        if '2x' in imaging_resolution_input_str:\n",
    "            image_resolutions.append('2x')\n",
    "        assert len(image_resolutions) > 0\n",
    "        \n",
    "        for ii in range(number_of_samples):\n",
    "            sample_number_str = str(ii+1)\n",
    "            sample_name = 'sample-' + '0'*(3-len(sample_number_str))+sample_number_str\n",
    "            # Sample() table\n",
    "            sample_insert_dict = sample_master_dict.copy()\n",
    "            sample_insert_dict['sample_name'] = sample_name\n",
    "            sample_insert_list.append(sample_insert_dict)\n",
    "            if was_imaged:\n",
    "                # ImagingRequest() table\n",
    "                imaging_request_insert_dict = imaging_request_master_dict.copy()\n",
    "                imaging_request_insert_dict['sample_name'] = sample_name\n",
    "                imaging_request_insert_list.append(imaging_request_insert_dict)\n",
    "                # ProcessingRequest() table\n",
    "                processing_request_insert_dict = processing_request_master_dict.copy()\n",
    "                processing_request_insert_dict['sample_name'] = sample_name\n",
    "                processing_request_insert_list.append(processing_request_insert_dict) \n",
    "                # ImagingResolutionRequest() table\n",
    "                for image_resolution in image_resolutions:\n",
    "                    imaging_resolution_request_insert_dict = imaging_resolution_request_master_dict.copy()\n",
    "                    imaging_resolution_request_insert_dict['sample_name'] = sample_name\n",
    "                    imaging_resolution_request_insert_dict['image_resolution'] = image_resolution\n",
    "                    imaging_resolution_request_insert_list.append(imaging_resolution_request_insert_dict)\n",
    "                    \n",
    "    if insert:\n",
    "        db_lightsheet.User().insert(user_insert_list,skip_duplicates=True)\n",
    "        db_lightsheet.Request().insert(request_insert_list,skip_duplicates=True)\n",
    "        db_lightsheet.Request.ClearingBatch().insert(clearing_batch_insert_list,skip_duplicates=True)\n",
    "        db_lightsheet.Request.Sample().insert(sample_insert_list,skip_duplicates=True)\n",
    "        db_lightsheet.Request.ImagingRequest().insert(imaging_request_insert_list,skip_duplicates=True)\n",
    "        db_lightsheet.Request.ImagingResolutionRequest().insert(\n",
    "            imaging_resolution_request_insert_list,skip_duplicates=True)\n",
    "        db_lightsheet.Request.ProcessingRequest.insert(processing_request_insert_list,skip_duplicates=True)\n",
    "        \n",
    "ingest_request_form(insert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicated_args = dict(number_of_samples='number_of_samples',description='description',\n",
    "        species='species',datetime_submitted='datetime_submitted')\n",
    "username = 'ejdennis'\n",
    "request_name = 'X050'\n",
    "request_contents = db_lightsheet.Request() & f'request_name=\"{request_name}\"' & \\\n",
    "    f'username=\"{username}\"'\n",
    "clearing_batch_contents = db_lightsheet.Request.ClearingBatch() & \\\n",
    "    f'request_name=\"{request_name}\"' & f'username=\"{username}\"'\n",
    "imaging_request_contents = db_lightsheet.Request.ImagingRequest() & \\\n",
    "    f'request_name=\"{request_name}\"' & f'username=\"{username}\"'\n",
    "replicated_args = dict(number_of_samples='number_of_samples',description='description',\n",
    "        species='species',datetime_submitted='datetime_submitted')\n",
    "\n",
    "sample_joined_contents = dj.U('username','request_name').aggr(\n",
    "        request_contents * clearing_batch_contents,\n",
    "        number_of_samples='number_of_samples',\n",
    "        number_in_batch='number_in_batch',\n",
    "        description='description',\n",
    "        species='species',\n",
    "        datetime_submitted='TIMESTAMP(date_submitted,time_submitted)',\n",
    "        n_cleared='CONVERT(SUM(IF(clearing_progress=\"complete\",number_in_batch,0)),char)').proj(\n",
    "        **replicated_args,\n",
    "            fraction_cleared='CONCAT(n_cleared,\"/\",CONVERT(number_of_samples,char))')\n",
    "# sample_joined_contents * imaging_request_contents\n",
    "imaging_joined_contents = sample_joined_contents.aggr(\n",
    "    imaging_request_contents,\n",
    "    **replicated_args,\n",
    "    fraction_cleared='fraction_cleared',\n",
    "    n_imaged='CONVERT(SUM(imaging_progress=\"complete\"),char)',\n",
    "    total_imaging_requests='CONVERT(COUNT(*),char)',\n",
    "    keep_all_rows=True\n",
    "    ).proj(**replicated_args,\n",
    "        fraction_cleared='fraction_cleared',\n",
    "        # fraction_imaged='CONCAT(n_imaged,\"/\",total_imaging_requests)'\n",
    "        fraction_imaged='IF(n_imaged is NULL,\"0/0\",CONCAT(n_imaged,\"/\",total_imaging_requests))' \n",
    "        )\n",
    "imaging_joined_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dj.U('username','request_name')*sample_joined_contents).aggr(\n",
    "sample_joined_contents * imaging_request_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_joined_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sample_joined_contents * imaging_request_contents).aggr(\n",
    "    fraction_cleared='fraction_cleared',\n",
    "    n_imaged='CONVERT(SUM(imaging_progress=\"complete\"),char)',\n",
    "    total_imaging_requests='CONVERT(COUNT(*),char)',\n",
    "    keep_all_rows=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicated_args = dict(number_of_samples='number_of_samples',description='description',\n",
    "        species='species')\n",
    "username='ejdennis'\n",
    "request_name='X050'\n",
    "# username='soline'\n",
    "# request_name='Mouse092,_MouseK01'\n",
    "request_contents = db_lightsheet.Request() & f'request_name=\"{request_name}\"' & \\\n",
    "            f'username=\"{username}\"'\n",
    "request_contents = request_contents.proj('description','species','number_of_samples',\n",
    "    datetime_submitted='TIMESTAMP(date_submitted,time_submitted)')\n",
    "sample_contents = db_lightsheet.Request.Sample() & f'request_name=\"{request_name}\"' & f'username=\"{username}\"' \n",
    "clearing_batch_contents = db_lightsheet.Request.ClearingBatch() & \\\n",
    "f'request_name=\"{request_name}\"' & f'username=\"{username}\"' \n",
    "imaging_request_contents = db_lightsheet.Request.ImagingRequest() & \\\n",
    " f'request_name=\"{request_name}\"' & f'username=\"{username}\"' \n",
    "processing_request_contents = db_lightsheet.Request.ProcessingRequest() & \\\n",
    " f'request_name=\"{request_name}\"' & f'username=\"{username}\"' \n",
    "sample_joined_contents = request_contents * sample_contents * clearing_batch_contents\n",
    "imaging_joined_contents = sample_joined_contents.aggr(\n",
    "    imaging_request_contents,\n",
    "    **replicated_args,\n",
    "    imaging_request_number='imaging_request_number',\n",
    "    n_imaged='CONVERT(SUM(imaging_progress=\"complete\"),char)',\n",
    "    total_imaging_requests='COUNT(*)',\n",
    "    keep_all_rows=True\n",
    "    ).proj(**replicated_args,\n",
    "           total_imaging_requests='IF(n_imaged is NULL, \"0\",total_imaging_requests)',\n",
    "           imaging_request_number='IF(imaging_request_number is NULL, 499,imaging_request_number)'\n",
    "        # fraction_imaged='CONCAT(n_imaged,\"/\",total_imaging_requests)'\n",
    "        )\n",
    "processing_joined_contents = (dj.U('username','request_name') * imaging_joined_contents).aggr(   \n",
    "    processing_request_contents,\n",
    "    ''\n",
    "    **replicated_args,\n",
    "    total_imaging_requests='total_imaging_requests',\n",
    "    n_processed='CONVERT(SUM(processing_progress=\"complete\"),char)',\n",
    "    total_processing_requests='CONVERT(COUNT(processing_progress),char)',\n",
    "    keep_all_rows=True\n",
    "    ).proj(\n",
    "        **replicated_args,\n",
    "        total_imaging_requests='total_imaging_requests',\n",
    "        total_processing_requests='IF(n_processed is NULL,0,total_processing_requests)',\n",
    "        \n",
    "        )\n",
    "processing_joined_contents\n",
    "# imaging_joined_contents\n",
    "\n",
    "# sample_joined_contents = sample_contents.aggr(\n",
    "#         request_contents * clearing_batch_contents,\n",
    "#         number_of_samples='number_of_samples',\n",
    "#         number_in_batch='number_in_batch',\n",
    "#         description='description',\n",
    "#         species='species',\n",
    "#         ).proj(\n",
    "#             **replicated_args,\n",
    "#             fraction_cleared='CONCAT(n_cleared,\"/\",CONVERT(number_of_samples,char))')\n",
    "# sample_joined_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# username='ejdennis'\n",
    "# request_name='X050'\n",
    "# username='soline'\n",
    "# request_name='Mouse092,_MouseK01'\n",
    "request_contents = db_lightsheet.Request() & f'request_name=\"{request_name}\"' & \\\n",
    "            f'username=\"{username}\"'\n",
    "sample_contents = db_lightsheet.Request.Sample() & f'request_name=\"{request_name}\"' & \\\n",
    "            f'username=\"{username}\"'\n",
    "clearing_batch_contents = db_lightsheet.Request.ClearingBatch() & f'request_name=\"{request_name}\"' & \\\n",
    "            f'username=\"{username}\"'\n",
    "imaging_request_contents = db_lightsheet.Request.ImagingRequest() & f'request_name=\"{request_name}\"' & \\\n",
    "            f'username=\"{username}\"'\n",
    "processing_request_contents = db_lightsheet.Request.ProcessingRequest() & f'request_name=\"{request_name}\"' & \\\n",
    "            f'username=\"{username}\"'\n",
    "\n",
    "replicated_args = dict(description='description',\n",
    "        species='species')\n",
    "\n",
    "sample_joined_contents = request_contents * sample_contents * clearing_batch_contents\n",
    "imaging_joined_contents = sample_joined_contents.aggr(\n",
    "    imaging_request_contents,\n",
    "    **replicated_args,\n",
    "    imaging_request_number='imaging_request_number',\n",
    "    n_imaged='CONVERT(SUM(imaging_progress=\"complete\"),char)',\n",
    "    total_imaging_requests='COUNT(*)',\n",
    "    keep_all_rows=True\n",
    "    ).proj(**replicated_args,\n",
    "           total_imaging_requests='IF(n_imaged is NULL, \"0\",total_imaging_requests)',\n",
    "           imaging_request_number='IF(imaging_request_number is NULL, -1,imaging_request_number)'\n",
    "        )\n",
    "imaging_joined_contents\n",
    "processing_joined_contents = (dj.U('username','request_name') * imaging_joined_contents).aggr(   \n",
    "        processing_request_contents,\n",
    "        'imaging_request_number',\n",
    "        **replicated_args,\n",
    "        processing_request_number='processing_request_number',\n",
    "        total_imaging_requests='total_imaging_requests',\n",
    "        n_processed='CONVERT(SUM(processing_progress=\"complete\"),char)',\n",
    "        total_processing_requests='CONVERT(COUNT(processing_progress),char)',\n",
    "        keep_all_rows=True).proj(\n",
    "            **replicated_args,\n",
    "            imaging_request_number='IF(imaging_request_number = -1,\"N/A\",imaging_request_number)',\n",
    "            processing_request_number='IF(processing_request_number is NULL, \"N/A\",processing_request_number)',\n",
    "            total_imaging_requests='total_imaging_requests',\n",
    "            total_processing_requests='IF(n_processed is NULL,0,total_processing_requests)', \n",
    "            )\n",
    "processing_joined_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_contents = db_lightsheet.Request()\n",
    "sample_contents = db_lightsheet.Request.Sample()\n",
    "clearing_batch_contents = db_lightsheet.Request.ClearingBatch()\n",
    "imaging_request_contents = db_lightsheet.Request.ImagingRequest()\n",
    "processing_request_contents = db_lightsheet.Request.ProcessingRequest()\n",
    "clearing_joined_contents = (sample_contents * request_contents * clearing_batch_contents).proj(\n",
    "        request_name='request_name',sample_name='sample_name',\n",
    "        species='species',clearing_protocol='clearing_protocol',\n",
    "        clearing_progress='clearing_progress',\n",
    "        datetime_submitted='TIMESTAMP(date_submitted,time_submitted)')\n",
    "''' Now figure out what fraction of imaging requests have been fulfilled '''    \n",
    "replicated_args = dict(species='species',clearing_protocol='clearing_protocol',\n",
    "imaging_request_number='imaging_request_number',imager='imager',\n",
    "imaging_progress='imaging_progress',\n",
    "clearing_progress='clearing_progress',\n",
    "antibody1='antibody1',antibody2='antibody2',\n",
    "clearing_batch_number='clearing_batch_number',\n",
    "datetime_submitted='datetime_submitted')\n",
    "\n",
    "# imaging_joined_contents = dj.U('username','request_name','sample_name').aggr(\n",
    "#     clearing_joined_contents*imaging_request_contents,\n",
    "#     **replicated_args)\n",
    "\n",
    "# processing_joined_contents = (dj.U('username','request_name')*imaging_joined_contents).aggr(   \n",
    "# processing_request_contents,\n",
    "# **replicated_args,processor='processor',processing_progress='processing_progress',\n",
    "# processing_request_number='processing_request_number',\n",
    "# keep_all_rows=True\n",
    "# )\n",
    "# sample_joined_contents = request_contents * sample_contents * clearing_joined_contents \n",
    "imaging_joined_contents = clearing_joined_contents.aggr(\n",
    "    imaging_request_contents,\n",
    "    'imaging_request_number',\n",
    "    **replicated_args,\n",
    "    n_imaged='CONVERT(SUM(imaging_progress=\"complete\"),char)',\n",
    "    total_imaging_requests='COUNT(*)',\n",
    "    keep_all_rows=True\n",
    "    ).proj(\n",
    "    'imaging_request_number')\n",
    "imaging_joined_contents \n",
    "# processing_joined_contents = (dj.U('username','request_name') * imaging_joined_contents).aggr(   \n",
    "#         processing_request_contents,\n",
    "#         'imaging_request_number',\n",
    "#         **replicated_args,\n",
    "#         processing_request_number='processing_request_number',\n",
    "#         total_imaging_requests='total_imaging_requests',\n",
    "#         n_processed='CONVERT(SUM(processing_progress=\"complete\"),char)',\n",
    "#         total_processing_requests='CONVERT(COUNT(processing_progress),char)',\n",
    "#         keep_all_rows=True).proj(\n",
    "#             **replicated_args,\n",
    "#             imaging_request_number='IF(imaging_request_number = -1,\"N/A\",imaging_request_number)',\n",
    "#             processing_request_number='IF(processing_request_number is NULL, \"N/A\",processing_request_number)',\n",
    "#             total_imaging_requests='total_imaging_requests',\n",
    "#             total_processing_requests='IF(n_processed is NULL,0,total_processing_requests)', \n",
    "#             )\n",
    "# processing_joined_contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_contents = db_lightsheet.Request()\n",
    "sample_contents = db_lightsheet.Request.Sample()\n",
    "clearing_batch_contents = db_lightsheet.Request.ClearingBatch()\n",
    "imaging_request_contents = db_lightsheet.Request.ImagingRequest()\n",
    "processing_request_contents = db_lightsheet.Request.ProcessingRequest()\n",
    "\n",
    "\n",
    "replicated_args = dict(number_of_samples='number_of_samples',description='description',\n",
    "    species='species')\n",
    "sample_joined_contents = request_contents * sample_contents * clearing_batch_contents\n",
    "imaging_joined_contents = sample_joined_contents.aggr(\n",
    "    imaging_request_contents,\n",
    "    **replicated_args,\n",
    "    imaging_request_number='imaging_request_number',\n",
    "    n_imaged='CONVERT(SUM(imaging_progress=\"complete\"),char)',\n",
    "    total_imaging_requests='COUNT(*)',\n",
    "    keep_all_rows=True\n",
    "    ).proj(**replicated_args,\n",
    "           total_imaging_requests='IF(n_imaged is NULL, \"0\",total_imaging_requests)',\n",
    "           imaging_request_number='IF(imaging_request_number is NULL, \"N/A\",imaging_request_number)'\n",
    "        # fraction_imaged='CONCAT(n_imaged,\"/\",total_imaging_requests)'\n",
    "        )\n",
    "processing_joined_contents = (dj.U('username','request_name') * imaging_joined_contents).aggr(   \n",
    "    processing_request_contents,\n",
    "    **replicated_args,\n",
    "    imaging_request_number='imaging_request_number',\n",
    "    processing_request_number='processing_request_number',\n",
    "    total_imaging_requests='total_imaging_requests',\n",
    "    n_processed='CONVERT(SUM(processing_progress=\"complete\"),char)',\n",
    "    total_processing_requests='CONVERT(COUNT(processing_progress),char)',\n",
    "    keep_all_rows=True\n",
    "    ).proj(\n",
    "        **replicated_args,\n",
    "        processing_request_number='IF(processing_request_number is NULL, \"N/A\",processing_request_number)',\n",
    "        total_imaging_requests='total_imaging_requests',\n",
    "        total_processing_requests='IF(n_processed is NULL,0,total_processing_requests)', \n",
    "        )\n",
    "processing_joined_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "lightserv",
   "language": "python",
   "name": "lightserv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
